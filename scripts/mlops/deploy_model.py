"""
ëª¨ë¸ í‰ê°€ ë° ìë™ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

- ìƒˆ ëª¨ë¸ê³¼ í˜„ì¬ í”„ë¡œë•ì…˜ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
- ì„±ëŠ¥ì´ ê°œì„ ë˜ì—ˆì„ ë•Œë§Œ ë°°í¬
- ë°°í¬ íˆìŠ¤í† ë¦¬ ê¸°ë¡
"""

import os
import sys
import json
import shutil
import pickle
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from pathlib import Path
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support


# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


# ==================== ëª¨ë¸ ì •ì˜ (ì¬í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼) ====================
class HairstyleRecommender(nn.Module):
    """í—¤ì–´ìŠ¤íƒ€ì¼ ì¶”ì²œ ëª¨ë¸"""

    def __init__(self, n_faces=5, n_skins=3, n_styles=6,
                 emb_dim=16, hidden_dim=64):
        super().__init__()

        self.face_emb = nn.Embedding(n_faces, emb_dim)
        self.skin_emb = nn.Embedding(n_skins, emb_dim)
        self.style_emb = nn.Embedding(n_styles, emb_dim)

        self.shared_layers = nn.Sequential(
            nn.Linear(emb_dim * 3, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        self.score_head = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

        self.feedback_head = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 2)
        )

    def forward(self, face, skin, style):
        face_emb = self.face_emb(face)
        skin_emb = self.skin_emb(skin)
        style_emb = self.style_emb(style)

        x = torch.cat([face_emb, skin_emb, style_emb], dim=1)
        shared = self.shared_layers(x)

        score = self.score_head(shared).squeeze()
        feedback_logits = self.feedback_head(shared)

        return score, feedback_logits


# ==================== ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ====================
class HairstyleDataset(Dataset):
    """í—¤ì–´ìŠ¤íƒ€ì¼ ì¶”ì²œ ë°ì´í„°ì…‹"""

    def __init__(self, csv_path, encoders):
        self.df = pd.read_csv(csv_path)

        self.face_encoded = encoders['face'].transform(self.df['face_shape'])
        self.skin_encoded = encoders['skin'].transform(self.df['skin_tone'])
        self.style_encoded = encoders['style'].transform(self.df['hairstyle'])

        self.scores = self.df['score'].values
        self.feedbacks = (self.df['feedback'] == 'like').astype(int).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        return {
            'face': torch.tensor(self.face_encoded[idx], dtype=torch.long),
            'skin': torch.tensor(self.skin_encoded[idx], dtype=torch.long),
            'style': torch.tensor(self.style_encoded[idx], dtype=torch.long),
            'score': torch.tensor(self.scores[idx], dtype=torch.float32),
            'feedback': torch.tensor(self.feedbacks[idx], dtype=torch.long)
        }


# ==================== ëª¨ë¸ ë°°í¬ í´ë˜ìŠ¤ ====================
class ModelDeployer:
    """ëª¨ë¸ í‰ê°€ ë° ë°°í¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        new_model_path="models/checkpoints/model_latest.pth",
        new_encoder_path="models/checkpoints/encoders_latest.pkl",
        current_model_path="models/final_model.pth",
        current_encoder_path="models/encoders.pkl",
        test_data_path="data_source/test_data.csv",
        deployment_log_path="models/deployment_history.json"
    ):
        self.project_root = project_root
        self.new_model_path = self.project_root / new_model_path
        self.new_encoder_path = self.project_root / new_encoder_path
        self.current_model_path = self.project_root / current_model_path
        self.current_encoder_path = self.project_root / current_encoder_path
        self.test_data_path = self.project_root / test_data_path
        self.deployment_log_path = self.project_root / deployment_log_path

        self.device = torch.device("cpu")

    def load_encoders(self, encoder_path):
        """ì¸ì½”ë” ë¡œë“œ"""
        with open(encoder_path, 'rb') as f:
            encoders = pickle.load(f)
        return encoders

    def load_model(self, model_path, encoder_path):
        """ëª¨ë¸ ë¡œë“œ"""
        # ì¸ì½”ë” ë¡œë“œ
        encoders = self.load_encoders(encoder_path)

        # ëª¨ë¸ ìƒì„±
        n_faces = len(encoders['face'].classes_)
        n_skins = len(encoders['skin'].classes_)
        n_styles = len(encoders['style'].classes_)

        model = HairstyleRecommender(
            n_faces=n_faces,
            n_skins=n_skins,
            n_styles=n_styles
        ).to(self.device)

        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(torch.load(model_path, map_location=self.device))
        model.eval()

        return model, encoders

    def evaluate_model(self, model, encoders):
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_dataset = HairstyleDataset(self.test_data_path, encoders)
        test_loader = DataLoader(test_dataset, batch_size=64)

        all_preds = []
        all_targets = []

        with torch.no_grad():
            for batch in test_loader:
                face = batch['face'].to(self.device)
                skin = batch['skin'].to(self.device)
                style = batch['style'].to(self.device)
                feedback_target = batch['feedback'].to(self.device)

                _, feedback_logits = model(face, skin, style)
                feedback_pred = torch.argmax(feedback_logits, dim=1)

                all_preds.extend(feedback_pred.cpu().numpy())
                all_targets.extend(feedback_target.cpu().numpy())

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(all_targets, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_targets,
            all_preds,
            average='weighted',
            zero_division=0
        )

        metrics = {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1': float(f1)
        }

        return metrics

    def compare_models(self):
        """ìƒˆ ëª¨ë¸ê³¼ í˜„ì¬ ëª¨ë¸ ë¹„êµ"""
        print("=" * 60)
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print("=" * 60)

        # ìƒˆ ëª¨ë¸ í‰ê°€
        if not self.new_model_path.exists():
            print("âŒ ìƒˆ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        print("\nğŸ†• ìƒˆ ëª¨ë¸ í‰ê°€ ì¤‘...")
        new_model, new_encoders = self.load_model(self.new_model_path, self.new_encoder_path)
        new_metrics = self.evaluate_model(new_model, new_encoders)

        print(f"   Accuracy: {new_metrics['accuracy']:.4f}")
        print(f"   Precision: {new_metrics['precision']:.4f}")
        print(f"   Recall: {new_metrics['recall']:.4f}")
        print(f"   F1-Score: {new_metrics['f1']:.4f}")

        # í˜„ì¬ í”„ë¡œë•ì…˜ ëª¨ë¸ í‰ê°€ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
        current_metrics = None
        if self.current_model_path.exists() and self.current_encoder_path.exists():
            print("\nğŸ“¦ í˜„ì¬ í”„ë¡œë•ì…˜ ëª¨ë¸ í‰ê°€ ì¤‘...")
            try:
                current_model, current_encoders = self.load_model(
                    self.current_model_path,
                    self.current_encoder_path
                )
                current_metrics = self.evaluate_model(current_model, current_encoders)

                print(f"   Accuracy: {current_metrics['accuracy']:.4f}")
                print(f"   Precision: {current_metrics['precision']:.4f}")
                print(f"   Recall: {current_metrics['recall']:.4f}")
                print(f"   F1-Score: {current_metrics['f1']:.4f}")

            except Exception as e:
                print(f"   âš ï¸ í˜„ì¬ ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
                current_metrics = None
        else:
            print("\nâš ï¸ í˜„ì¬ í”„ë¡œë•ì…˜ ëª¨ë¸ ì—†ìŒ (ì²« ë°°í¬)")

        return {
            'new': new_metrics,
            'current': current_metrics
        }

    def should_deploy(self, comparison, min_improvement=0.0):
        """
        ë°°í¬ ì—¬ë¶€ ê²°ì •

        Args:
            comparison: ëª¨ë¸ ë¹„êµ ê²°ê³¼
            min_improvement: ìµœì†Œ ê°œì„  í­ (F1-score ê¸°ì¤€)

        Returns:
            bool: ë°°í¬ ì—¬ë¶€
        """
        # í˜„ì¬ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ ë°°í¬
        if comparison['current'] is None:
            print("\nâœ… ë°°í¬ ê²°ì •: ì²« ë°°í¬ (í˜„ì¬ ëª¨ë¸ ì—†ìŒ)")
            return True

        # F1-score ë¹„êµ
        new_f1 = comparison['new']['f1']
        current_f1 = comparison['current']['f1']
        improvement = new_f1 - current_f1

        print("\n" + "=" * 60)
        print("ğŸ¯ ë°°í¬ ì—¬ë¶€ ê²°ì •")
        print("=" * 60)
        print(f"   í˜„ì¬ F1: {current_f1:.4f}")
        print(f"   ìƒˆ ëª¨ë¸ F1: {new_f1:.4f}")
        print(f"   ê°œì„ í­: {improvement:+.4f}")
        print(f"   ìµœì†Œ ìš”êµ¬ ê°œì„ í­: {min_improvement:+.4f}")

        if improvement >= min_improvement:
            print(f"\nâœ… ë°°í¬ ê²°ì •: ì„±ëŠ¥ ê°œì„ ë¨ ({improvement:+.4f})")
            return True
        else:
            print(f"\nâŒ ë°°í¬ ê±°ë¶€: ì„±ëŠ¥ ê°œì„  ë¶€ì¡± ({improvement:+.4f} < {min_improvement:+.4f})")
            return False

    def deploy(self):
        """í”„ë¡œë•ì…˜ìœ¼ë¡œ ë°°í¬"""
        print("\n" + "=" * 60)
        print("ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬ ì‹œì‘")
        print("=" * 60)

        # ê¸°ì¡´ ëª¨ë¸ ë°±ì—…
        if self.current_model_path.exists():
            backup_dir = self.project_root / "models" / "backups"
            backup_dir.mkdir(exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_model = backup_dir / f"final_model_{timestamp}.pth"
            backup_encoder = backup_dir / f"encoders_{timestamp}.pkl"

            shutil.copy(self.current_model_path, backup_model)
            shutil.copy(self.current_encoder_path, backup_encoder)

            print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë°±ì—…:")
            print(f"   {backup_model}")
            print(f"   {backup_encoder}")

        # ìƒˆ ëª¨ë¸ì„ í”„ë¡œë•ì…˜ìœ¼ë¡œ ë³µì‚¬
        shutil.copy(self.new_model_path, self.current_model_path)
        shutil.copy(self.new_encoder_path, self.current_encoder_path)

        print(f"\nâœ… í”„ë¡œë•ì…˜ ë°°í¬ ì™„ë£Œ:")
        print(f"   {self.current_model_path}")
        print(f"   {self.current_encoder_path}")

    def log_deployment(self, comparison, deployed):
        """ë°°í¬ íˆìŠ¤í† ë¦¬ ê¸°ë¡"""
        # ê¸°ì¡´ ë¡œê·¸ ë¡œë“œ
        if self.deployment_log_path.exists():
            with open(self.deployment_log_path, 'r', encoding='utf-8') as f:
                history = json.load(f)
        else:
            history = []

        # ìƒˆ ë°°í¬ ê¸°ë¡ ì¶”ê°€
        record = {
            'timestamp': datetime.now().isoformat(),
            'deployed': deployed,
            'new_model_metrics': comparison['new'],
            'current_model_metrics': comparison['current']
        }

        history.append(record)

        # ì €ì¥
        with open(self.deployment_log_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… ë°°í¬ íˆìŠ¤í† ë¦¬ ê¸°ë¡: {self.deployment_log_path}")

    def run(self, min_improvement=0.0, auto_deploy=True):
        """ì „ì²´ ë°°í¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("\n" + "ğŸš€" * 30)
        print("ëª¨ë¸ ë°°í¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        print("ğŸš€" * 30 + "\n")

        # 1. ëª¨ë¸ ë¹„êµ
        comparison = self.compare_models()

        if comparison is None:
            print("\nâŒ ëª¨ë¸ ë¹„êµ ì‹¤íŒ¨")
            return False

        # 2. ë°°í¬ ì—¬ë¶€ ê²°ì •
        should_deploy = self.should_deploy(comparison, min_improvement)

        # 3. ë°°í¬ (ìë™ ë˜ëŠ” ìˆ˜ë™)
        deployed = False
        if should_deploy:
            if auto_deploy:
                self.deploy()
                deployed = True
            else:
                print("\nâš ï¸ ìë™ ë°°í¬ ë¹„í™œì„±í™”. ìˆ˜ë™ìœ¼ë¡œ ë°°í¬í•˜ì„¸ìš”.")
        else:
            print("\nâš ï¸ ì„±ëŠ¥ ê°œì„ ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ë°°í¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # 4. íˆìŠ¤í† ë¦¬ ê¸°ë¡
        self.log_deployment(comparison, deployed)

        print("\n" + "=" * 60)
        print("âœ… ë°°í¬ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        print("=" * 60)

        return deployed


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="ëª¨ë¸ í‰ê°€ ë° ë°°í¬")
    parser.add_argument(
        "--min-improvement",
        type=float,
        default=0.0,
        help="ìµœì†Œ F1-score ê°œì„ í­ (ê¸°ë³¸: 0.0, ì¡°ê¸ˆì´ë¼ë„ ê°œì„ ë˜ë©´ ë°°í¬)"
    )
    parser.add_argument(
        "--no-auto-deploy",
        action="store_true",
        help="ìë™ ë°°í¬ ë¹„í™œì„±í™” (í‰ê°€ë§Œ ìˆ˜í–‰)"
    )

    args = parser.parse_args()

    try:
        deployer = ModelDeployer()
        deployed = deployer.run(
            min_improvement=args.min_improvement,
            auto_deploy=not args.no_auto_deploy
        )

        if deployed:
            print("\nğŸ‰ ìƒˆ ëª¨ë¸ì´ í”„ë¡œë•ì…˜ì— ë°°í¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("âš ï¸ ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì—¬ ìƒˆ ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”.")
            return 0
        else:
            print("\nâš ï¸ ëª¨ë¸ì´ ë°°í¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 1

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
