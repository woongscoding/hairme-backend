"""
í•©ì„± ë°ì´í„°ì™€ ì‹¤ì œ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±

- í•©ì„± ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„°ë¥¼ í•©ì¹¨
- í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• 
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from sklearn.model_selection import train_test_split


# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


class TrainingDataPreparer:
    """í•™ìŠµ ë°ì´í„° ì¤€ë¹„ í´ë˜ìŠ¤"""

    def __init__(
        self,
        synthetic_data_path="data_source/synthetic_hairstyle_data.csv",
        real_data_path="data_source/real_user_data_latest.csv",
        output_dir="data_source"
    ):
        """
        Args:
            synthetic_data_path: í•©ì„± ë°ì´í„° ê²½ë¡œ
            real_data_path: ì‹¤ì œ ë°ì´í„° ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.project_root = project_root
        self.synthetic_path = self.project_root / synthetic_data_path
        self.real_path = self.project_root / real_data_path
        self.output_dir = self.project_root / output_dir

        self.synthetic_data = None
        self.real_data = None
        self.combined_data = None

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        print("=" * 60)
        print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        print("=" * 60)

        # í•©ì„± ë°ì´í„° ë¡œë“œ
        if self.synthetic_path.exists():
            self.synthetic_data = pd.read_csv(self.synthetic_path)
            print(f"âœ… í•©ì„± ë°ì´í„° ë¡œë“œ: {len(self.synthetic_data)}ê±´")
        else:
            print(f"âš ï¸ í•©ì„± ë°ì´í„° íŒŒì¼ ì—†ìŒ: {self.synthetic_path}")
            self.synthetic_data = pd.DataFrame()

        # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        if self.real_path.exists():
            self.real_data = pd.read_csv(self.real_path)
            print(f"âœ… ì‹¤ì œ ë°ì´í„° ë¡œë“œ: {len(self.real_data)}ê±´")
        else:
            print(f"âš ï¸ ì‹¤ì œ ë°ì´í„° íŒŒì¼ ì—†ìŒ: {self.real_path}")
            self.real_data = pd.DataFrame()

        if self.synthetic_data.empty and self.real_data.empty:
            raise ValueError("í•©ì„± ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„°ê°€ ëª¨ë‘ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")

    def combine_data(self, real_data_weight=2.0):
        """
        ë°ì´í„° ë³‘í•©

        Args:
            real_data_weight: ì‹¤ì œ ë°ì´í„°ì˜ ê°€ì¤‘ì¹˜ (ì‹¤ì œ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë³µì œí• ì§€)
                             ì˜ˆ: 2.0ì´ë©´ ì‹¤ì œ ë°ì´í„°ë¥¼ 2ë°°ë¡œ ì¦í­
        """
        print("\n" + "=" * 60)
        print("ğŸ”— ë°ì´í„° ë³‘í•© ì¤‘...")
        print("=" * 60)

        datasets = []

        # í•©ì„± ë°ì´í„° ì¶”ê°€
        if not self.synthetic_data.empty:
            synthetic = self.synthetic_data.copy()
            synthetic['data_source'] = 'synthetic'
            datasets.append(synthetic)
            print(f"   í•©ì„± ë°ì´í„°: {len(synthetic)}ê±´")

        # ì‹¤ì œ ë°ì´í„° ì¶”ê°€ (ê°€ì¤‘ì¹˜ ì ìš©)
        if not self.real_data.empty:
            real = self.real_data.copy()
            real['data_source'] = 'real'

            # ì‹¤ì œ ë°ì´í„° ë³µì œ (ì¤‘ìš”ë„ ì¦í­)
            if real_data_weight > 1.0:
                replications = int(real_data_weight)
                for i in range(replications):
                    datasets.append(real.copy())
                print(f"   ì‹¤ì œ ë°ì´í„°: {len(real)}ê±´ Ã— {replications} = {len(real) * replications}ê±´")
            else:
                datasets.append(real)
                print(f"   ì‹¤ì œ ë°ì´í„°: {len(real)}ê±´")

        # ë³‘í•©
        self.combined_data = pd.concat(datasets, ignore_index=True)

        # ì…”í”Œ
        self.combined_data = self.combined_data.sample(frac=1, random_state=42).reset_index(drop=True)

        print(f"âœ… ë³‘í•© ì™„ë£Œ: ì´ {len(self.combined_data)}ê±´")

        # í†µê³„ ì¶œë ¥
        self._print_combined_statistics()

    def _print_combined_statistics(self):
        """ë³‘í•©ëœ ë°ì´í„° í†µê³„ ì¶œë ¥"""
        df = self.combined_data

        print("\nğŸ“Š ë³‘í•© ë°ì´í„° í†µê³„:")
        print(f"   ì´ ë ˆì½”ë“œ: {len(df)}ê±´")

        if 'data_source' in df.columns:
            print(f"\n   ë°ì´í„° ì¶œì²˜:")
            print(df['data_source'].value_counts().to_string())

        print(f"\n   ì–¼êµ´í˜• ë¶„í¬:")
        print(df['face_shape'].value_counts().to_string())

        print(f"\n   í”¼ë¶€í†¤ ë¶„í¬:")
        print(df['skin_tone'].value_counts().to_string())

        print(f"\n   í—¤ì–´ìŠ¤íƒ€ì¼ ë¶„í¬:")
        print(df['hairstyle'].value_counts().to_string())

        print(f"\n   í”¼ë“œë°± ë¶„í¬:")
        feedback_counts = df['feedback'].value_counts()
        print(feedback_counts.to_string())

        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨
        if 'like' in feedback_counts.index and 'dislike' in feedback_counts.index:
            like_count = feedback_counts['like']
            dislike_count = feedback_counts['dislike']
            imbalance_ratio = like_count / dislike_count if dislike_count > 0 else float('inf')
            print(f"\n   âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨ (like/dislike): {imbalance_ratio:.2f}")

            if imbalance_ratio > 3.0:
                print(f"   âš ï¸ ê²½ê³ : likeê°€ dislikeë³´ë‹¤ {imbalance_ratio:.1f}ë°° ë§ìŠµë‹ˆë‹¤.")
                print(f"         ì¬í•™ìŠµ ì‹œ class_weightë¥¼ ì ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

        print(f"\n   ë„¤ì´ë²„ í´ë¦­ë¥ : {df['naver_clicked'].mean()*100:.1f}%")
        print(f"   í‰ê·  ì ìˆ˜: {df['score'].mean():.3f}")

    def split_data(self, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
        """
        í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„í• 

        Args:
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
            val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            test_ratio: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
        """
        print("\n" + "=" * 60)
        print("âœ‚ï¸ ë°ì´í„° ë¶„í•  ì¤‘...")
        print("=" * 60)

        # ë¹„ìœ¨ ê²€ì¦
        total_ratio = train_ratio + val_ratio + test_ratio
        if not np.isclose(total_ratio, 1.0):
            raise ValueError(f"ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤: {total_ratio}")

        df = self.combined_data

        # 1ì°¨ ë¶„í• : train + (val + test)
        train_df, temp_df = train_test_split(
            df,
            test_size=(val_ratio + test_ratio),
            random_state=42,
            stratify=df['feedback']  # í”¼ë“œë°± ë¹„ìœ¨ ìœ ì§€
        )

        # 2ì°¨ ë¶„í• : val + test
        val_size = val_ratio / (val_ratio + test_ratio)
        val_df, test_df = train_test_split(
            temp_df,
            test_size=(1 - val_size),
            random_state=42,
            stratify=temp_df['feedback']
        )

        print(f"âœ… í•™ìŠµ ë°ì´í„°: {len(train_df)}ê±´ ({len(train_df)/len(df)*100:.1f}%)")
        print(f"âœ… ê²€ì¦ ë°ì´í„°: {len(val_df)}ê±´ ({len(val_df)/len(df)*100:.1f}%)")
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê±´ ({len(test_df)/len(df)*100:.1f}%)")

        return train_df, val_df, test_df

    def save_datasets(self, train_df, val_df, test_df):
        """ë°ì´í„°ì…‹ ì €ì¥"""
        print("\n" + "=" * 60)
        print("ğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...")
        print("=" * 60)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # íƒ€ì„ìŠ¤íƒ¬í”„ ë²„ì „
        train_path = self.output_dir / f"train_data_{timestamp}.csv"
        val_path = self.output_dir / f"val_data_{timestamp}.csv"
        test_path = self.output_dir / f"test_data_{timestamp}.csv"

        # ìµœì‹  ë²„ì „ (ì‹¬ë³¼ë¦­ ë§í¬ì²˜ëŸ¼ ì‚¬ìš©)
        train_latest = self.output_dir / "train_data.csv"
        val_latest = self.output_dir / "val_data.csv"
        test_latest = self.output_dir / "test_data.csv"

        # data_source ì»¬ëŸ¼ ì œê±° (í•™ìŠµì— ë¶ˆí•„ìš”)
        train_clean = train_df.drop(columns=['data_source'], errors='ignore')
        val_clean = val_df.drop(columns=['data_source'], errors='ignore')
        test_clean = test_df.drop(columns=['data_source'], errors='ignore')

        # ì €ì¥
        train_clean.to_csv(train_path, index=False, encoding='utf-8-sig')
        val_clean.to_csv(val_path, index=False, encoding='utf-8-sig')
        test_clean.to_csv(test_path, index=False, encoding='utf-8-sig')

        train_clean.to_csv(train_latest, index=False, encoding='utf-8-sig')
        val_clean.to_csv(val_latest, index=False, encoding='utf-8-sig')
        test_clean.to_csv(test_latest, index=False, encoding='utf-8-sig')

        print(f"âœ… ì €ì¥ ì™„ë£Œ:")
        print(f"   {train_path}")
        print(f"   {val_path}")
        print(f"   {test_path}")

        # ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_report(train_df, val_df, test_df, timestamp)

        return {
            'train': train_path,
            'val': val_path,
            'test': test_path
        }

    def _generate_report(self, train_df, val_df, test_df, timestamp):
        """ë°ì´í„° ì¤€ë¹„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_path = self.output_dir / f"data_preparation_report_{timestamp}.txt"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("ë°ì´í„° ì¤€ë¹„ ë¦¬í¬íŠ¸\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # ì „ì²´ í†µê³„
            f.write("## ì „ì²´ ë°ì´í„° í†µê³„\n")
            f.write(f"ì´ ë ˆì½”ë“œ: {len(self.combined_data)}ê±´\n\n")

            if 'data_source' in self.combined_data.columns:
                f.write("ë°ì´í„° ì¶œì²˜:\n")
                f.write(self.combined_data['data_source'].value_counts().to_string())
                f.write("\n\n")

            # ë¶„í•  í†µê³„
            f.write("## ë°ì´í„° ë¶„í• \n")
            f.write(f"í•™ìŠµ ë°ì´í„°: {len(train_df)}ê±´\n")
            f.write(f"ê²€ì¦ ë°ì´í„°: {len(val_df)}ê±´\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê±´\n\n")

            # ê° ì„¸íŠ¸ì˜ í”¼ë“œë°± ë¶„í¬
            for name, df in [("í•™ìŠµ", train_df), ("ê²€ì¦", val_df), ("í…ŒìŠ¤íŠ¸", test_df)]:
                f.write(f"### {name} ë°ì´í„° í”¼ë“œë°± ë¶„í¬\n")
                f.write(df['feedback'].value_counts().to_string())
                f.write("\n\n")

        print(f"ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„±: {report_path}")

    def prepare(self, real_data_weight=2.0):
        """ì „ì²´ ì¤€ë¹„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("\n" + "ğŸš€" * 30)
        print("í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
        print("ğŸš€" * 30 + "\n")

        # 1. ë°ì´í„° ë¡œë“œ
        self.load_data()

        # 2. ë°ì´í„° ë³‘í•©
        self.combine_data(real_data_weight=real_data_weight)

        # 3. ë°ì´í„° ë¶„í• 
        train_df, val_df, test_df = self.split_data()

        # 4. ì €ì¥
        paths = self.save_datasets(train_df, val_df, test_df)

        print("\n" + "=" * 60)
        print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
        print("=" * 60)

        return paths


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="í•™ìŠµ ë°ì´í„° ì¤€ë¹„")
    parser.add_argument(
        "--real-weight",
        type=float,
        default=2.0,
        help="ì‹¤ì œ ë°ì´í„° ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 2.0, ì‹¤ì œ ë°ì´í„°ë¥¼ 2ë°°ë¡œ ì¦í­)"
    )
    parser.add_argument(
        "--synthetic-data",
        type=str,
        default="data_source/synthetic_hairstyle_data.csv",
        help="í•©ì„± ë°ì´í„° ê²½ë¡œ"
    )
    parser.add_argument(
        "--real-data",
        type=str,
        default="data_source/real_user_data_latest.csv",
        help="ì‹¤ì œ ë°ì´í„° ê²½ë¡œ"
    )

    args = parser.parse_args()

    try:
        preparer = TrainingDataPreparer(
            synthetic_data_path=args.synthetic_data,
            real_data_path=args.real_data
        )

        paths = preparer.prepare(real_data_weight=args.real_weight)

        print("\nâœ… ìƒì„±ëœ íŒŒì¼:")
        for key, path in paths.items():
            print(f"   {key}: {path}")

        return 0

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
