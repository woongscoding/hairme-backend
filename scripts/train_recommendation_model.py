#!/usr/bin/env python3
"""
í—¤ì–´ìŠ¤íƒ€ì¼ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ

PyTorchë¡œ Neural Network íšŒê·€ ëª¨ë¸ í•™ìŠµ

Model Architecture:
  Input (392) â†’ Dense(256) â†’ ReLU â†’ Dropout(0.3)
              â†’ Dense(128) â†’ ReLU â†’ Dropout(0.2)
              â†’ Dense(64) â†’ ReLU
              â†’ Dense(1) - Output

Loss: MSE (Mean Squared Error)
Optimizer: Adam

Author: HairMe ML Team
Date: 2025-11-08
Version: 1.0.0
"""

import argparse
import json
import sys
import time
from pathlib import Path
from typing import Tuple, Dict
import numpy as np
import matplotlib.pyplot as plt

# Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from collections import Counter


# ==================== ì„¤ì • ====================
class Config:
    """í•™ìŠµ ì„¤ì •"""

    # ëª¨ë¸ êµ¬ì¡°
    INPUT_DIM = 392
    HIDDEN_DIMS = [256, 128, 64]
    OUTPUT_DIM = 1
    DROPOUT_RATES = [0.3, 0.2, 0.0]

    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    BATCH_SIZE = 32
    LEARNING_RATE = 0.001
    NUM_EPOCHS = 100
    EARLY_STOPPING_PATIENCE = 15

    # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ
    DEFAULT_DATASET_PATH = "data_source/ml_training_dataset.npz"
    DEFAULT_OUTPUT_DIR = "models"


# ==================== ë°ì´í„°ì…‹ ====================
class HairstyleDataset(Dataset):
    """PyTorch Dataset for hairstyle recommendation"""

    def __init__(self, X: np.ndarray, y: np.ndarray):
        """
        ì´ˆê¸°í™”

        Args:
            X: íŠ¹ì§• í–‰ë ¬ (N, 392)
            y: íƒ€ê²Ÿ ë²¡í„° (N,)
        """
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y).unsqueeze(1)  # (N, 1)

    def __len__(self) -> int:
        return len(self.X)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.X[idx], self.y[idx]


# ==================== ëª¨ë¸ ====================
class RecommendationModel(nn.Module):
    """Neural Network for hairstyle recommendation score prediction"""

    def __init__(
        self,
        input_dim: int = Config.INPUT_DIM,
        hidden_dims: list = None,
        dropout_rates: list = None
    ):
        """
        ì´ˆê¸°í™”

        Args:
            input_dim: ì…ë ¥ ì°¨ì›
            hidden_dims: ì€ë‹‰ì¸µ ì°¨ì› ë¦¬ìŠ¤íŠ¸
            dropout_rates: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ë¦¬ìŠ¤íŠ¸
        """
        super(RecommendationModel, self).__init__()

        if hidden_dims is None:
            hidden_dims = Config.HIDDEN_DIMS
        if dropout_rates is None:
            dropout_rates = Config.DROPOUT_RATES

        layers = []
        prev_dim = input_dim

        # ì€ë‹‰ì¸µ êµ¬ì¶•
        for hidden_dim, dropout_rate in zip(hidden_dims, dropout_rates):
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())

            if dropout_rate > 0:
                layers.append(nn.Dropout(dropout_rate))

            prev_dim = hidden_dim

        # ì¶œë ¥ì¸µ
        layers.append(nn.Linear(prev_dim, Config.OUTPUT_DIM))

        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        return self.network(x)


# ==================== í•™ìŠµ ====================
class Trainer:
    """ëª¨ë¸ í•™ìŠµ"""

    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        device: torch.device
    ):
        """
        ì´ˆê¸°í™”

        Args:
            model: í•™ìŠµí•  ëª¨ë¸
            train_loader: í•™ìŠµ ë°ì´í„° ë¡œë”
            val_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
            device: í•™ìŠµ ë””ë°”ì´ìŠ¤
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device

        self.criterion = nn.MSELoss()
        self.optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)

        # í•™ìŠµ ì´ë ¥
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_mae': [],
            'val_mae': []
        }

        self.best_val_loss = float('inf')
        self.patience_counter = 0

    def train_epoch(self) -> Tuple[float, float]:
        """1 ì—í­ í•™ìŠµ"""
        self.model.train()
        total_loss = 0.0
        total_mae = 0.0
        n_batches = 0

        for X_batch, y_batch in self.train_loader:
            X_batch = X_batch.to(self.device)
            y_batch = y_batch.to(self.device)

            # Forward
            self.optimizer.zero_grad()
            predictions = self.model(X_batch)
            loss = self.criterion(predictions, y_batch)

            # Backward
            loss.backward()
            self.optimizer.step()

            # í†µê³„
            total_loss += loss.item()
            total_mae += torch.mean(torch.abs(predictions - y_batch)).item()
            n_batches += 1

        avg_loss = total_loss / n_batches
        avg_mae = total_mae / n_batches

        return avg_loss, avg_mae

    def validate(self) -> Tuple[float, float]:
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0.0
        total_mae = 0.0
        n_batches = 0

        with torch.no_grad():
            for X_batch, y_batch in self.val_loader:
                X_batch = X_batch.to(self.device)
                y_batch = y_batch.to(self.device)

                predictions = self.model(X_batch)
                loss = self.criterion(predictions, y_batch)

                total_loss += loss.item()
                total_mae += torch.mean(torch.abs(predictions - y_batch)).item()
                n_batches += 1

        avg_loss = total_loss / n_batches
        avg_mae = total_mae / n_batches

        return avg_loss, avg_mae

    def train(self, num_epochs: int = Config.NUM_EPOCHS) -> Dict:
        """ì „ì²´ í•™ìŠµ ë£¨í”„"""
        print(f"\nğŸš€ í•™ìŠµ ì‹œì‘...")
        print(f"  ì—í­ ìˆ˜: {num_epochs}")
        print(f"  ë°°ì¹˜ í¬ê¸°: {Config.BATCH_SIZE}")
        print(f"  í•™ìŠµë¥ : {Config.LEARNING_RATE}")
        print(f"  Early Stopping Patience: {Config.EARLY_STOPPING_PATIENCE}")
        print(f"  ë””ë°”ì´ìŠ¤: {self.device}")

        start_time = time.time()

        for epoch in range(num_epochs):
            # í•™ìŠµ
            train_loss, train_mae = self.train_epoch()
            val_loss, val_mae = self.validate()

            # ì´ë ¥ ì €ì¥
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_mae'].append(train_mae)
            self.history['val_mae'].append(val_mae)

            # ì¶œë ¥ (10 ì—í­ë§ˆë‹¤)
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(f"  Epoch {epoch+1:3d}/{num_epochs} - "
                      f"Train Loss: {train_loss:.4f}, MAE: {train_mae:.2f} | "
                      f"Val Loss: {val_loss:.4f}, MAE: {val_mae:.2f}")

            # Early Stopping
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
            else:
                self.patience_counter += 1

            if self.patience_counter >= Config.EARLY_STOPPING_PATIENCE:
                print(f"\n  âš ï¸  Early Stopping at epoch {epoch+1}")
                break

        elapsed_time = time.time() - start_time

        print(f"\n  âœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"  â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        print(f"  ğŸ“Š ìµœì¢… ì„±ëŠ¥:")
        print(f"    - Train Loss: {self.history['train_loss'][-1]:.4f}, "
              f"MAE: {self.history['train_mae'][-1]:.2f}")
        print(f"    - Val Loss: {self.history['val_loss'][-1]:.4f}, "
              f"MAE: {self.history['val_mae'][-1]:.2f}")

        return self.history


# ==================== í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ====================
def compute_sample_weights(X: np.ndarray, skin_tone_names: list = None) -> np.ndarray:
    """
    í”¼ë¶€í†¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê³„ì‚°

    Args:
        X: íŠ¹ì§• í–‰ë ¬ (N, 392) - [face(4) + tone(4) + emb(384)]
        skin_tone_names: í”¼ë¶€í†¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì¶œë ¥ìš©)

    Returns:
        ê° ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ ë°°ì—´ (N,)
    """
    if skin_tone_names is None:
        skin_tone_names = ["ê²¨ìš¸ì¿¨", "ê°€ì„ì›œ", "ë´„ì›œ", "ì—¬ë¦„ì¿¨"]

    print(f"\nâš–ï¸  í”¼ë¶€í†¤ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")

    # í”¼ë¶€í†¤ one-hotì€ ì¸ë±ìŠ¤ 4-7
    skin_tone_onehot = X[:, 4:8]

    # ê° ìƒ˜í”Œì˜ í”¼ë¶€í†¤ í´ë˜ìŠ¤ ì¶”ì¶œ (argmax)
    skin_tone_classes = np.argmax(skin_tone_onehot, axis=1)

    # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜
    class_counts = Counter(skin_tone_classes)

    print(f"\n  ğŸ“Š í”¼ë¶€í†¤ ë¶„í¬:")
    for class_idx, count in sorted(class_counts.items()):
        tone_name = skin_tone_names[class_idx] if class_idx < len(skin_tone_names) else f"Unknown_{class_idx}"
        percentage = count / len(X) * 100
        print(f"    - {tone_name}: {count}ê°œ ({percentage:.1f}%)")

    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (balanced)
    # weight = n_samples / (n_classes * n_samples_per_class)
    n_samples = len(X)
    n_classes = len(class_counts)

    class_weights = {}
    for class_idx, count in class_counts.items():
        class_weights[class_idx] = n_samples / (n_classes * count)

    print(f"\n  âš–ï¸  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:")
    for class_idx, weight in sorted(class_weights.items()):
        tone_name = skin_tone_names[class_idx] if class_idx < len(skin_tone_names) else f"Unknown_{class_idx}"
        print(f"    - {tone_name}: {weight:.3f}")

    # ê° ìƒ˜í”Œì— ê°€ì¤‘ì¹˜ í• ë‹¹
    sample_weights = np.array([class_weights[class_idx] for class_idx in skin_tone_classes])

    print(f"\n  âœ… ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ")
    print(f"    - ìµœì†Œ ê°€ì¤‘ì¹˜: {sample_weights.min():.3f}")
    print(f"    - ìµœëŒ€ ê°€ì¤‘ì¹˜: {sample_weights.max():.3f}")
    print(f"    - ê°€ì¤‘ì¹˜ ë¹„ìœ¨: 1:{sample_weights.max()/sample_weights.min():.1f}")

    return sample_weights


# ==================== ì €ì¥ ë° ì‹œê°í™” ====================
class ModelExporter:
    """ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥"""

    @staticmethod
    def save_model(model: nn.Module, output_dir: Path):
        """ëª¨ë¸ ì €ì¥"""
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

        # PyTorch ëª¨ë¸ ì €ì¥
        model_path = output_dir / "hairstyle_recommender.pt"
        torch.save(model.state_dict(), model_path)
        print(f"  âœ… ëª¨ë¸ ì €ì¥: {model_path}")

        # ëª¨ë¸ í¬ê¸°
        model_size = model_path.stat().st_size / 1024
        print(f"  ğŸ“Š ëª¨ë¸ í¬ê¸°: {model_size:.1f} KB")

    @staticmethod
    def save_history(history: Dict, output_dir: Path):
        """í•™ìŠµ ì´ë ¥ ì €ì¥"""
        history_path = output_dir / "training_history.json"

        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2)

        print(f"  âœ… í•™ìŠµ ì´ë ¥ ì €ì¥: {history_path}")

    @staticmethod
    def plot_history(history: Dict, output_dir: Path):
        """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
        print(f"\nğŸ“Š í•™ìŠµ ê³¡ì„  ì €ì¥ ì¤‘...")

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        # Loss ê³¡ì„ 
        ax1.plot(history['train_loss'], label='Train Loss')
        ax1.plot(history['val_loss'], label='Val Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('MSE Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.legend()
        ax1.grid(True)

        # MAE ê³¡ì„ 
        ax2.plot(history['train_mae'], label='Train MAE')
        ax2.plot(history['val_mae'], label='Val MAE')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('MAE (Mean Absolute Error)')
        ax2.set_title('Training and Validation MAE')
        ax2.legend()
        ax2.grid(True)

        plt.tight_layout()

        plot_path = output_dir / "training_curves.png"
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()

        print(f"  âœ… í•™ìŠµ ê³¡ì„  ì €ì¥: {plot_path}")


# ==================== ë©”ì¸ í•¨ìˆ˜ ====================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="í—¤ì–´ìŠ¤íƒ€ì¼ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ"
    )

    parser.add_argument(
        '-d', '--dataset',
        type=str,
        default=Config.DEFAULT_DATASET_PATH,
        help=f'ë°ì´í„°ì…‹ ê²½ë¡œ (ê¸°ë³¸ê°’: {Config.DEFAULT_DATASET_PATH})'
    )

    parser.add_argument(
        '-o', '--output-dir',
        type=str,
        default=Config.DEFAULT_OUTPUT_DIR,
        help=f'ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: {Config.DEFAULT_OUTPUT_DIR})'
    )

    parser.add_argument(
        '--epochs',
        type=int,
        default=Config.NUM_EPOCHS,
        help=f'í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸ê°’: {Config.NUM_EPOCHS})'
    )

    args = parser.parse_args()

    dataset_path = Path(args.dataset)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("ğŸ¨ í—¤ì–´ìŠ¤íƒ€ì¼ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ v1.0.0")
    print("=" * 60)
    print(f"ë°ì´í„°ì…‹: {dataset_path}")
    print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {output_dir.absolute()}")
    print("=" * 60)

    try:
        # 1. ë°ì´í„° ë¡œë“œ
        print(f"\nğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”©...")
        data = np.load(dataset_path)
        X_train, y_train = data['X_train'], data['y_train']
        X_val, y_val = data['X_val'], data['y_val']

        print(f"  âœ… ë¡œë“œ ì™„ë£Œ:")
        print(f"    - Train: {len(X_train)}ê°œ")
        print(f"    - Val: {len(X_val)}ê°œ")

        # 2. í”¼ë¶€í†¤ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        sample_weights = compute_sample_weights(X_train)

        # 3. ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±
        train_dataset = HairstyleDataset(X_train, y_train)
        val_dataset = HairstyleDataset(X_val, y_val)

        # WeightedRandomSampler ìƒì„± (í”¼ë¶€í†¤ ë¶ˆê· í˜• í•´ê²°)
        sampler = WeightedRandomSampler(
            weights=sample_weights,
            num_samples=len(sample_weights),
            replacement=True  # ì¤‘ë³µ ìƒ˜í”Œë§ í—ˆìš©
        )

        train_loader = DataLoader(
            train_dataset,
            batch_size=Config.BATCH_SIZE,
            sampler=sampler  # shuffle=True ëŒ€ì‹  sampler ì‚¬ìš©
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=Config.BATCH_SIZE,
            shuffle=False
        )

        print(f"\n  âœ… DataLoader ìƒì„± ì™„ë£Œ:")
        print(f"    - Train Loader: WeightedRandomSampler ì ìš© (í”¼ë¶€í†¤ ê· í˜•)")
        print(f"    - Batch Size: {Config.BATCH_SIZE}")

        # 4. ëª¨ë¸ ìƒì„±
        print(f"\nğŸ¤– ëª¨ë¸ ìƒì„±...")
        model = RecommendationModel()

        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        print(f"  âœ… ëª¨ë¸ êµ¬ì¡°:")
        print(f"    - ì…ë ¥: {Config.INPUT_DIM}ì°¨ì›")
        print(f"    - ì€ë‹‰ì¸µ: {Config.HIDDEN_DIMS}")
        print(f"    - ì¶œë ¥: {Config.OUTPUT_DIM}ì°¨ì›")
        print(f"    - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
        print(f"    - í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}ê°œ")

        # 5. ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # 6. í•™ìŠµ
        trainer = Trainer(model, train_loader, val_loader, device)
        history = trainer.train(num_epochs=args.epochs)

        # 7. ì €ì¥
        exporter = ModelExporter()
        exporter.save_model(model, output_dir)
        exporter.save_history(history, output_dir)
        exporter.plot_history(history, output_dir)

        print("\n" + "=" * 60)
        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ê²°ê³¼:")
        print(f"  - ìµœì¢… Val MAE: {history['val_mae'][-1]:.2f}ì ")
        print(f"  - ì €ì¥ëœ íŒŒì¼:")
        print(f"    * {output_dir / 'hairstyle_recommender.pt'}")
        print(f"    * {output_dir / 'training_history.json'}")
        print(f"    * {output_dir / 'training_curves.png'}")
        print("=" * 60)

        return 0

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
