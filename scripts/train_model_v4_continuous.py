#!/usr/bin/env python3
"""
v4 ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ - ì—°ì†í˜• ë³€ìˆ˜ ê¸°ë°˜

ì…ë ¥ ë°ì´í„° í˜•ì‹ (NPZ):
- face_features: [N, 6] - MediaPipe ì–¼êµ´ ì¸¡ì •ê°’
- skin_features: [N, 2] - MediaPipe í”¼ë¶€ ì¸¡ì •ê°’
- hairstyles: [N] - í—¤ì–´ìŠ¤íƒ€ì¼ëª…
- scores: [N] - ì¶”ì²œ ì ìˆ˜ (0-100)

Author: HairMe ML Team
Date: 2025-11-15
"""

import os
import sys
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from pathlib import Path
import logging
from typing import Dict, Tuple
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from models.ml_recommender_v4 import ContinuousRecommenderV4
from sentence_transformers import SentenceTransformer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)


class HairstyleDatasetV4(Dataset):
    """ì—°ì†í˜• ë³€ìˆ˜ ê¸°ë°˜ í—¤ì–´ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹"""

    def __init__(
        self,
        face_features: np.ndarray,
        skin_features: np.ndarray,
        style_embeddings: np.ndarray,
        scores: np.ndarray
    ):
        """
        Args:
            face_features: [N, 6]
            skin_features: [N, 2]
            style_embeddings: [N, 384]
            scores: [N]
        """
        self.face_features = torch.tensor(face_features, dtype=torch.float32)
        self.skin_features = torch.tensor(skin_features, dtype=torch.float32)
        self.style_embeddings = torch.tensor(style_embeddings, dtype=torch.float32)
        self.scores = torch.tensor(scores, dtype=torch.float32).unsqueeze(1)

    def __len__(self):
        return len(self.scores)

    def __getitem__(self, idx):
        return (
            self.face_features[idx],
            self.skin_features[idx],
            self.style_embeddings[idx],
            self.scores[idx]
        )


def load_training_data(data_path: str) -> Dict:
    """
    NPZ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬

    Args:
        data_path: NPZ íŒŒì¼ ê²½ë¡œ

    Returns:
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {data_path}")

    data = np.load(data_path, allow_pickle=True)

    face_features = data['face_features']  # [N, 6]
    skin_features = data['skin_features']  # [N, 2]
    hairstyles = data['hairstyles']  # [N]
    scores = data['scores']  # [N]

    logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    logger.info(f"  - ìƒ˜í”Œ ìˆ˜: {len(scores):,}")
    logger.info(f"  - Face features: {face_features.shape}")
    logger.info(f"  - Skin features: {skin_features.shape}")
    logger.info(f"  - Hairstyles: {len(hairstyles)}")

    # í—¤ì–´ìŠ¤íƒ€ì¼ ì„ë² ë”© ìƒì„±
    logger.info("ğŸ”„ í—¤ì–´ìŠ¤íƒ€ì¼ ì„ë² ë”© ìƒì„± ì¤‘...")
    sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    style_embeddings = sentence_model.encode(
        hairstyles.tolist(),
        show_progress_bar=True,
        convert_to_numpy=True
    )

    logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {style_embeddings.shape}")

    # ë°ì´í„° ì •ê·œí™” (ì˜µì…˜)
    # face_featuresì˜ ìŠ¤ì¼€ì¼ì´ í¬ê²Œ ë‹¤ë¥´ë¯€ë¡œ ì •ê·œí™” ê³ ë ¤
    # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì›ë³¸ ì‚¬ìš© (ëª¨ë¸ì—ì„œ BatchNorm ì‚¬ìš©)

    # í†µê³„
    logger.info(f"\nğŸ“Š ë°ì´í„° í†µê³„:")
    logger.info(f"  - ì ìˆ˜ ë²”ìœ„: {scores.min():.1f} ~ {scores.max():.1f}")
    logger.info(f"  - ì ìˆ˜ í‰ê· : {scores.mean():.1f} Â± {scores.std():.1f}")
    logger.info(f"  - ê³ ì ìˆ˜ (â‰¥80): {(scores >= 80).sum():,}ê°œ ({(scores >= 80).sum()/len(scores)*100:.1f}%)")
    logger.info(f"  - ì €ì ìˆ˜ (<40): {(scores < 40).sum():,}ê°œ ({(scores < 40).sum()/len(scores)*100:.1f}%)")

    return {
        'face_features': face_features,
        'skin_features': skin_features,
        'style_embeddings': style_embeddings,
        'scores': scores,
        'hairstyles': hairstyles
    }


def create_dataloaders(
    data: Dict,
    batch_size: int = 64,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    Train/Val/Test ë°ì´í„°ë¡œë” ìƒì„±

    Args:
        data: ì „ì²˜ë¦¬ëœ ë°ì´í„°
        batch_size: ë°°ì¹˜ í¬ê¸°
        val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
        test_ratio: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨

    Returns:
        (train_loader, val_loader, test_loader)
    """
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = HairstyleDatasetV4(
        face_features=data['face_features'],
        skin_features=data['skin_features'],
        style_embeddings=data['style_embeddings'],
        scores=data['scores']
    )

    # Train/Val/Test ë¶„í• 
    total_size = len(dataset)
    test_size = int(total_size * test_ratio)
    val_size = int(total_size * val_ratio)
    train_size = total_size - val_size - test_size

    train_dataset, val_dataset, test_dataset = random_split(
        dataset,
        [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )

    logger.info(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    logger.info(f"  - Train: {len(train_dataset):,} ({len(train_dataset)/total_size*100:.1f}%)")
    logger.info(f"  - Val:   {len(val_dataset):,} ({len(val_dataset)/total_size*100:.1f}%)")
    logger.info(f"  - Test:  {len(test_dataset):,} ({len(test_dataset)/total_size*100:.1f}%)")

    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # Windowsì—ì„œëŠ” 0 ê¶Œì¥
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )

    return train_loader, val_loader, test_loader


def train_epoch(
    model: nn.Module,
    train_loader: DataLoader,
    optimizer: optim.Optimizer,
    criterion: nn.Module,
    device: torch.device
) -> float:
    """1 ì—í­ í•™ìŠµ"""
    model.train()
    total_loss = 0.0

    for face_feat, skin_feat, style_emb, scores in train_loader:
        face_feat = face_feat.to(device)
        skin_feat = skin_feat.to(device)
        style_emb = style_emb.to(device)
        scores = scores.to(device)

        # Forward
        optimizer.zero_grad()
        pred_scores = model(face_feat, skin_feat, style_emb)

        # Loss
        loss = criterion(pred_scores, scores)

        # Backward
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    return avg_loss


def validate(
    model: nn.Module,
    val_loader: DataLoader,
    criterion: nn.Module,
    device: torch.device
) -> Tuple[float, float]:
    """ê²€ì¦"""
    model.eval()
    total_loss = 0.0
    total_mae = 0.0

    with torch.no_grad():
        for face_feat, skin_feat, style_emb, scores in val_loader:
            face_feat = face_feat.to(device)
            skin_feat = skin_feat.to(device)
            style_emb = style_emb.to(device)
            scores = scores.to(device)

            # Predict
            pred_scores = model(face_feat, skin_feat, style_emb)

            # Loss
            loss = criterion(pred_scores, scores)
            mae = torch.abs(pred_scores - scores).mean()

            total_loss += loss.item()
            total_mae += mae.item()

    avg_loss = total_loss / len(val_loader)
    avg_mae = total_mae / len(val_loader)

    return avg_loss, avg_mae


def train_model(
    data_path: str,
    output_dir: str = "models",
    epochs: int = 100,
    batch_size: int = 64,
    learning_rate: float = 0.001,
    patience: int = 15,
    use_attention: bool = True
):
    """
    ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜

    Args:
        data_path: NPZ ë°ì´í„° ê²½ë¡œ
        output_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        epochs: í•™ìŠµ ì—í­ ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        learning_rate: í•™ìŠµë¥ 
        patience: Early stopping patience
        use_attention: Attention ì‚¬ìš© ì—¬ë¶€
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ v4 ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    logger.info("=" * 60)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {device}")

    # ë°ì´í„° ë¡œë“œ
    data = load_training_data(data_path)

    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader, val_loader, test_loader = create_dataloaders(
        data,
        batch_size=batch_size
    )

    # ëª¨ë¸ ìƒì„±
    model = ContinuousRecommenderV4(
        face_feat_dim=6,
        skin_feat_dim=2,
        style_embed_dim=384,
        use_attention=use_attention,
        dropout_rate=0.3
    ).to(device)

    logger.info(f"\nğŸ—ï¸  ëª¨ë¸ êµ¬ì¡°:")
    logger.info(f"  - Face features: 6 â†’ 64")
    logger.info(f"  - Skin features: 2 â†’ 32")
    logger.info(f"  - Style embedding: 384")
    logger.info(f"  - Total input: 480")
    logger.info(f"  - Attention: {use_attention}")

    # í•™ìŠµ ì„¤ì •
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=5
    )

    # í•™ìŠµ ê¸°ë¡
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_mae': [],
        'lr': []
    }

    best_val_loss = float('inf')
    patience_counter = 0

    # í•™ìŠµ ì‹œì‘
    logger.info(f"\nğŸ‹ï¸  í•™ìŠµ ì‹œì‘:")
    logger.info(f"  - Epochs: {epochs}")
    logger.info(f"  - Batch size: {batch_size}")
    logger.info(f"  - Learning rate: {learning_rate}")
    logger.info(f"  - Early stopping patience: {patience}")

    for epoch in range(epochs):
        # í•™ìŠµ
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)

        # ê²€ì¦
        val_loss, val_mae = validate(model, val_loader, criterion, device)

        # í•™ìŠµë¥ 
        current_lr = optimizer.param_groups[0]['lr']

        # ê¸°ë¡
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_mae'].append(val_mae)
        history['lr'].append(current_lr)

        # ì¶œë ¥
        if (epoch + 1) % 5 == 0 or epoch == 0:
            logger.info(
                f"Epoch [{epoch+1:3d}/{epochs}] "
                f"Train Loss: {train_loss:.4f} | "
                f"Val Loss: {val_loss:.4f} | "
                f"Val MAE: {val_mae:.2f} | "
                f"LR: {current_lr:.6f}"
            )

        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(val_loss)

        # Best ëª¨ë¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0

            model_path = output_dir / "hairstyle_recommender_v4.pt"
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_loss': best_val_loss,
                'history': history,
                'config': {
                    'face_feat_dim': 6,
                    'skin_feat_dim': 2,
                    'style_embed_dim': 384,
                    'use_attention': use_attention
                }
            }, model_path)

            logger.info(f"  âœ… Best ëª¨ë¸ ì €ì¥: {model_path}")

        else:
            patience_counter += 1

        # Early stopping
        if patience_counter >= patience:
            logger.info(f"\nâ¸ï¸  Early stopping at epoch {epoch + 1}")
            break

    # ìµœì¢… í…ŒìŠ¤íŠ¸
    logger.info(f"\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€:")
    test_loss, test_mae = validate(model, test_loader, criterion, device)
    logger.info(f"  - Test Loss: {test_loss:.4f}")
    logger.info(f"  - Test MAE: {test_mae:.2f}")

    # í•™ìŠµ ê¸°ë¡ ì €ì¥
    history_path = output_dir / "training_history_v4.json"
    with open(history_path, 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=2)

    logger.info(f"\nğŸ“Š í•™ìŠµ ê¸°ë¡ ì €ì¥: {history_path}")

    logger.info("\n" + "=" * 60)
    logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
    logger.info("=" * 60)
    logger.info(f"  - Best Val Loss: {best_val_loss:.4f}")
    logger.info(f"  - Test MAE: {test_mae:.2f}")
    logger.info(f"  - ëª¨ë¸ ê²½ë¡œ: {output_dir / 'hairstyle_recommender_v4.pt'}")


def main():
    parser = argparse.ArgumentParser(description="v4 ëª¨ë¸ í•™ìŠµ")
    parser.add_argument(
        "--data",
        type=str,
        required=True,
        help="í•™ìŠµ ë°ì´í„° NPZ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="models",
        help="ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: models)"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=100,
        help="í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸: 100)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=64,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)"
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=0.001,
        help="í•™ìŠµë¥  (ê¸°ë³¸: 0.001)"
    )
    parser.add_argument(
        "--patience",
        type=int,
        default=15,
        help="Early stopping patience (ê¸°ë³¸: 15)"
    )
    parser.add_argument(
        "--no-attention",
        action="store_true",
        help="Attention ë¹„í™œì„±í™”"
    )

    args = parser.parse_args()

    train_model(
        data_path=args.data,
        output_dir=args.output_dir,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        patience=args.patience,
        use_attention=not args.no_attention
    )


if __name__ == "__main__":
    main()
