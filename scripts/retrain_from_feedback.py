#!/usr/bin/env python3
"""
ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ëª¨ë¸ ì¬í•™ìŠµ (v2.0)

Progressive Data Mixing ì „ëµ:
- í”¼ë“œë°± < 500ê°œ: 100% Gemini, 0% Feedback
- í”¼ë“œë°± < 1000ê°œ: 70% Gemini, 30% Feedback
- í”¼ë“œë°± < 2000ê°œ: 40% Gemini, 60% Feedback
- í”¼ë“œë°± < 5000ê°œ: 20% Gemini, 80% Feedback
- í”¼ë“œë°± >= 5000ê°œ: 0% Gemini, 100% Feedback

Ground Truth Rules:
- ğŸ‘ (like) â†’ 90.0 (user LIKED this combination)
- ğŸ‘ (dislike) â†’ 10.0 (user DISLIKED this combination)

Author: HairMe ML Team
Date: 2025-11-13
Version: 2.0.0
"""

import argparse
import sys
import time
from pathlib import Path
from typing import Tuple
import numpy as np
import json
from datetime import datetime

# Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split


# ==================== ì„¤ì • ====================
class Config:
    """ì¬í•™ìŠµ ì„¤ì •"""

    # ë°ì´í„° ê²½ë¡œ
    FEEDBACK_NPZ_PATH = "data/feedback_training_data.npz"
    GEMINI_NPZ_PATH = "data_source/ml_training_dataset.npz"
    MODEL_PATH = "models/hairstyle_recommender.pt"
    BACKUP_DIR = "models/backups"

    # í•™ìŠµ ì„¤ì •
    BATCH_SIZE = 32
    LEARNING_RATE = 0.0001  # ì¬í•™ìŠµì€ ë‚®ì€ learning rate
    NUM_EPOCHS = 100
    EARLY_STOPPING_PATIENCE = 15

    # ë””ë°”ì´ìŠ¤
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# ==================== Progressive Mixing ====================
def get_mixing_ratios(feedback_count: int) -> Tuple[float, float]:
    """
    í”¼ë“œë°± ê°œìˆ˜ì— ë”°ë¼ Gemini/Feedback ë°ì´í„° í˜¼í•© ë¹„ìœ¨ ê²°ì •

    Args:
        feedback_count: ì´ í”¼ë“œë°± ê°œìˆ˜

    Returns:
        (gemini_ratio, feedback_ratio)
    """
    if feedback_count < 500:
        phase = "Phase 1: Bootstrapping"
        gemini_ratio, feedback_ratio = 1.0, 0.0
    elif feedback_count < 1000:
        phase = "Phase 2: Initial Learning"
        gemini_ratio, feedback_ratio = 0.7, 0.3
    elif feedback_count < 2000:
        phase = "Phase 3: Balanced Learning"
        gemini_ratio, feedback_ratio = 0.4, 0.6
    elif feedback_count < 5000:
        phase = "Phase 4: User-Driven Learning"
        gemini_ratio, feedback_ratio = 0.2, 0.8
    else:
        phase = "Phase 5: Pure Feedback"
        gemini_ratio, feedback_ratio = 0.0, 1.0

    print(f"\nğŸ“Š {phase}")
    print(f"  - Gemini ë°ì´í„°: {gemini_ratio*100:.0f}%")
    print(f"  - Feedback ë°ì´í„°: {feedback_ratio*100:.0f}%")

    return gemini_ratio, feedback_ratio


def prepare_training_data(feedback_count: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (Progressive Mixing)

    Args:
        feedback_count: í”¼ë“œë°± ê°œìˆ˜ (ì´ë¯¸ ì•Œê³  ìˆëŠ” ê°’)

    Returns:
        (X, y) - íŠ¹ì§• í–‰ë ¬, íƒ€ê²Ÿ ë²¡í„°
    """
    print("\n" + "="*60)
    print("ğŸ“‚ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    print("="*60)

    # 1. Mixing ë¹„ìœ¨ ê²°ì •
    gemini_ratio, feedback_ratio = get_mixing_ratios(feedback_count)

    X_list = []
    y_list = []

    # 2. Gemini ë°ì´í„° ë¡œë“œ (í•„ìš” ì‹œ)
    if gemini_ratio > 0:
        try:
            gemini_data = np.load(Config.GEMINI_NPZ_PATH)
            gemini_X = gemini_data['X_train']
            gemini_y = gemini_data['y_train']

            # Gemini ë°ì´í„° ìƒ˜í”Œë§
            gemini_sample_size = int(len(gemini_X) * gemini_ratio)
            gemini_indices = np.random.choice(
                len(gemini_X),
                gemini_sample_size,
                replace=False
            )

            X_list.append(gemini_X[gemini_indices])
            y_list.append(gemini_y[gemini_indices])

            print(f"\nâœ… Gemini ë°ì´í„° ë¡œë“œ: {len(gemini_X)}ê°œ â†’ {gemini_sample_size}ê°œ ìƒ˜í”Œë§")

        except FileNotFoundError:
            print(f"\nâš ï¸  Gemini ë°ì´í„° íŒŒì¼ ì—†ìŒ: {Config.GEMINI_NPZ_PATH}")

    # 3. Feedback ë°ì´í„° ë¡œë“œ (í•„ìš” ì‹œ)
    if feedback_ratio > 0:
        try:
            feedback_data = np.load(Config.FEEDBACK_NPZ_PATH)
            feedback_X = feedback_data['X']
            feedback_y = feedback_data['y']

            X_list.append(feedback_X)
            y_list.append(feedback_y)

            print(f"âœ… Feedback ë°ì´í„° ë¡œë“œ: {len(feedback_X)}ê°œ")

        except FileNotFoundError:
            print(f"\nâš ï¸  Feedback ë°ì´í„° íŒŒì¼ ì—†ìŒ: {Config.FEEDBACK_NPZ_PATH}")

    # 4. ë°ì´í„° ë³‘í•©
    if not X_list:
        raise ValueError("í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

    X_combined = np.vstack(X_list)
    y_combined = np.concatenate(y_list)

    print(f"\nâœ… ìµœì¢… ë°ì´í„°: {len(X_combined)}ê°œ")
    print(f"  - íŠ¹ì§• ì°¨ì›: {X_combined.shape[1]}")
    print(f"  - ì ìˆ˜ ë²”ìœ„: {y_combined.min():.1f} ~ {y_combined.max():.1f}")

    return X_combined, y_combined


# ==================== ëª¨ë¸ ì •ì˜ ====================
class RecommendationModel(nn.Module):
    """PyTorch ì¶”ì²œ ëª¨ë¸"""

    def __init__(self, input_dim: int = 392):
        super(RecommendationModel, self).__init__()

        hidden_dims = [256, 128, 64]
        dropout_rates = [0.3, 0.2, 0.0]

        layers = []
        prev_dim = input_dim

        for hidden_dim, dropout_rate in zip(hidden_dims, dropout_rates):
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())

            if dropout_rate > 0:
                layers.append(nn.Dropout(dropout_rate))

            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))

        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)


class HairstyleDataset(Dataset):
    """PyTorch Dataset"""

    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y).unsqueeze(1)

    def __len__(self) -> int:
        return len(self.X)

    def __getitem__(self, idx: int):
        return self.X[idx], self.y[idx]


# ==================== í•™ìŠµ í•¨ìˆ˜ ====================
def train_model(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    model_path: str,
    num_epochs: int = 100
) -> nn.Module:
    """
    ëª¨ë¸ ì¬í•™ìŠµ

    Args:
        X_train, y_train: í•™ìŠµ ë°ì´í„°
        X_val, y_val: ê²€ì¦ ë°ì´í„°
        model_path: ê¸°ì¡´ ëª¨ë¸ ê²½ë¡œ
        num_epochs: ì—í­ ìˆ˜

    Returns:
        í•™ìŠµëœ ëª¨ë¸
    """
    print("\n" + "="*60)
    print("ğŸš€ ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘")
    print("="*60)

    device = Config.DEVICE
    print(f"ë””ë°”ì´ìŠ¤: {device}")

    # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
    model = RecommendationModel()

    try:
        model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))
        print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {model_path}")
    except FileNotFoundError:
        print(f"âš ï¸  ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ, ìƒˆë¡œ ì´ˆê¸°í™”")

    model.to(device)

    # ë°ì´í„°ì…‹
    train_dataset = HairstyleDataset(X_train, y_train)
    val_dataset = HairstyleDataset(X_val, y_val)

    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)

    # ì˜µí‹°ë§ˆì´ì €
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)

    best_val_loss = float('inf')
    patience_counter = 0

    print(f"\ní•™ìŠµ ì„¤ì •:")
    print(f"  - Epochs: {num_epochs}")
    print(f"  - Batch Size: {Config.BATCH_SIZE}")
    print(f"  - Learning Rate: {Config.LEARNING_RATE}")
    print(f"  - Early Stopping Patience: {Config.EARLY_STOPPING_PATIENCE}")

    print("\ní•™ìŠµ ì‹œì‘...\n")

    for epoch in range(num_epochs):
        # Train
        model.train()
        train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)

        # Validate
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                predictions = model(X_batch)
                loss = criterion(predictions, y_batch)
                val_loss += loss.item()

        val_loss /= len(val_loader)

        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # Early Stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= Config.EARLY_STOPPING_PATIENCE:
            print(f"\nâš ï¸  Early Stopping at epoch {epoch+1}")
            break

    print(f"\nâœ… ì¬í•™ìŠµ ì™„ë£Œ! ìµœì¢… Val Loss: {val_loss:.4f}")

    return model


# ==================== ë©”ì¸ í•¨ìˆ˜ ====================
def main():
    """ë©”ì¸ ì‹¤í–‰"""

    parser = argparse.ArgumentParser(
        description="ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ëª¨ë¸ ì¬í•™ìŠµ (Progressive Mixing v2.0)"
    )
    parser.add_argument(
        '--feedback-count',
        type=int,
        required=True,
        help='í˜„ì¬ í”¼ë“œë°± ê°œìˆ˜ (500, 1000, 2000, 5000)'
    )
    parser.add_argument(
        '--save-model',
        action='store_true',
        help='í•™ìŠµ í›„ ëª¨ë¸ ì €ì¥'
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=Config.NUM_EPOCHS,
        help=f'ì—í­ ìˆ˜ (ê¸°ë³¸ê°’: {Config.NUM_EPOCHS})'
    )

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ”„ MLOps ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ v2.0")
    print("   (Progressive Data Mixing Strategy)")
    print("=" * 60)
    print(f"í”¼ë“œë°± ê°œìˆ˜: {args.feedback_count}")
    print(f"ëª¨ë¸ ì €ì¥: {'ì˜ˆ' if args.save_model else 'ì•„ë‹ˆì˜¤'}")
    print("=" * 60)

    try:
        # 1. í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (Progressive Mixing)
        X_combined, y_combined = prepare_training_data(args.feedback_count)

        # 2. Train/Val Split
        X_train, X_val, y_train, y_val = train_test_split(
            X_combined, y_combined,
            test_size=0.2,
            random_state=42
        )

        print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"  - Train: {len(X_train)}ê°œ")
        print(f"  - Val: {len(X_val)}ê°œ")

        # 3. ê¸°ì¡´ ëª¨ë¸ ë°±ì—… (ì €ì¥ ëª¨ë“œì¼ ë•Œë§Œ)
        if args.save_model:
            backup_dir = Path(Config.BACKUP_DIR)
            backup_dir.mkdir(parents=True, exist_ok=True)

            timestamp = time.strftime("%Y%m%d_%H%M%S")
            feedback_count = args.feedback_count
            backup_filename = f"model_backup_{feedback_count}_{timestamp}.pt"
            backup_path = backup_dir / backup_filename

            try:
                import shutil
                shutil.copy(Config.MODEL_PATH, backup_path)
                print(f"\nğŸ’¾ ê¸°ì¡´ ëª¨ë¸ ë°±ì—…: {backup_path}")
            except FileNotFoundError:
                print(f"\nâš ï¸  ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ, ë°±ì—… ìƒëµ")

        # 4. ì¬í•™ìŠµ
        new_model = train_model(
            X_train, y_train,
            X_val, y_val,
            Config.MODEL_PATH,
            args.epochs
        )

        # 5. ìƒˆ ëª¨ë¸ ì €ì¥
        if args.save_model:
            new_model_path = Config.MODEL_PATH.replace(
                '.pt',
                f'_v2_{args.feedback_count}.pt'
            )
            torch.save(new_model.state_dict(), new_model_path)
            print(f"\nâœ… ìƒˆ ëª¨ë¸ ì €ì¥: {new_model_path}")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "version": "2.0",
                "feedback_count": args.feedback_count,
                "train_size": len(X_train),
                "val_size": len(X_val),
                "timestamp": datetime.now().isoformat()
            }

            metadata_path = new_model_path.replace('.pt', '_metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

        print("\n" + "=" * 60)
        print("ğŸ‰ ì¬í•™ìŠµ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ê²°ê³¼:")
        print(f"  - í”¼ë“œë°± ê°œìˆ˜: {args.feedback_count}")
        print(f"  - í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ")
        print(f"  - ê²€ì¦ ë°ì´í„°: {len(X_val)}ê°œ")
        if args.save_model:
            print(f"  - ì €ì¥ ê²½ë¡œ: {new_model_path}")
        print("=" * 60)

        return 0

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
