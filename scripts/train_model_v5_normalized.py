#!/usr/bin/env python3
"""
v5 ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ - ë¼ë²¨ ì •ê·œí™” ë²„ì „

**í•µì‹¬ ê°œì„ :**
- ë¼ë²¨(ì ìˆ˜)ì„ 0~1ë¡œ ì •ê·œí™” í›„ í•™ìŠµ
- ëª¨ë¸ ì¶œë ¥ì¸µì— Sigmoid í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš© (0~1 ì¶œë ¥ ë³´ì¥)
- ì¶”ë¡  ì‹œ ì—­ë³€í™˜ìœ¼ë¡œ ì›ë˜ ìŠ¤ì¼€ì¼ ë³µì› (0~1 â†’ 10~95)

**ë¼ë²¨ ì •ê·œí™”:**
- ì›ë³¸ ì ìˆ˜ ë²”ìœ„: 10~95ì 
- normalized_label = (label - 10) / 85
- ì—­ë³€í™˜: final_score = model_output * 85 + 10

Author: HairMe ML Team
Date: 2025-11-27
"""

import os
import sys
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from pathlib import Path
import logging
from typing import Dict, Tuple, List
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from sentence_transformers import SentenceTransformer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

# ========== ë¼ë²¨ ì •ê·œí™” ìƒìˆ˜ ==========
LABEL_MIN = 10.0   # ì›ë³¸ ì ìˆ˜ ìµœì†Œê°’
LABEL_MAX = 95.0   # ì›ë³¸ ì ìˆ˜ ìµœëŒ€ê°’
LABEL_RANGE = LABEL_MAX - LABEL_MIN  # 85


def normalize_score(score: np.ndarray) -> np.ndarray:
    """ì›ë³¸ ì ìˆ˜ë¥¼ 0~1ë¡œ ì •ê·œí™”"""
    return (score - LABEL_MIN) / LABEL_RANGE


def denormalize_score(normalized: np.ndarray) -> np.ndarray:
    """0~1 ì ìˆ˜ë¥¼ ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ì—­ë³€í™˜"""
    return normalized * LABEL_RANGE + LABEL_MIN


# ========== ëª¨ë¸ ì •ì˜ ==========
class AttentionLayer(nn.Module):
    """Multi-head self-attention layer"""

    def __init__(self, embed_dim: int, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.norm = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        attn_out, _ = self.attention(x, x, x)
        x = self.norm(x + self.dropout(attn_out))
        return x


class NormalizedRecommenderV5(nn.Module):
    """
    ì •ê·œí™”ëœ ë¼ë²¨ ê¸°ë°˜ ì¶”ì²œ ëª¨ë¸ v5

    í•µì‹¬ íŠ¹ì§•:
    - ì¶œë ¥ì¸µì— Sigmoid í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš© (0~1 ì¶œë ¥ ë³´ì¥)
    - í•™ìŠµ ì‹œ ë¼ë²¨ì„ 0~1ë¡œ ì •ê·œí™”
    - ì¶”ë¡  ì‹œ ì—­ë³€í™˜ í•„ìš” (0~1 â†’ 10~95)

    ì…ë ¥:
    - face_features: [batch, 6] - MediaPipe ì–¼êµ´ ì¸¡ì •ê°’
    - skin_features: [batch, 2] - MediaPipe í”¼ë¶€ ì¸¡ì •ê°’
    - style_emb: [batch, 384] - í—¤ì–´ìŠ¤íƒ€ì¼ ì„ë² ë”©
    """

    def __init__(
        self,
        face_feat_dim: int = 6,
        skin_feat_dim: int = 2,
        style_embed_dim: int = 384,
        use_attention: bool = True,
        dropout_rate: float = 0.3
    ):
        super().__init__()

        self.face_feat_dim = face_feat_dim
        self.skin_feat_dim = skin_feat_dim
        self.style_embed_dim = style_embed_dim

        # Input projection layers
        self.face_projection = nn.Sequential(
            nn.Linear(face_feat_dim, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5)
        )

        self.skin_projection = nn.Sequential(
            nn.Linear(skin_feat_dim, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5)
        )

        # Total dimension after projection
        self.total_dim = 64 + 32 + style_embed_dim  # 480

        # Attention layer
        self.use_attention = use_attention
        if use_attention:
            self.attention = AttentionLayer(
                embed_dim=self.total_dim,
                num_heads=8,
                dropout=0.1
            )

        # Feature fusion network
        self.fc1 = nn.Linear(self.total_dim, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.dropout1 = nn.Dropout(dropout_rate)

        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)
        self.dropout2 = nn.Dropout(dropout_rate * 0.7)

        # Residual connection
        self.residual_proj = nn.Linear(self.total_dim, 128)

        self.fc3 = nn.Linear(128, 64)
        self.bn3 = nn.BatchNorm1d(64)
        self.dropout3 = nn.Dropout(dropout_rate * 0.5)

        self.fc4 = nn.Linear(64, 32)
        self.fc_out = nn.Linear(32, 1)

        # Sigmoid í™œì„±í™” í•¨ìˆ˜ - ì¶œë ¥ì„ 0~1ë¡œ ì œí•œ
        self.sigmoid = nn.Sigmoid()

    def forward(
        self,
        face_features: torch.Tensor,
        skin_features: torch.Tensor,
        style_emb: torch.Tensor
    ) -> torch.Tensor:
        """Forward pass - ì¶œë ¥ ë²”ìœ„: 0~1 (Sigmoid)"""
        # Project features
        face_proj = self.face_projection(face_features)
        skin_proj = self.skin_projection(skin_features)

        # Concatenate all features
        x = torch.cat([face_proj, skin_proj, style_emb], dim=1)

        # Apply attention if enabled
        if self.use_attention:
            x_att = x.unsqueeze(1)
            x_att = self.attention(x_att)
            x = x_att.squeeze(1)

        # Store for residual
        residual = self.residual_proj(x)

        # Main network
        x = self.fc1(x)
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.dropout1(x)

        x = self.fc2(x)
        x = self.bn2(x)
        x = torch.relu(x)
        x = self.dropout2(x)

        # Add residual connection
        x = x + residual

        x = self.fc3(x)
        x = self.bn3(x)
        x = torch.relu(x)
        x = self.dropout3(x)

        x = self.fc4(x)
        x = torch.relu(x)

        x = self.fc_out(x)

        # Sigmoidë¡œ 0~1 ì¶œë ¥ ë³´ì¥
        x = self.sigmoid(x)

        return x.squeeze(-1)


# ========== ë°ì´í„°ì…‹ ==========
class HairstyleDatasetV5(Dataset):
    """ì •ê·œí™”ëœ ë¼ë²¨ ê¸°ë°˜ í—¤ì–´ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹"""

    def __init__(
        self,
        face_features: np.ndarray,
        skin_features: np.ndarray,
        style_embeddings: np.ndarray,
        scores: np.ndarray,
        normalize_labels: bool = True
    ):
        """
        Args:
            face_features: [N, 6]
            skin_features: [N, 2]
            style_embeddings: [N, 384]
            scores: [N] - ì›ë³¸ ì ìˆ˜ (10~95)
            normalize_labels: ë¼ë²¨ ì •ê·œí™” ì—¬ë¶€
        """
        self.face_features = torch.tensor(face_features, dtype=torch.float32)
        self.skin_features = torch.tensor(skin_features, dtype=torch.float32)
        self.style_embeddings = torch.tensor(style_embeddings, dtype=torch.float32)

        # ë¼ë²¨ ì •ê·œí™”
        if normalize_labels:
            normalized_scores = normalize_score(scores)
            self.scores = torch.tensor(normalized_scores, dtype=torch.float32).unsqueeze(1)
            logger.info(f"  ë¼ë²¨ ì •ê·œí™” ì ìš©: {scores.min():.1f}~{scores.max():.1f} â†’ {normalized_scores.min():.3f}~{normalized_scores.max():.3f}")
        else:
            self.scores = torch.tensor(scores, dtype=torch.float32).unsqueeze(1)

    def __len__(self):
        return len(self.scores)

    def __getitem__(self, idx):
        return (
            self.face_features[idx],
            self.skin_features[idx],
            self.style_embeddings[idx],
            self.scores[idx]
        )


def load_training_data(data_path: str) -> Dict:
    """NPZ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {data_path}")

    # allow_pickle=True: hairstyles í•„ë“œê°€ ë¬¸ìì—´ ë°°ì—´ì´ë¯€ë¡œ í•„ìš”
    data = np.load(data_path, allow_pickle=True)

    face_features = data['face_features']  # [N, 6]
    skin_features = data['skin_features']  # [N, 2]
    hairstyles = data['hairstyles']  # [N]
    scores = data['scores']  # [N]

    logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    logger.info(f"  - ìƒ˜í”Œ ìˆ˜: {len(scores):,}")
    logger.info(f"  - Face features: {face_features.shape}")
    logger.info(f"  - Skin features: {skin_features.shape}")
    logger.info(f"  - Hairstyles: {len(hairstyles)}")

    # í—¤ì–´ìŠ¤íƒ€ì¼ ì„ë² ë”© ìƒì„±
    logger.info("ğŸ”„ í—¤ì–´ìŠ¤íƒ€ì¼ ì„ë² ë”© ìƒì„± ì¤‘...")
    sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    style_embeddings = sentence_model.encode(
        hairstyles.tolist(),
        show_progress_bar=True,
        convert_to_numpy=True
    )

    logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {style_embeddings.shape}")

    # í†µê³„
    logger.info(f"\nğŸ“Š ì›ë³¸ ë°ì´í„° í†µê³„:")
    logger.info(f"  - ì ìˆ˜ ë²”ìœ„: {scores.min():.1f} ~ {scores.max():.1f}")
    logger.info(f"  - ì ìˆ˜ í‰ê· : {scores.mean():.1f} Â± {scores.std():.1f}")
    logger.info(f"  - ê³ ì ìˆ˜ (â‰¥75): {(scores >= 75).sum():,}ê°œ ({(scores >= 75).sum()/len(scores)*100:.1f}%)")
    logger.info(f"  - ì €ì ìˆ˜ (â‰¤40): {(scores <= 40).sum():,}ê°œ ({(scores <= 40).sum()/len(scores)*100:.1f}%)")

    return {
        'face_features': face_features,
        'skin_features': skin_features,
        'style_embeddings': style_embeddings,
        'scores': scores,
        'hairstyles': hairstyles
    }


def create_dataloaders_no_leakage(
    data: Dict,
    batch_size: int = 64,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    samples_per_face: int = 6
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€: ì–¼êµ´ ë‹¨ìœ„ë¡œ train/val/test ë¶„í• """

    # ë°ì´í„°ì…‹ ìƒì„± (ë¼ë²¨ ì •ê·œí™” ì ìš©)
    dataset = HairstyleDatasetV5(
        face_features=data['face_features'],
        skin_features=data['skin_features'],
        style_embeddings=data['style_embeddings'],
        scores=data['scores'],
        normalize_labels=True  # ë¼ë²¨ ì •ê·œí™” í™œì„±í™”
    )

    total_samples = len(dataset)
    num_faces = total_samples // samples_per_face

    logger.info(f"\nğŸ” ë°ì´í„° ë¦¬í‚¤ì§€ ë°©ì§€ ëª¨ë“œ:")
    logger.info(f"  - ì´ ìƒ˜í”Œ: {total_samples:,}ê°œ")
    logger.info(f"  - ì´ ì–¼êµ´: {num_faces:,}ê°œ")
    logger.info(f"  - ì–¼êµ´ë‹¹ ìƒ˜í”Œ: {samples_per_face}ê°œ")

    # ì–¼êµ´ ì¸ë±ìŠ¤ ì…”í”Œ
    np.random.seed(42)
    face_indices = np.random.permutation(num_faces)

    # ì–¼êµ´ ë‹¨ìœ„ë¡œ ë¶„í• 
    num_test_faces = int(num_faces * test_ratio)
    num_val_faces = int(num_faces * val_ratio)
    num_train_faces = num_faces - num_test_faces - num_val_faces

    train_face_indices = face_indices[:num_train_faces]
    val_face_indices = face_indices[num_train_faces:num_train_faces + num_val_faces]
    test_face_indices = face_indices[num_train_faces + num_val_faces:]

    logger.info(f"\nğŸ“Š ì–¼êµ´ ë‹¨ìœ„ ë¶„í• :")
    logger.info(f"  - Train: {num_train_faces:,}ê°œ ì–¼êµ´ ({num_train_faces/num_faces*100:.1f}%)")
    logger.info(f"  - Val:   {num_val_faces:,}ê°œ ì–¼êµ´ ({num_val_faces/num_faces*100:.1f}%)")
    logger.info(f"  - Test:  {num_test_faces:,}ê°œ ì–¼êµ´ ({num_test_faces/num_faces*100:.1f}%)")

    # ì–¼êµ´ ì¸ë±ìŠ¤ â†’ ìƒ˜í”Œ ì¸ë±ìŠ¤ ë³€í™˜
    def face_indices_to_sample_indices(face_idxs: np.ndarray) -> List[int]:
        sample_idxs = []
        for face_idx in face_idxs:
            start = face_idx * samples_per_face
            end = start + samples_per_face
            sample_idxs.extend(range(start, end))
        return sample_idxs

    train_sample_indices = face_indices_to_sample_indices(train_face_indices)
    val_sample_indices = face_indices_to_sample_indices(val_face_indices)
    test_sample_indices = face_indices_to_sample_indices(test_face_indices)

    # Subset ìƒì„±
    train_dataset = Subset(dataset, train_sample_indices)
    val_dataset = Subset(dataset, val_sample_indices)
    test_dataset = Subset(dataset, test_sample_indices)

    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )

    return train_loader, val_loader, test_loader


def train_epoch(
    model: nn.Module,
    train_loader: DataLoader,
    optimizer: optim.Optimizer,
    criterion: nn.Module,
    device: torch.device
) -> float:
    """1 ì—í­ í•™ìŠµ"""
    model.train()
    total_loss = 0.0

    for face_feat, skin_feat, style_emb, scores in train_loader:
        face_feat = face_feat.to(device)
        skin_feat = skin_feat.to(device)
        style_emb = style_emb.to(device)
        scores = scores.to(device)

        optimizer.zero_grad()
        pred_scores = model(face_feat, skin_feat, style_emb)

        # ì†ì‹¤ ê³„ì‚° (ì •ê·œí™”ëœ ìŠ¤ì¼€ì¼ì—ì„œ)
        loss = criterion(pred_scores.unsqueeze(1), scores)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    return avg_loss


def validate(
    model: nn.Module,
    val_loader: DataLoader,
    criterion: nn.Module,
    device: torch.device
) -> Tuple[float, float, float]:
    """
    ê²€ì¦ - ì •ê·œí™”ëœ ìŠ¤ì¼€ì¼ê³¼ ì›ë³¸ ìŠ¤ì¼€ì¼ ëª¨ë‘ì—ì„œ MAE ê³„ì‚°

    Returns:
        (avg_loss, normalized_mae, original_scale_mae)
    """
    model.eval()
    total_loss = 0.0
    total_normalized_mae = 0.0
    total_original_mae = 0.0

    with torch.no_grad():
        for face_feat, skin_feat, style_emb, scores in val_loader:
            face_feat = face_feat.to(device)
            skin_feat = skin_feat.to(device)
            style_emb = style_emb.to(device)
            scores = scores.to(device)

            pred_scores = model(face_feat, skin_feat, style_emb)

            # ì •ê·œí™”ëœ ìŠ¤ì¼€ì¼ì—ì„œì˜ ì†ì‹¤ê³¼ MAE
            loss = criterion(pred_scores.unsqueeze(1), scores)
            normalized_mae = torch.abs(pred_scores.unsqueeze(1) - scores).mean()

            # ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ì—­ë³€í™˜ í›„ MAE
            pred_original = pred_scores.unsqueeze(1) * LABEL_RANGE + LABEL_MIN
            scores_original = scores * LABEL_RANGE + LABEL_MIN
            original_mae = torch.abs(pred_original - scores_original).mean()

            total_loss += loss.item()
            total_normalized_mae += normalized_mae.item()
            total_original_mae += original_mae.item()

    num_batches = len(val_loader)
    return (
        total_loss / num_batches,
        total_normalized_mae / num_batches,
        total_original_mae / num_batches
    )


def train_model(
    data_path: str,
    output_dir: str = "models",
    epochs: int = 100,
    batch_size: int = 64,
    learning_rate: float = 0.001,
    patience: int = 15,
    use_attention: bool = True
):
    """ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜ (ë¼ë²¨ ì •ê·œí™” ì ìš©)"""
    logger.info("=" * 60)
    logger.info("ğŸš€ v5 ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë¼ë²¨ ì •ê·œí™”)")
    logger.info("=" * 60)
    logger.info(f"  - ë¼ë²¨ ì •ê·œí™”: {LABEL_MIN}~{LABEL_MAX} â†’ 0~1")
    logger.info(f"  - ì¶œë ¥ì¸µ: Sigmoid (0~1 ë³´ì¥)")

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {device}")

    # ë°ì´í„° ë¡œë“œ
    data = load_training_data(data_path)

    # ë°ì´í„°ë¡œë” ìƒì„± (ë¼ë²¨ ì •ê·œí™” í¬í•¨)
    train_loader, val_loader, test_loader = create_dataloaders_no_leakage(
        data,
        batch_size=batch_size
    )

    # ëª¨ë¸ ìƒì„± (Sigmoid ì¶œë ¥ì¸µ)
    model = NormalizedRecommenderV5(
        face_feat_dim=6,
        skin_feat_dim=2,
        style_embed_dim=384,
        use_attention=use_attention,
        dropout_rate=0.3
    ).to(device)

    logger.info(f"\nğŸ—ï¸  ëª¨ë¸ êµ¬ì¡° (v5 - ì •ê·œí™”):")
    logger.info(f"  - Face features: 6 â†’ 64")
    logger.info(f"  - Skin features: 2 â†’ 32")
    logger.info(f"  - Style embedding: 384")
    logger.info(f"  - Total input: 480")
    logger.info(f"  - Attention: {use_attention}")
    logger.info(f"  - ì¶œë ¥ì¸µ: Sigmoid (0~1)")

    # í•™ìŠµ ì„¤ì •
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=5
    )

    # í•™ìŠµ ê¸°ë¡
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_mae_normalized': [],
        'val_mae_original': [],
        'lr': []
    }

    best_val_loss = float('inf')
    patience_counter = 0

    logger.info(f"\nğŸ‹ï¸  í•™ìŠµ ì‹œì‘:")
    logger.info(f"  - Epochs: {epochs}")
    logger.info(f"  - Batch size: {batch_size}")
    logger.info(f"  - Learning rate: {learning_rate}")
    logger.info(f"  - Early stopping patience: {patience}")

    for epoch in range(epochs):
        # í•™ìŠµ
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)

        # ê²€ì¦
        val_loss, val_mae_norm, val_mae_orig = validate(model, val_loader, criterion, device)

        current_lr = optimizer.param_groups[0]['lr']

        # ê¸°ë¡
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_mae_normalized'].append(val_mae_norm)
        history['val_mae_original'].append(val_mae_orig)
        history['lr'].append(current_lr)

        # ì¶œë ¥
        if (epoch + 1) % 5 == 0 or epoch == 0:
            logger.info(
                f"Epoch [{epoch+1:3d}/{epochs}] "
                f"Train Loss: {train_loss:.4f} | "
                f"Val Loss: {val_loss:.4f} | "
                f"MAE(orig): {val_mae_orig:.2f} | "
                f"LR: {current_lr:.6f}"
            )

        scheduler.step(val_loss)

        # Best ëª¨ë¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0

            model_path = output_dir / "hairstyle_recommender_v5_normalized.pt"
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_loss': best_val_loss,
                'history': history,
                'config': {
                    'face_feat_dim': 6,
                    'skin_feat_dim': 2,
                    'style_embed_dim': 384,
                    'use_attention': use_attention,
                    'normalized': True,
                    'label_min': LABEL_MIN,
                    'label_max': LABEL_MAX,
                    'label_range': LABEL_RANGE
                }
            }, model_path)

            logger.info(f"  âœ… Best ëª¨ë¸ ì €ì¥: {model_path}")

        else:
            patience_counter += 1

        if patience_counter >= patience:
            logger.info(f"\nâ¸ï¸  Early stopping at epoch {epoch + 1}")
            break

    # ìµœì¢… í…ŒìŠ¤íŠ¸
    logger.info(f"\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€:")
    test_loss, test_mae_norm, test_mae_orig = validate(model, test_loader, criterion, device)
    logger.info(f"  - Test Loss: {test_loss:.4f}")
    logger.info(f"  - Test MAE (ì •ê·œí™”): {test_mae_norm:.4f}")
    logger.info(f"  - Test MAE (ì›ë³¸ ìŠ¤ì¼€ì¼): {test_mae_orig:.2f}ì ")

    # í•™ìŠµ ê¸°ë¡ ì €ì¥
    history_path = output_dir / "training_history_v5_normalized.json"
    with open(history_path, 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=2)

    logger.info(f"\nğŸ“Š í•™ìŠµ ê¸°ë¡ ì €ì¥: {history_path}")

    # ëª¨ë¸ ì¶œë ¥ ë²”ìœ„ ê²€ì¦
    logger.info(f"\nğŸ” ëª¨ë¸ ì¶œë ¥ ë²”ìœ„ ê²€ì¦:")
    model.eval()
    with torch.no_grad():
        all_preds = []
        all_labels = []
        for face_feat, skin_feat, style_emb, scores in test_loader:
            face_feat = face_feat.to(device)
            skin_feat = skin_feat.to(device)
            style_emb = style_emb.to(device)

            pred = model(face_feat, skin_feat, style_emb)
            all_preds.append(pred.cpu().numpy())
            all_labels.append(scores.cpu().numpy())

        all_preds = np.concatenate(all_preds)
        all_labels = np.concatenate(all_labels).flatten()

        # ì •ê·œí™”ëœ ì¶œë ¥ ë²”ìœ„
        logger.info(f"  - ì •ê·œí™”ëœ ì¶œë ¥ ë²”ìœ„: {all_preds.min():.4f} ~ {all_preds.max():.4f}")
        logger.info(f"  - ì •ê·œí™”ëœ ë¼ë²¨ ë²”ìœ„: {all_labels.min():.4f} ~ {all_labels.max():.4f}")

        # ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ì—­ë³€í™˜
        preds_orig = all_preds * LABEL_RANGE + LABEL_MIN
        labels_orig = all_labels * LABEL_RANGE + LABEL_MIN

        logger.info(f"  - ì›ë³¸ ìŠ¤ì¼€ì¼ ì¶œë ¥ ë²”ìœ„: {preds_orig.min():.1f} ~ {preds_orig.max():.1f}")
        logger.info(f"  - ì›ë³¸ ìŠ¤ì¼€ì¼ ë¼ë²¨ ë²”ìœ„: {labels_orig.min():.1f} ~ {labels_orig.max():.1f}")

        # ì¶”ì²œ/ë¹„ì¶”ì²œ ë¶„ë¥˜ ì •í™•ë„
        high_threshold = normalize_score(np.array([75.0]))[0]  # 75ì  â†’ ì •ê·œí™”
        low_threshold = normalize_score(np.array([40.0]))[0]   # 40ì  â†’ ì •ê·œí™”

        high_labels = all_labels >= high_threshold
        low_labels = all_labels <= low_threshold

        high_correct = (all_preds[high_labels] >= high_threshold).sum()
        low_correct = (all_preds[low_labels] <= low_threshold).sum()

        logger.info(f"\nğŸ“Š ë¶„ë¥˜ ì •í™•ë„:")
        logger.info(f"  - ê³ ì ìˆ˜(â‰¥75ì ) ì˜ˆì¸¡ ì •í™•ë„: {high_correct}/{high_labels.sum()} ({high_correct/high_labels.sum()*100:.1f}%)")
        logger.info(f"  - ì €ì ìˆ˜(â‰¤40ì ) ì˜ˆì¸¡ ì •í™•ë„: {low_correct}/{low_labels.sum()} ({low_correct/low_labels.sum()*100:.1f}%)")

    logger.info("\n" + "=" * 60)
    logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
    logger.info("=" * 60)
    logger.info(f"  - Best Val Loss: {best_val_loss:.4f}")
    logger.info(f"  - Test MAE: {test_mae_orig:.2f}ì  (ì›ë³¸ ìŠ¤ì¼€ì¼)")
    logger.info(f"  - ëª¨ë¸ ê²½ë¡œ: {output_dir / 'hairstyle_recommender_v5_normalized.pt'}")


def main():
    parser = argparse.ArgumentParser(description="v5 ëª¨ë¸ í•™ìŠµ (ë¼ë²¨ ì •ê·œí™”)")
    parser.add_argument(
        "--data",
        type=str,
        default="data_source/ai_face_1000.npz",
        help="í•™ìŠµ ë°ì´í„° NPZ ê²½ë¡œ (ê¸°ë³¸: data_source/ai_face_1000.npz)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="models",
        help="ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: models)"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=100,
        help="í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸: 100)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=64,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)"
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=0.001,
        help="í•™ìŠµë¥  (ê¸°ë³¸: 0.001)"
    )
    parser.add_argument(
        "--patience",
        type=int,
        default=15,
        help="Early stopping patience (ê¸°ë³¸: 15)"
    )
    parser.add_argument(
        "--no-attention",
        action="store_true",
        help="Attention ë¹„í™œì„±í™”"
    )

    args = parser.parse_args()

    train_model(
        data_path=args.data,
        output_dir=args.output_dir,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        patience=args.patience,
        use_attention=not args.no_attention
    )


if __name__ == "__main__":
    main()
