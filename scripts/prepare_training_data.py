#!/usr/bin/env python3
"""
ML í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬

í•©ì„± ë°ì´í„°ì™€ ì„ë² ë”©ì„ ê²°í•©í•˜ì—¬ ML ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±

Input features (392ì°¨ì›):
  - ì–¼êµ´í˜• one-hot (4ì°¨ì›)
  - í”¼ë¶€í†¤ one-hot (4ì°¨ì›)
  - í—¤ì–´ìŠ¤íƒ€ì¼ ì„ë² ë”© (384ì°¨ì›)

Target:
  - ì¶”ì²œ ì ìˆ˜ (0-100)

Author: HairMe ML Team
Date: 2025-11-08
Version: 1.0.0
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from utils.style_preprocessor import normalize_style_name


# ==================== ì„¤ì • ====================
class Config:
    """ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •"""

    # ì¹´í…Œê³ ë¦¬ ì˜µì…˜ (ìˆœì„œ ì¤‘ìš”!)
    FACE_SHAPES = ["ê°ì§„í˜•", "ë‘¥ê·¼í˜•", "ê¸´í˜•", "ê³„ë€í˜•"]
    SKIN_TONES = ["ê²¨ìš¸ì¿¨", "ê°€ì„ì›œ", "ë´„ì›œ", "ì—¬ë¦„ì¿¨"]

    # ì°¨ì› í¬ê¸°
    FACE_SHAPE_DIM = 4
    SKIN_TONE_DIM = 4
    EMBEDDING_DIM = 384
    TOTAL_INPUT_DIM = FACE_SHAPE_DIM + SKIN_TONE_DIM + EMBEDDING_DIM  # 392

    # Train/Val split
    VAL_RATIO = 0.2
    RANDOM_SEED = 42

    # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ
    DEFAULT_DATA_PATH = "data_source/final_training_data_3200.json"  # 3855ê°œ ì¡°í•©!
    DEFAULT_EMBEDDING_PATH = "data_source/style_embeddings.npz"
    DEFAULT_OUTPUT_DIR = "data_source"


# ==================== ë°ì´í„° ë¡œë” ====================
class DataLoader:
    """ë°ì´í„° ë° ì„ë² ë”© ë¡œë”©"""

    @staticmethod
    def load_training_data(file_path: Path) -> Dict:
        """í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“‚ í•™ìŠµ ë°ì´í„° ë¡œë”©: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"  âœ… ë¡œë“œ ì™„ë£Œ: {data['metadata']['total_combinations']}ê°œ ì¡°í•©")
        return data

    @staticmethod
    def load_embeddings(file_path: Path) -> Tuple[np.ndarray, Dict[str, int]]:
        """ìŠ¤íƒ€ì¼ ì„ë² ë”© ë¡œë“œ"""
        print(f"ğŸ“‚ ì„ë² ë”© ë¡œë”©: {file_path}")

        data = np.load(file_path, allow_pickle=True)
        styles = data['styles']
        embeddings = data['embeddings']

        # ìŠ¤íƒ€ì¼ëª… -> ì¸ë±ìŠ¤ ë§¤í•‘
        style_to_idx = {style: idx for idx, style in enumerate(styles)}

        print(f"  âœ… ë¡œë“œ ì™„ë£Œ: {len(styles)}ê°œ ìŠ¤íƒ€ì¼, ì„ë² ë”© shape {embeddings.shape}")
        return embeddings, style_to_idx


# ==================== íŠ¹ì§• ë³€í™˜ ====================
class FeatureTransformer:
    """íŠ¹ì§• ë²¡í„° ë³€í™˜"""

    def __init__(self, embeddings: np.ndarray, style_to_idx: Dict[str, int]):
        """
        ì´ˆê¸°í™”

        Args:
            embeddings: ìŠ¤íƒ€ì¼ ì„ë² ë”© ë°°ì—´
            style_to_idx: ìŠ¤íƒ€ì¼ëª… -> ì¸ë±ìŠ¤ ë§¤í•‘
        """
        self.embeddings = embeddings
        self.style_to_idx = style_to_idx

    @staticmethod
    def encode_face_shape(face_shape: str) -> np.ndarray:
        """ì–¼êµ´í˜•ì„ one-hot ì¸ì½”ë”©"""
        vec = np.zeros(Config.FACE_SHAPE_DIM, dtype=np.float32)
        if face_shape in Config.FACE_SHAPES:
            idx = Config.FACE_SHAPES.index(face_shape)
            vec[idx] = 1.0
        return vec

    @staticmethod
    def encode_skin_tone(skin_tone: str) -> np.ndarray:
        """í”¼ë¶€í†¤ì„ one-hot ì¸ì½”ë”©"""
        vec = np.zeros(Config.SKIN_TONE_DIM, dtype=np.float32)
        if skin_tone in Config.SKIN_TONES:
            idx = Config.SKIN_TONES.index(skin_tone)
            vec[idx] = 1.0
        return vec

    def encode_hairstyle(self, hairstyle: str) -> np.ndarray:
        """í—¤ì–´ìŠ¤íƒ€ì¼ëª…ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ë„ì–´ì“°ê¸° ì •ê·œí™” ì ìš©)"""
        # ì •ê·œí™” ì ìš©
        normalized = normalize_style_name(hairstyle)

        if normalized in self.style_to_idx:
            idx = self.style_to_idx[normalized]
            return self.embeddings[idx].astype(np.float32)
        else:
            # ë¯¸ë“±ë¡ ìŠ¤íƒ€ì¼ì€ ì œë¡œ ë²¡í„° (ë°œìƒí•˜ë©´ ì•ˆë¨)
            print(f"  âš ï¸  ê²½ê³ : ë¯¸ë“±ë¡ ìŠ¤íƒ€ì¼ '{hairstyle}' (ì •ê·œí™”: '{normalized}')")
            return np.zeros(Config.EMBEDDING_DIM, dtype=np.float32)

    def transform_combination(self, combo: Dict) -> np.ndarray:
        """
        í•˜ë‚˜ì˜ ì¡°í•©ì„ íŠ¹ì§• ë²¡í„°ë¡œ ë³€í™˜

        Args:
            combo: ì¡°í•© ë”•ì…”ë„ˆë¦¬

        Returns:
            íŠ¹ì§• ë²¡í„° (392ì°¨ì›)
        """
        face_vec = self.encode_face_shape(combo['face_shape'])
        tone_vec = self.encode_skin_tone(combo['skin_tone'])
        style_vec = self.encode_hairstyle(combo['hairstyle'])

        # ì—°ê²°: [face(4) + tone(4) + style(384)] = 392
        feature = np.concatenate([face_vec, tone_vec, style_vec])

        return feature


# ==================== ë°ì´í„°ì…‹ ìƒì„± ====================
class DatasetBuilder:
    """í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹ êµ¬ì¶•"""

    def __init__(self, transformer: FeatureTransformer):
        """ì´ˆê¸°í™”"""
        self.transformer = transformer

    def build_dataset(self, training_data: Dict) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì „ì²´ ë°ì´í„°ì…‹ êµ¬ì¶•

        Args:
            training_data: í•™ìŠµ ë°ì´í„° ë”•ì…”ë„ˆë¦¬

        Returns:
            (X, y) - íŠ¹ì§• í–‰ë ¬, íƒ€ê²Ÿ ë²¡í„°
        """
        print(f"\nğŸ”„ ë°ì´í„°ì…‹ êµ¬ì¶• ì¤‘...")

        X_list = []
        y_list = []

        for image_data in training_data['training_data']:
            for combo in image_data['combinations']:
                # íŠ¹ì§• ë²¡í„° ìƒì„±
                feature = self.transformer.transform_combination(combo)
                X_list.append(feature)

                # íƒ€ê²Ÿ (ì¶”ì²œ ì ìˆ˜)
                score = combo['recommendation_score']
                y_list.append(score)

        X = np.array(X_list, dtype=np.float32)
        y = np.array(y_list, dtype=np.float32)

        print(f"  âœ… ë°ì´í„°ì…‹ êµ¬ì¶• ì™„ë£Œ:")
        print(f"    - ìƒ˜í”Œ ìˆ˜: {len(X)}ê°œ")
        print(f"    - íŠ¹ì§• ì°¨ì›: {X.shape[1]}ì°¨ì›")
        print(f"    - ì ìˆ˜ ë²”ìœ„: {y.min():.1f} ~ {y.max():.1f}")
        print(f"    - ì ìˆ˜ í‰ê· : {y.mean():.1f} Â± {y.std():.1f}")

        return X, y

    @staticmethod
    def split_dataset(
        X: np.ndarray,
        y: np.ndarray,
        val_ratio: float = Config.VAL_RATIO
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Train/Validation split

        Args:
            X: íŠ¹ì§• í–‰ë ¬
            y: íƒ€ê²Ÿ ë²¡í„°
            val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨

        Returns:
            (X_train, X_val, y_train, y_val)
        """
        print(f"\nğŸ“Š Train/Validation Split...")

        X_train, X_val, y_train, y_val = train_test_split(
            X, y,
            test_size=val_ratio,
            random_state=Config.RANDOM_SEED,
            shuffle=True
        )

        print(f"  âœ… Split ì™„ë£Œ:")
        print(f"    - Train: {len(X_train)}ê°œ ({100*(1-val_ratio):.0f}%)")
        print(f"    - Val:   {len(X_val)}ê°œ ({100*val_ratio:.0f}%)")

        return X_train, X_val, y_train, y_val


# ==================== ì €ì¥ ====================
class DatasetExporter:
    """ë°ì´í„°ì…‹ ì €ì¥"""

    @staticmethod
    def save_dataset(
        X_train: np.ndarray,
        X_val: np.ndarray,
        y_train: np.ndarray,
        y_val: np.ndarray,
        output_dir: Path
    ):
        """
        ë°ì´í„°ì…‹ì„ NPZ íŒŒì¼ë¡œ ì €ì¥

        Args:
            X_train, X_val, y_train, y_val: ë°ì´í„°ì…‹
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        print(f"\nğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...")

        # NPZ íŒŒì¼ë¡œ ì €ì¥
        npz_path = output_dir / "ml_training_dataset.npz"
        np.savez_compressed(
            npz_path,
            X_train=X_train,
            X_val=X_val,
            y_train=y_train,
            y_val=y_val
        )

        print(f"  âœ… NPZ ì €ì¥: {npz_path}")

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "feature_dim": Config.TOTAL_INPUT_DIM,
            "face_shape_dim": Config.FACE_SHAPE_DIM,
            "skin_tone_dim": Config.SKIN_TONE_DIM,
            "embedding_dim": Config.EMBEDDING_DIM,
            "face_shapes": Config.FACE_SHAPES,
            "skin_tones": Config.SKIN_TONES,
            "train_size": int(len(X_train)),
            "val_size": int(len(X_val)),
            "target_min": float(y_train.min()),
            "target_max": float(y_train.max())
        }

        json_path = output_dir / "ml_dataset_metadata.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"  âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {json_path}")

        # íŒŒì¼ í¬ê¸°
        npz_size = npz_path.stat().st_size / 1024
        print(f"\n  ğŸ“Š íŒŒì¼ í¬ê¸°: {npz_size:.1f} KB")

    @staticmethod
    def save_csv_sample(
        X_train: np.ndarray,
        y_train: np.ndarray,
        output_dir: Path,
        n_samples: int = 100
    ):
        """ìƒ˜í”Œ CSV ì €ì¥ (ê²€ì¦ìš©)"""
        print(f"\nğŸ’¾ ìƒ˜í”Œ CSV ì €ì¥ ì¤‘ (ì²˜ìŒ {n_samples}ê°œ)...")

        # íŠ¹ì§• ì»¬ëŸ¼ëª… ìƒì„±
        feature_cols = []
        feature_cols += [f"face_{shape}" for shape in Config.FACE_SHAPES]
        feature_cols += [f"tone_{tone}" for tone in Config.SKIN_TONES]
        feature_cols += [f"emb_{i}" for i in range(Config.EMBEDDING_DIM)]

        # DataFrame ìƒì„±
        df = pd.DataFrame(
            X_train[:n_samples],
            columns=feature_cols
        )
        df['score'] = y_train[:n_samples]

        csv_path = output_dir / "training_sample.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8')

        print(f"  âœ… CSV ì €ì¥: {csv_path}")


# ==================== ë©”ì¸ í•¨ìˆ˜ ====================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ML í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ ë° ìƒì„±"
    )

    parser.add_argument(
        '-d', '--data',
        type=str,
        default=Config.DEFAULT_DATA_PATH,
        help=f'í•™ìŠµ ë°ì´í„° ê²½ë¡œ (ê¸°ë³¸ê°’: {Config.DEFAULT_DATA_PATH})'
    )

    parser.add_argument(
        '-e', '--embeddings',
        type=str,
        default=Config.DEFAULT_EMBEDDING_PATH,
        help=f'ì„ë² ë”© íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: {Config.DEFAULT_EMBEDDING_PATH})'
    )

    parser.add_argument(
        '-o', '--output-dir',
        type=str,
        default=Config.DEFAULT_OUTPUT_DIR,
        help=f'ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: {Config.DEFAULT_OUTPUT_DIR})'
    )

    args = parser.parse_args()

    data_path = Path(args.data)
    embedding_path = Path(args.embeddings)
    output_dir = Path(args.output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("ğŸ¨ ML í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ v1.0.0")
    print("=" * 60)
    print(f"í•™ìŠµ ë°ì´í„°: {data_path}")
    print(f"ì„ë² ë”© íŒŒì¼: {embedding_path}")
    print(f"ì¶œë ¥ ê²½ë¡œ: {output_dir.absolute()}")
    print(f"íŠ¹ì§• ì°¨ì›: {Config.TOTAL_INPUT_DIM}ì°¨ì›")
    print("=" * 60)

    try:
        # 1. ë°ì´í„° ë¡œë“œ
        loader = DataLoader()
        training_data = loader.load_training_data(data_path)
        embeddings, style_to_idx = loader.load_embeddings(embedding_path)

        # 2. íŠ¹ì§• ë³€í™˜ê¸° ìƒì„±
        transformer = FeatureTransformer(embeddings, style_to_idx)

        # 3. ë°ì´í„°ì…‹ êµ¬ì¶•
        builder = DatasetBuilder(transformer)
        X, y = builder.build_dataset(training_data)

        # 4. Train/Val split
        X_train, X_val, y_train, y_val = builder.split_dataset(X, y)

        # 5. ì €ì¥
        exporter = DatasetExporter()
        exporter.save_dataset(X_train, X_val, y_train, y_val, output_dir)
        exporter.save_csv_sample(X_train, y_train, output_dir)

        print("\n" + "=" * 60)
        print("ğŸ‰ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ê²°ê³¼:")
        print(f"  - Train: {len(X_train)}ê°œ")
        print(f"  - Val: {len(X_val)}ê°œ")
        print(f"  - íŠ¹ì§• ì°¨ì›: {X.shape[1]}ì°¨ì›")
        print(f"  - ì¶œë ¥ íŒŒì¼:")
        print(f"    * {output_dir / 'ml_training_dataset.npz'}")
        print(f"    * {output_dir / 'ml_dataset_metadata.json'}")
        print(f"    * {output_dir / 'training_sample.csv'}")
        print("=" * 60)

        return 0

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
