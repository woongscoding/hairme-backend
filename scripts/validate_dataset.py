#!/usr/bin/env python3
"""
ë°ì´í„°ì…‹ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í’ˆì§ˆê³¼ ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

Author: HairMe ML Team
Date: 2025-11-15
"""

import argparse
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import logging
from typing import Dict
import json

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)


def load_dataset(data_path: str) -> Dict:
    """NPZ ë°ì´í„°ì…‹ ë¡œë“œ"""
    logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {data_path}")

    data = np.load(data_path, allow_pickle=False)

    face_features = data['face_features']  # [N, 6]
    skin_features = data['skin_features']  # [N, 2]
    hairstyles = data['hairstyles']  # [N]
    scores = data['scores']  # [N]

    # ë©”íƒ€ë°ì´í„° (ìˆëŠ” ê²½ìš°)
    metadata = data.get('metadata', None)

    logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    logger.info(f"  - ìƒ˜í”Œ ìˆ˜: {len(scores):,}")
    logger.info(f"  - Face features: {face_features.shape}")
    logger.info(f"  - Skin features: {skin_features.shape}")
    logger.info(f"  - Hairstyles: {len(hairstyles)}")

    return {
        'face_features': face_features,
        'skin_features': skin_features,
        'hairstyles': hairstyles,
        'scores': scores,
        'metadata': metadata
    }


def validate_feature_ranges(data: Dict):
    """íŠ¹ì§• ê°’ ë²”ìœ„ ê²€ì¦"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ” íŠ¹ì§• ê°’ ë²”ìœ„ ê²€ì¦")
    logger.info("=" * 60)

    face_features = data['face_features']
    skin_features = data['skin_features']

    # Face features ê²€ì¦
    face_names = [
        'face_ratio',
        'forehead_width',
        'cheekbone_width',
        'jaw_width',
        'forehead_ratio',
        'jaw_ratio'
    ]

    # ì˜ˆìƒ ë²”ìœ„ (MediaPipe ê¸°ì¤€)
    expected_ranges = {
        'face_ratio': (0.8, 1.6),
        'forehead_width': (100, 300),
        'cheekbone_width': (120, 350),
        'jaw_width': (80, 280),
        'forehead_ratio': (0.7, 1.2),
        'jaw_ratio': (0.6, 1.1)
    }

    logger.info("\nğŸ“Š Face Features ë²”ìœ„:")
    for i, name in enumerate(face_names):
        values = face_features[:, i]
        min_val, max_val = expected_ranges[name]

        out_of_range = ((values < min_val) | (values > max_val)).sum()
        out_of_range_pct = out_of_range / len(values) * 100

        logger.info(f"  {name:20s}: [{values.min():7.2f}, {values.max():7.2f}]  "
                   f"(ì˜ˆìƒ: [{min_val:7.2f}, {max_val:7.2f}])  "
                   f"ë²”ìœ„ ë°–: {out_of_range:4d} ({out_of_range_pct:.1f}%)")

    # Skin features ê²€ì¦
    logger.info("\nğŸ“Š Skin Features ë²”ìœ„:")

    ita_values = skin_features[:, 0]
    hue_values = skin_features[:, 1]

    logger.info(f"  ITA í”¼ë¶€í†¤:         [{ita_values.min():7.2f}, {ita_values.max():7.2f}]  "
               f"(ì˜ˆìƒ: [-50, 70])")
    logger.info(f"  Hue:                [{hue_values.min():7.2f}, {hue_values.max():7.2f}]  "
               f"(ì˜ˆìƒ: [0, 179])")

    # ì´ìƒì¹˜ ê°ì§€
    logger.info("\nâš ï¸  ì´ìƒì¹˜ í™•ì¸:")

    # Face ratioê°€ ë„ˆë¬´ ê·¹ë‹¨ì ì¸ ê²½ìš°
    extreme_ratio = ((face_features[:, 0] < 0.7) | (face_features[:, 0] > 1.7)).sum()
    if extreme_ratio > 0:
        logger.warning(f"  - ê·¹ë‹¨ì  face_ratio: {extreme_ratio}ê°œ")

    # ë„ˆë¹„ ê°’ì´ ë¹„ì •ìƒì ì¸ ê²½ìš°
    for i, name in enumerate(['forehead_width', 'cheekbone_width', 'jaw_width']):
        idx = i + 1
        extreme = ((face_features[:, idx] < 50) | (face_features[:, idx] > 400)).sum()
        if extreme > 0:
            logger.warning(f"  - ê·¹ë‹¨ì  {name}: {extreme}ê°œ")


def validate_score_distribution(data: Dict):
    """ì ìˆ˜ ë¶„í¬ ê²€ì¦"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š ì ìˆ˜ ë¶„í¬ ê²€ì¦")
    logger.info("=" * 60)

    scores = data['scores']

    logger.info(f"\nê¸°ë³¸ í†µê³„:")
    logger.info(f"  - í‰ê· : {scores.mean():.2f}")
    logger.info(f"  - í‘œì¤€í¸ì°¨: {scores.std():.2f}")
    logger.info(f"  - ì¤‘ì•™ê°’: {np.median(scores):.2f}")
    logger.info(f"  - ìµœì†Œê°’: {scores.min():.2f}")
    logger.info(f"  - ìµœëŒ€ê°’: {scores.max():.2f}")

    # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
    logger.info(f"\nì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬:")
    bins = [0, 20, 40, 60, 80, 100]
    labels = ['ë§¤ìš° ë‚®ìŒ (0-20)', 'ë‚®ìŒ (20-40)', 'ì¤‘ê°„ (40-60)', 'ë†’ìŒ (60-80)', 'ë§¤ìš° ë†’ìŒ (80-100)']

    for i in range(len(bins) - 1):
        count = ((scores >= bins[i]) & (scores < bins[i+1])).sum()
        pct = count / len(scores) * 100
        logger.info(f"  {labels[i]:20s}: {count:5d}ê°œ ({pct:5.1f}%)")

    # ê· í˜• ê²€ì¦
    high_scores = (scores >= 80).sum()
    low_scores = (scores < 40).sum()
    high_pct = high_scores / len(scores) * 100
    low_pct = low_scores / len(scores) * 100

    logger.info(f"\në°ì´í„° ê· í˜•:")
    logger.info(f"  - ê³ ì ìˆ˜ (â‰¥80): {high_scores:,}ê°œ ({high_pct:.1f}%)")
    logger.info(f"  - ì €ì ìˆ˜ (<40): {low_scores:,}ê°œ ({low_pct:.1f}%)")

    if high_pct < 20 or low_pct < 20:
        logger.warning(f"  âš ï¸ ë°ì´í„° ë¶ˆê· í˜• ê°ì§€! í•™ìŠµì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def validate_hairstyle_distribution(data: Dict):
    """í—¤ì–´ìŠ¤íƒ€ì¼ ë¶„í¬ ê²€ì¦"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ’‡ í—¤ì–´ìŠ¤íƒ€ì¼ ë¶„í¬ ê²€ì¦")
    logger.info("=" * 60)

    hairstyles = data['hairstyles']
    scores = data['scores']

    # ê³ ìœ  í—¤ì–´ìŠ¤íƒ€ì¼ ê°œìˆ˜
    unique_styles = np.unique(hairstyles)
    logger.info(f"\nê³ ìœ  í—¤ì–´ìŠ¤íƒ€ì¼: {len(unique_styles)}ê°œ")

    # ìƒìœ„ 20ê°œ í—¤ì–´ìŠ¤íƒ€ì¼
    from collections import Counter
    style_counts = Counter(hairstyles.tolist())
    most_common = style_counts.most_common(20)

    logger.info(f"\nìƒìœ„ 20ê°œ í—¤ì–´ìŠ¤íƒ€ì¼:")
    for rank, (style, count) in enumerate(most_common, 1):
        pct = count / len(hairstyles) * 100

        # í•´ë‹¹ ìŠ¤íƒ€ì¼ì˜ í‰ê·  ì ìˆ˜
        style_scores = scores[hairstyles == style]
        avg_score = style_scores.mean()

        logger.info(f"  {rank:2d}. {style:30s}: {count:4d}ê°œ ({pct:4.1f}%)  í‰ê· ì ìˆ˜: {avg_score:.1f}")

    # ì €ë¹ˆë„ ìŠ¤íƒ€ì¼ (3ê°œ ì´í•˜)
    low_freq_styles = [style for style, count in style_counts.items() if count <= 3]
    if low_freq_styles:
        logger.warning(f"\nâš ï¸ ì €ë¹ˆë„ ìŠ¤íƒ€ì¼ (â‰¤3ê°œ): {len(low_freq_styles)}ê°œ")


def validate_metadata(data: Dict):
    """ë©”íƒ€ë°ì´í„° ê²€ì¦"""
    if data['metadata'] is None:
        logger.info("\nâš ï¸ ë©”íƒ€ë°ì´í„° ì—†ìŒ")
        return

    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“‹ ë©”íƒ€ë°ì´í„° ê²€ì¦")
    logger.info("=" * 60)

    metadata = data['metadata']

    # Face shape ë¶„í¬
    face_shapes = [m['face_shape'] for m in metadata]
    from collections import Counter
    face_shape_counts = Counter(face_shapes)

    logger.info(f"\nì–¼êµ´í˜• ë¶„í¬:")
    for shape, count in face_shape_counts.most_common():
        pct = count / len(face_shapes) * 100
        logger.info(f"  {shape:10s}: {count:4d}ê°œ ({pct:5.1f}%)")

    # Skin tone ë¶„í¬
    skin_tones = [m['skin_tone'] for m in metadata]
    skin_tone_counts = Counter(skin_tones)

    logger.info(f"\ní”¼ë¶€í†¤ ë¶„í¬:")
    for tone, count in skin_tone_counts.most_common():
        pct = count / len(skin_tones) * 100
        logger.info(f"  {tone:10s}: {count:4d}ê°œ ({pct:5.1f}%)")

    # ì‹ ë¢°ë„ ë¶„í¬
    confidences = [m['confidence'] for m in metadata]
    avg_conf = np.mean(confidences)
    logger.info(f"\nMediaPipe ì‹ ë¢°ë„:")
    logger.info(f"  - í‰ê· : {avg_conf:.1%}")
    logger.info(f"  - ìµœì†Œ: {min(confidences):.1%}")
    logger.info(f"  - ìµœëŒ€: {max(confidences):.1%}")

    low_conf_count = sum(1 for c in confidences if c < 0.8)
    if low_conf_count > 0:
        logger.warning(f"  âš ï¸ ë‚®ì€ ì‹ ë¢°ë„ (<80%): {low_conf_count}ê°œ ì–¼êµ´")


def save_validation_report(data: Dict, output_path: str):
    """ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥"""
    logger.info(f"\nğŸ“„ ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")

    report = {
        'total_samples': len(data['scores']),
        'face_features_shape': data['face_features'].shape,
        'skin_features_shape': data['skin_features'].shape,
        'score_stats': {
            'mean': float(data['scores'].mean()),
            'std': float(data['scores'].std()),
            'min': float(data['scores'].min()),
            'max': float(data['scores'].max()),
            'median': float(np.median(data['scores']))
        },
        'unique_hairstyles': len(np.unique(data['hairstyles'])),
        'has_metadata': data['metadata'] is not None
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    logger.info("âœ… ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ")


def main():
    parser = argparse.ArgumentParser(description="ë°ì´í„°ì…‹ ê²€ì¦")
    parser.add_argument(
        "--data",
        type=str,
        required=True,
        help="ê²€ì¦í•  NPZ ë°ì´í„° ê²½ë¡œ"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="validation_report.json",
        help="ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ"
    )

    args = parser.parse_args()

    logger.info("=" * 60)
    logger.info("ğŸ” ë°ì´í„°ì…‹ ê²€ì¦ ì‹œì‘")
    logger.info("=" * 60)

    # ë°ì´í„° ë¡œë“œ
    data = load_dataset(args.data)

    # ê²€ì¦ ìˆ˜í–‰
    validate_feature_ranges(data)
    validate_score_distribution(data)
    validate_hairstyle_distribution(data)
    validate_metadata(data)

    # ë¦¬í¬íŠ¸ ì €ì¥
    save_validation_report(data, args.output)

    logger.info("\n" + "=" * 60)
    logger.info("âœ… ë°ì´í„°ì…‹ ê²€ì¦ ì™„ë£Œ!")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
