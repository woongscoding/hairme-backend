#!/usr/bin/env python3
"""
ML ê¸°ë°˜ í—¤ì–´ìŠ¤íƒ€ì¼ ì¶”ì²œê¸° v3 - Hybrid Architecture

âœ¨ v3.0 ê°œì„ ì‚¬í•­:
- í•™ìŠµê°€ëŠ¥í•œ Face/Tone ì„ë² ë”©
- Attention mechanism
- Residual connections
- Combination-level evaluation ì§€ì›
- ìƒˆë¡œìš´ ì¡°í•©ì— ëŒ€í•œ ì¶”ë¡  ëŠ¥ë ¥

Author: HairMe ML Team
Date: 2025-11-11
Version: 3.0.0
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging

logger = logging.getLogger(__name__)


class AttentionLayer(nn.Module):
    """Multi-head self-attention layer"""

    def __init__(self, embed_dim: int, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.norm = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Self-attention
        attn_out, _ = self.attention(x, x, x)

        # Residual connection + layer norm
        x = self.norm(x + self.dropout(attn_out))

        return x


class HybridRecommenderV3(nn.Module):
    """
    Hybrid ì¶”ì²œ ëª¨ë¸ v3
    í•™ìŠµê°€ëŠ¥í•œ ì„ë² ë”© + Attention + Residual connections
    """

    def __init__(
        self,
        n_face_shapes: int = 5,  # ì‚¬ê°í˜• í¬í•¨
        n_skin_tones: int = 4,
        face_embed_dim: int = 32,
        tone_embed_dim: int = 32,
        style_embed_dim: int = 384,
        use_attention: bool = True,
        dropout_rate: float = 0.3
    ):
        super().__init__()

        # í•™ìŠµê°€ëŠ¥í•œ ì„ë² ë”© ë ˆì´ì–´
        self.face_embedding = nn.Embedding(n_face_shapes, face_embed_dim)
        self.tone_embedding = nn.Embedding(n_skin_tones, tone_embed_dim)

        # ì„ë² ë”© ì´ˆê¸°í™” (Xavier)
        nn.init.xavier_uniform_(self.face_embedding.weight)
        nn.init.xavier_uniform_(self.tone_embedding.weight)

        # Total dimension
        self.total_dim = face_embed_dim + tone_embed_dim + style_embed_dim

        # Attention layer (optional)
        self.use_attention = use_attention
        if use_attention:
            self.attention = AttentionLayer(
                embed_dim=self.total_dim,
                num_heads=8,
                dropout=0.1
            )

        # Feature fusion network with residual connections
        self.fc1 = nn.Linear(self.total_dim, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.dropout1 = nn.Dropout(dropout_rate)

        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)
        self.dropout2 = nn.Dropout(dropout_rate * 0.7)

        # Residual connection for dimension matching
        self.residual_proj = nn.Linear(self.total_dim, 128)

        self.fc3 = nn.Linear(128, 64)
        self.bn3 = nn.BatchNorm1d(64)
        self.dropout3 = nn.Dropout(dropout_rate * 0.5)

        self.fc4 = nn.Linear(64, 32)
        self.fc_out = nn.Linear(32, 1)

        # Initialize weights
        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize network weights"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(
        self,
        face_idx: torch.Tensor,
        tone_idx: torch.Tensor,
        style_emb: torch.Tensor
    ) -> torch.Tensor:
        """
        Forward pass

        Args:
            face_idx: Face shape indices [batch_size]
            tone_idx: Skin tone indices [batch_size]
            style_emb: Style embeddings [batch_size, 384]

        Returns:
            Recommendation scores [batch_size, 1]
        """
        # Get embeddings
        face_emb = self.face_embedding(face_idx)  # [batch, face_dim]
        tone_emb = self.tone_embedding(tone_idx)  # [batch, tone_dim]

        # Concatenate all features
        combined = torch.cat([face_emb, tone_emb, style_emb], dim=-1)  # [batch, total_dim]

        # Apply attention if enabled
        if self.use_attention:
            # Reshape for attention (needs 3D input)
            combined_3d = combined.unsqueeze(1)  # [batch, 1, total_dim]
            attended = self.attention(combined_3d)  # [batch, 1, total_dim]
            combined = attended.squeeze(1)  # [batch, total_dim]

        # Store original for residual
        residual = self.residual_proj(combined)

        # First block
        x = self.fc1(combined)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout1(x)

        # Second block with residual
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = x + residual  # Residual connection
        x = self.dropout2(x)

        # Third block
        x = self.fc3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.dropout3(x)

        # Output layers
        x = self.fc4(x)
        x = F.relu(x)
        score = self.fc_out(x)

        return score


class MLHairstyleRecommenderV3:
    """
    ML ê¸°ë°˜ í—¤ì–´ìŠ¤íƒ€ì¼ ì¶”ì²œê¸° v3
    Hybrid architecture with learned embeddings
    """

    # ì¹´í…Œê³ ë¦¬ ì •ì˜
    FACE_SHAPES = ["ê°ì§„í˜•", "ë‘¥ê·¼í˜•", "ê¸´í˜•", "ê³„ë€í˜•", "ì‚¬ê°í˜•"]
    SKIN_TONES = ["ê²¨ìš¸ì¿¨", "ê°€ì„ì›œ", "ë´„ì›œ", "ì—¬ë¦„ì¿¨"]

    def __init__(
        self,
        model_path: Optional[str] = None,
        embeddings_path: str = "data_source/style_embeddings.npz",
        use_attention: bool = True
    ):
        """
        ì´ˆê¸°í™”

        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ ìƒˆ ëª¨ë¸ ìƒì„±)
            embeddings_path: í—¤ì–´ìŠ¤íƒ€ì¼ ì„ë² ë”© ê²½ë¡œ
            use_attention: Attention ì‚¬ìš© ì—¬ë¶€
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Face/Tone ì¸ë±ìŠ¤ ë§¤í•‘
        self.face_to_idx = {face: idx for idx, face in enumerate(self.FACE_SHAPES)}
        self.tone_to_idx = {tone: idx for idx, tone in enumerate(self.SKIN_TONES)}

        # ëª¨ë¸ ìƒì„±
        self.model = HybridRecommenderV3(
            n_face_shapes=len(self.FACE_SHAPES),
            n_skin_tones=len(self.SKIN_TONES),
            use_attention=use_attention
        )

        # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if model_path and Path(model_path).exists():
            logger.info(f"ğŸ“‚ ëª¨ë¸ ë¡œë”©: {model_path}")
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)

            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                logger.info(f"  Epoch {checkpoint.get('epoch', 'unknown')}, "
                          f"Best Val Loss: {checkpoint.get('best_val_loss', 'unknown'):.3f}")
            else:
                self.model.load_state_dict(checkpoint)

            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")

        self.model.to(self.device)
        self.model.eval()

        # í—¤ì–´ìŠ¤íƒ€ì¼ ì„ë² ë”© ë¡œë“œ
        logger.info(f"ğŸ“‚ ì„ë² ë”© ë¡œë”©: {embeddings_path}")
        data = np.load(embeddings_path, allow_pickle=True)
        self.styles = data['styles'].tolist()
        self.embeddings = data['embeddings']
        self.style_to_idx = {style: idx for idx, style in enumerate(self.styles)}
        logger.info(f"âœ… ì„ë² ë”© ë¡œë“œ ì™„ë£Œ: {len(self.styles)}ê°œ ìŠ¤íƒ€ì¼")

        # Sentence Transformer (ì‹¤ì‹œê°„ ì„ë² ë”©ìš©)
        self.sentence_model = None
        try:
            from sentence_transformers import SentenceTransformer
            model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
            logger.info(f"ğŸ“‚ Sentence Transformer ë¡œë”©: {model_name}")
            self.sentence_model = SentenceTransformer(model_name)
            logger.info("âœ… ì‹¤ì‹œê°„ ì„ë² ë”© í™œì„±í™”")
        except Exception as e:
            logger.warning(f"âš ï¸ Sentence Transformer ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    def predict_score(
        self,
        face_shape: str,
        skin_tone: str,
        hairstyle: str
    ) -> Tuple[float, Dict]:
        """
        í—¤ì–´ìŠ¤íƒ€ì¼ ì ìˆ˜ ì˜ˆì¸¡

        Args:
            face_shape: ì–¼êµ´í˜•
            skin_tone: í”¼ë¶€í†¤
            hairstyle: í—¤ì–´ìŠ¤íƒ€ì¼ëª…

        Returns:
            (ì ìˆ˜, ë””ë²„ê·¸ ì •ë³´)
        """
        # Face/Tone ì¸ë±ìŠ¤ ë³€í™˜
        face_idx = self.face_to_idx.get(face_shape, 3)  # ê¸°ë³¸ê°’: ê³„ë€í˜•
        tone_idx = self.tone_to_idx.get(skin_tone, 2)  # ê¸°ë³¸ê°’: ë´„ì›œ

        # ìŠ¤íƒ€ì¼ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
        from utils.style_preprocessor import normalize_style_name
        normalized = normalize_style_name(hairstyle)

        if normalized in self.style_to_idx:
            idx = self.style_to_idx[normalized]
            style_emb = self.embeddings[idx]
        elif self.sentence_model:
            # ì‹¤ì‹œê°„ ì„ë² ë”© ìƒì„±
            style_emb = self.sentence_model.encode(hairstyle)
            logger.info(f"âœ¨ ì‹¤ì‹œê°„ ì„ë² ë”© ìƒì„±: '{hairstyle}'")
        else:
            # Fallback: í‰ê·  ì„ë² ë”©
            style_emb = np.mean(self.embeddings, axis=0)
            logger.warning(f"âš ï¸ ë¯¸ë“±ë¡ ìŠ¤íƒ€ì¼, í‰ê·  ì„ë² ë”© ì‚¬ìš©: '{hairstyle}'")

        # í…ì„œ ë³€í™˜
        face_tensor = torch.tensor([face_idx], dtype=torch.long).to(self.device)
        tone_tensor = torch.tensor([tone_idx], dtype=torch.long).to(self.device)
        style_tensor = torch.tensor([style_emb], dtype=torch.float32).to(self.device)

        # ì˜ˆì¸¡
        with torch.no_grad():
            score_tensor = self.model(face_tensor, tone_tensor, style_tensor)
            score = score_tensor.item()

        # ì ìˆ˜ ë²”ìœ„ ì œí•œ (0-100)
        score = max(0.0, min(100.0, score))

        # ë””ë²„ê·¸ ì •ë³´
        debug_info = {
            'face_idx': face_idx,
            'tone_idx': tone_idx,
            'normalized_style': normalized,
            'style_found': normalized in self.style_to_idx,
            'raw_score': score_tensor.item(),
            'final_score': score,
            'combination': f"{face_shape}_{skin_tone}"
        }

        return score, debug_info

    def predict_batch(
        self,
        face_shapes: List[str],
        skin_tones: List[str],
        hairstyles: List[str]
    ) -> np.ndarray:
        """
        ë°°ì¹˜ ì˜ˆì¸¡

        Args:
            face_shapes: ì–¼êµ´í˜• ë¦¬ìŠ¤íŠ¸
            skin_tones: í”¼ë¶€í†¤ ë¦¬ìŠ¤íŠ¸
            hairstyles: í—¤ì–´ìŠ¤íƒ€ì¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì ìˆ˜ ë°°ì—´
        """
        # ì¸ë±ìŠ¤ ë³€í™˜
        face_indices = [self.face_to_idx.get(f, 3) for f in face_shapes]
        tone_indices = [self.tone_to_idx.get(t, 2) for t in skin_tones]

        # ìŠ¤íƒ€ì¼ ì„ë² ë”©
        from utils.style_preprocessor import normalize_style_name
        style_embeddings = []

        for style in hairstyles:
            normalized = normalize_style_name(style)
            if normalized in self.style_to_idx:
                idx = self.style_to_idx[normalized]
                style_embeddings.append(self.embeddings[idx])
            else:
                # í‰ê·  ì„ë² ë”© ì‚¬ìš©
                style_embeddings.append(np.mean(self.embeddings, axis=0))

        # í…ì„œ ë³€í™˜
        face_tensor = torch.tensor(face_indices, dtype=torch.long).to(self.device)
        tone_tensor = torch.tensor(tone_indices, dtype=torch.long).to(self.device)
        style_tensor = torch.tensor(style_embeddings, dtype=torch.float32).to(self.device)

        # ì˜ˆì¸¡
        with torch.no_grad():
            scores = self.model(face_tensor, tone_tensor, style_tensor)
            scores = scores.cpu().numpy().flatten()

        # ë²”ìœ„ ì œí•œ
        scores = np.clip(scores, 0, 100)

        return scores

    def get_embedding_weights(self) -> Dict:
        """
        í•™ìŠµëœ ì„ë² ë”© ê°€ì¤‘ì¹˜ ë°˜í™˜ (ë¶„ì„ìš©)

        Returns:
            Face/Tone ì„ë² ë”© ê°€ì¤‘ì¹˜
        """
        with torch.no_grad():
            face_weights = self.model.face_embedding.weight.cpu().numpy()
            tone_weights = self.model.tone_embedding.weight.cpu().numpy()

        return {
            'face_embeddings': face_weights,
            'tone_embeddings': tone_weights,
            'face_shapes': self.FACE_SHAPES,
            'skin_tones': self.SKIN_TONES
        }

    def analyze_combination_similarity(self) -> np.ndarray:
        """
        í•™ìŠµëœ ì„ë² ë”© ê¸°ë°˜ ì¡°í•© ê°„ ìœ ì‚¬ë„ ë¶„ì„

        Returns:
            ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤
        """
        embeddings = self.get_embedding_weights()
        face_emb = embeddings['face_embeddings']
        tone_emb = embeddings['tone_embeddings']

        # ëª¨ë“  ì¡°í•©ì˜ ì„ë² ë”© ìƒì„±
        combinations = []
        combo_names = []

        for i, face in enumerate(self.FACE_SHAPES):
            for j, tone in enumerate(self.SKIN_TONES):
                combo_emb = np.concatenate([face_emb[i], tone_emb[j]])
                combinations.append(combo_emb)
                combo_names.append(f"{face}_{tone}")

        combinations = np.array(combinations)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        from sklearn.metrics.pairwise import cosine_similarity
        similarity_matrix = cosine_similarity(combinations)

        return similarity_matrix, combo_names


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    recommender = MLHairstyleRecommenderV3()

    # ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    score, info = recommender.predict_score("ê³„ë€í˜•", "ë´„ì›œ", "ëŒ„ë”” ì»·")
    print(f"Score: {score:.1f}")
    print(f"Debug: {info}")

    # ë°°ì¹˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    faces = ["ê°ì§„í˜•", "ë‘¥ê·¼í˜•", "ê¸´í˜•"]
    tones = ["ê²¨ìš¸ì¿¨", "ê°€ì„ì›œ", "ì—¬ë¦„ì¿¨"]
    styles = ["ëŒ„ë”” ì»·", "íˆ¬ ë¸”ëŸ­ ì»·", "ë¦¬ì  íŠ¸ ì»·"]

    scores = recommender.predict_batch(faces, tones, styles)
    print(f"Batch scores: {scores}")