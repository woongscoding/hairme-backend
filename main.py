"""
HairMe Backend - AI-powered Hairstyle Recommendation Service
Version: 20.2.0 (MediaPipe transition complete)
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from config.settings import settings
from core.logging import logger, log_structured
from core.cache import init_redis
from core.ml_loader import load_ml_model, load_sentence_transformer
from database.connection import init_database
from database.migration import migrate_database_schema
from models.mediapipe_analyzer import MediaPipeFaceAnalyzer
from services.hybrid_recommender import get_hybrid_service
from services.feedback_collector import get_feedback_collector
from services.retrain_queue import get_retrain_queue
from routers.admin import router as admin_router
from api.endpoints.analyze import router as analyze_router, init_gemini
from api.endpoints.feedback import router as feedback_router
import api.endpoints.analyze as analyze_module


# ========== FastAPI App Initialization ==========
app = FastAPI(
    title=settings.APP_TITLE,
    description=settings.APP_DESCRIPTION,
    version=settings.APP_VERSION
)


# ========== CORS Middleware ==========
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ========== Register Routers ==========
app.include_router(admin_router, prefix="/api", tags=["admin"])
app.include_router(analyze_router, prefix="/api", tags=["analysis"])
app.include_router(feedback_router, prefix="/api", tags=["feedback"])


# ========== Startup Event ==========
@app.on_event("startup")
async def startup_event():
    """Initialize all services on server startup"""
    logger.info("ğŸš€ ì„œë²„ ì‹œì‘ ì¤‘...")

    # Initialize Gemini API
    init_gemini()

    # Initialize MediaPipe Face Analyzer
    try:
        analyzer = MediaPipeFaceAnalyzer()
        analyze_module.mediapipe_analyzer = analyzer
        logger.info("âœ… MediaPipe ì–¼êµ´ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        log_structured("mediapipe_initialized", {
            "status": "success",
            "landmarks": 478
        })
    except Exception as e:
        logger.error(f"âŒ MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        analyze_module.mediapipe_analyzer = None
        log_structured("mediapipe_initialized", {
            "status": "failed",
            "error": str(e)
        })

    # Load ML Model
    ml_loaded = load_ml_model()
    if ml_loaded:
        logger.info("âœ… ML ëª¨ë“œ: í™œì„±í™”")
        log_structured("ml_model_loaded", {
            "status": "success",
            "model_path": settings.ML_MODEL_PATH
        })
    else:
        logger.warning("âš ï¸ ML ëª¨ë“œ: ë¹„í™œì„±í™” (ê¸°ë³¸ ì ìˆ˜ ì‚¬ìš©)")
        log_structured("ml_model_loaded", {
            "status": "failed",
            "fallback": "default_score"
        })

    # Load Sentence Transformer
    st_loaded = load_sentence_transformer()
    if st_loaded:
        logger.info("âœ… ìŠ¤íƒ€ì¼ ì„ë² ë”©: í™œì„±í™”")
        log_structured("sentence_transformer_loaded", {
            "status": "success",
            "model": settings.SENTENCE_TRANSFORMER_MODEL,
            "embedding_dim": 384
        })
    else:
        logger.warning("âš ï¸ ìŠ¤íƒ€ì¼ ì„ë² ë”©: ë¹„í™œì„±í™” (ì„ë² ë”© ì—†ì´ ì§„í–‰)")
        log_structured("sentence_transformer_loaded", {
            "status": "failed",
            "fallback": "no_embedding"
        })

    # Initialize Hybrid Recommendation Service (Gemini + ML)
    try:
        if settings.GEMINI_API_KEY:
            service = get_hybrid_service(settings.GEMINI_API_KEY)
            analyze_module.hybrid_service = service
            logger.info("âœ… í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (Gemini + ML)")
            log_structured("hybrid_service_initialized", {
                "status": "success",
                "gemini_model": settings.MODEL_NAME,
                "ml_model": "hairstyle_recommender.pt"
            })
        else:
            logger.warning("âš ï¸ GEMINI_API_KEY ì—†ìŒ - í•˜ì´ë¸Œë¦¬ë“œ ì„œë¹„ìŠ¤ ë¹„í™œì„±í™”")
            analyze_module.hybrid_service = None
    except Exception as e:
        logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        analyze_module.hybrid_service = None
        log_structured("hybrid_service_initialized", {
            "status": "failed",
            "error": str(e)
        })

    # Initialize Feedback Collector
    try:
        collector = get_feedback_collector()
        analyze_module.feedback_collector = collector
        logger.info("âœ… í”¼ë“œë°± ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        log_structured("feedback_collector_initialized", {
            "status": "success"
        })
    except Exception as e:
        logger.error(f"âŒ í”¼ë“œë°± ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        analyze_module.feedback_collector = None
        log_structured("feedback_collector_initialized", {
            "status": "failed",
            "error": str(e)
        })

    # Initialize Retrain Queue
    try:
        queue = get_retrain_queue()
        analyze_module.retrain_queue = queue
        logger.info("âœ… ì¬í•™ìŠµ í ì´ˆê¸°í™” ì™„ë£Œ")
        log_structured("retrain_queue_initialized", {
            "status": "success"
        })
    except Exception as e:
        logger.error(f"âŒ ì¬í•™ìŠµ í ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        analyze_module.retrain_queue = None
        log_structured("retrain_queue_initialized", {
            "status": "failed",
            "error": str(e)
        })

    # Initialize Database
    db_initialized = init_database()
    if db_initialized:
        # Run schema migration
        migrate_database_schema()

    # Initialize Redis Cache
    init_redis()


# ========== Root Endpoint ==========
@app.get("/")
async def root():
    """Root endpoint with service status"""
    return {
        "message": f"{settings.APP_TITLE} - v{settings.APP_VERSION} (MediaPipe ì „í™˜ ì™„ë£Œ)",
        "version": settings.APP_VERSION,
        "model": settings.MODEL_NAME,
        "status": "running",
        "features": {
            "mediapipe_analysis": "enabled" if analyze_module.mediapipe_analyzer else "disabled",
            "gemini_analysis": "enabled" if settings.GEMINI_API_KEY else "disabled",
            "redis_cache": "enabled",
            "database": "enabled",
            "feedback_system": "enabled",
            "ml_prediction": "enabled",
            "style_embedding": "enabled"
        }
    }


# ========== Health Check Endpoint ==========
@app.get("/api/health")
async def health_check():
    """Health check endpoint for monitoring"""
    return {
        "status": "healthy",
        "version": settings.APP_VERSION,
        "model": settings.MODEL_NAME,
        "mediapipe_analysis": "enabled" if analyze_module.mediapipe_analyzer else "disabled",
        "gemini_api": "configured" if settings.GEMINI_API_KEY else "not_configured",
        "redis": "connected",
        "database": "connected",
        "feedback_system": "enabled",
        "ml_model": "enabled",
        "style_embedding": "enabled"
    }


# ========== Lambda Handler ==========
# For AWS Lambda deployment using Mangum
try:
    from mangum import Mangum
    handler = Mangum(app, lifespan="off")
    logger.info("âœ… Lambda handler initialized")
except ImportError:
    logger.warning("âš ï¸ Mangum not installed - Lambda handler not available")
    handler = None


# ========== Main Entry Point ==========
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
