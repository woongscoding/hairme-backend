"""Dependency injection providers for FastAPI (ML-only mode)"""

import os
from typing import Optional, TYPE_CHECKING, Union
from functools import lru_cache

if TYPE_CHECKING:
    from models.mediapipe_analyzer import MediaPipeFaceAnalyzer
    from services.face_detection_service import FaceDetectionService
    from services.hybrid_recommender import MLRecommendationService
    from models.onnx_recommender import ONNXHairstyleRecommender

from core.logging import logger

# Lambda í™˜ê²½ ê°ì§€
IS_LAMBDA = os.environ.get('AWS_LAMBDA_FUNCTION_NAME') is not None
USE_ONNX = os.environ.get('USE_ONNX', 'false').lower() == 'true' or IS_LAMBDA


# ========== Global Service Instances (Initialized at Startup) ==========
_mediapipe_analyzer: Optional['MediaPipeFaceAnalyzer'] = None
_face_detection_service: Optional['FaceDetectionService'] = None
_hybrid_service: Optional['MLRecommendationService'] = None
_onnx_recommender: Optional['ONNXHairstyleRecommender'] = None


# ========== Initialization Functions (Called from main.py) ==========
def init_services(
    mediapipe_analyzer: Optional['MediaPipeFaceAnalyzer'] = None,
    hybrid_service: Optional['MLRecommendationService'] = None
) -> None:
    """
    Initialize global service instances (ML-only mode)

    Called from main.py startup event

    Args:
        mediapipe_analyzer: MediaPipe face analyzer
        hybrid_service: ML recommendation service
    """
    global _mediapipe_analyzer, _face_detection_service
    global _hybrid_service

    _mediapipe_analyzer = mediapipe_analyzer
    _hybrid_service = hybrid_service

    # Initialize services with dependencies (lazy import)
    from services.face_detection_service import FaceDetectionService
    _face_detection_service = FaceDetectionService(mediapipe_analyzer)

    logger.info("âœ… ì˜ì¡´ì„± ì£¼ì… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (ML-only mode)")


# ========== Dependency Providers (for FastAPI Depends) ==========
@lru_cache()
def get_mediapipe_analyzer() -> 'MediaPipeFaceAnalyzer':
    """
    Get MediaPipeFaceAnalyzer instance (Lazy Initialization)
    """
    global _mediapipe_analyzer
    if _mediapipe_analyzer is None:
        logger.info("ğŸ¢ Lazy initializing MediaPipeFaceAnalyzer...")
        from models.mediapipe_analyzer import MediaPipeFaceAnalyzer
        _mediapipe_analyzer = MediaPipeFaceAnalyzer()
        # Update startup status
        import main
        main.startup_status["mediapipe"] = True
    return _mediapipe_analyzer


@lru_cache()
def get_face_detection_service() -> 'FaceDetectionService':
    """
    Get FaceDetectionService instance (Lazy Initialization)
    """
    global _face_detection_service
    if _face_detection_service is None:
        logger.info("ğŸ¢ Lazy initializing FaceDetectionService...")
        # Ensure MediaPipe is initialized
        mp_analyzer = get_mediapipe_analyzer()
        from services.face_detection_service import FaceDetectionService
        _face_detection_service = FaceDetectionService(mp_analyzer)
    return _face_detection_service


@lru_cache()
def get_hybrid_service() -> Union['MLRecommendationService', 'ONNXRecommenderWrapper']:
    """
    Get recommendation service instance (Lazy Initialization)

    Lambda í™˜ê²½ì—ì„œëŠ” ONNX ê¸°ë°˜ ê²½ëŸ‰ ì¶”ì²œê¸°ë¥¼ ì‚¬ìš©í•˜ê³ ,
    ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” PyTorch ê¸°ë°˜ MLRecommendationServiceë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    global _hybrid_service, _onnx_recommender

    # Lambda í™˜ê²½ ë˜ëŠ” USE_ONNX=trueì¸ ê²½ìš° ONNX ì‚¬ìš©
    if USE_ONNX:
        if _onnx_recommender is None:
            logger.info("ğŸš€ Lambda/ONNX ëª¨ë“œ: ONNXHairstyleRecommender ì´ˆê¸°í™” ì¤‘...")
            try:
                from models.onnx_recommender import get_onnx_recommender
                _onnx_recommender = get_onnx_recommender()

                # Update startup status
                try:
                    import main
                    main.startup_status["ml_service"] = True
                except Exception:
                    pass  # main ëª¨ë“ˆì´ ì—†ì„ ìˆ˜ ìˆìŒ

                logger.info("âœ… ONNX ì¶”ì²œê¸° ì´ˆê¸°í™” ì™„ë£Œ (ì½œë“œ ìŠ¤íƒ€íŠ¸ ìµœì í™”)")
            except Exception as e:
                logger.error(f"âŒ ONNX ì¶”ì²œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
                raise RuntimeError(f"Failed to initialize ONNX recommender: {e}")

        # ONNX ì¶”ì²œê¸°ë¥¼ MLRecommendationService ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘
        return ONNXRecommenderWrapper(_onnx_recommender)

    # ë¡œì»¬ í™˜ê²½: PyTorch ê¸°ë°˜ MLRecommendationService ì‚¬ìš©
    if _hybrid_service is None:
        logger.info("ğŸ¢ Lazy initializing MLRecommendationService (PyTorch)...")
        try:
            from services.hybrid_recommender import get_ml_recommendation_service

            _hybrid_service = get_ml_recommendation_service()

            # Update startup status
            import main
            main.startup_status["ml_service"] = True

            logger.info("âœ… MLRecommendationService initialized successfully")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize MLRecommendationService: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise RuntimeError(f"Failed to initialize ML service: {e}")
    return _hybrid_service


class ONNXRecommenderWrapper:
    """
    ONNX ì¶”ì²œê¸°ë¥¼ MLRecommendationService ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘

    API ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš© ê°€ëŠ¥
    """

    def __init__(self, onnx_recommender: 'ONNXHairstyleRecommender'):
        self.onnx_recommender = onnx_recommender
        self.ml_available = True
        self.reason_generator = None

        # Reason generator ë¡œë“œ ì‹œë„
        try:
            from services.reason_generator import get_reason_generator
            self.reason_generator = get_reason_generator()
        except Exception as e:
            logger.warning(f"âš ï¸ ì¶”ì²œ ì´ìœ  ìƒì„±ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")

    def recommend(
        self,
        image_data: bytes,
        face_shape: str,
        skin_tone: str,
        face_features: list = None,
        skin_features: list = None,
        gender: str = None
    ) -> dict:
        """
        MLRecommendationService.recommend() ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
        """
        from utils.style_preprocessor import normalize_style_name

        # ONNX ì¶”ì²œ ì‹¤í–‰
        ml_recommendations = self.onnx_recommender.recommend_top_k(
            face_shape=face_shape,
            skin_tone=skin_tone,
            k=3,
            face_features=face_features,
            skin_features=skin_features,
            gender=gender
        )

        # ì‘ë‹µ í˜•ì‹ ë³€í™˜ (MLRecommendationServiceì™€ ë™ì¼)
        result = []
        seen_styles = set()

        for rec in ml_recommendations:
            hairstyle_id = rec.get("hairstyle_id")
            style_name = rec.get("hairstyle", "").strip()
            ml_score = rec.get("score", 0.0)

            if not style_name:
                continue

            normalized_name = normalize_style_name(style_name)
            if normalized_name in seen_styles:
                continue

            # í…œí”Œë¦¿ ê¸°ë°˜ ì´ìœ  ìƒì„±
            if self.reason_generator:
                try:
                    reason = self.reason_generator.generate_with_score(
                        face_shape, skin_tone, style_name, ml_score
                    )
                except Exception:
                    reason = f"ML ëª¨ë¸ ì¶”ì²œ (ì ìˆ˜: {ml_score:.1f})"
            else:
                reason = f"ML ëª¨ë¸ ì¶”ì²œ (ì ìˆ˜: {ml_score:.1f})"

            result.append({
                "hairstyle_id": hairstyle_id,
                "style_name": style_name,
                "reason": reason,
                "source": "ml",
                "score": round(ml_score / 100.0, 2),
                "rank": len(result) + 1
            })

            seen_styles.add(normalized_name)

        # rank ì¬ì¡°ì •
        for idx, rec in enumerate(result, 1):
            rec['rank'] = idx

        return {
            "analysis": {
                "face_shape": face_shape,
                "personal_color": skin_tone,
                "features": "ONNX ëª¨ë¸ ê¸°ë°˜ ë¶„ì„"
            },
            "recommendations": result,
            "meta": {
                "total_count": len(result),
                "ml_count": len(result),
                "method": "onnx"
            }
        }


