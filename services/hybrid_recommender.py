"""
í•˜ì´ë¸Œë¦¬ë“œ í—¤ì–´ìŠ¤íƒ€ì¼ ì¶”ì²œ ì„œë¹„ìŠ¤

Gemini API + ML ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ìµœì ì˜ ì¶”ì²œ ì œê³µ

Circuit Breaker íŒ¨í„´ ì ìš©:
- Gemini API í˜¸ì¶œì— Circuit Breaker ì ìš© (5íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œ 60ì´ˆê°„ ì°¨ë‹¨)
- Circuit OPEN ì‹œ MediaPipe ë°ì´í„°ë§Œ ì‚¬ìš©í•œ fallback ì œê³µ

Author: HairMe ML Team
Date: 2025-11-08
Version: 1.1.0
"""

import logging
from typing import List, Dict, Optional, Any
import google.generativeai as genai
from PIL import Image
import io
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from models.ml_recommender import get_ml_recommender
from services.reason_generator import get_reason_generator
from services.circuit_breaker import gemini_breaker, with_circuit_breaker
from utils.style_preprocessor import normalize_style_name

logger = logging.getLogger(__name__)


class HybridRecommendationService:
    """Gemini + ML í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì„œë¹„ìŠ¤"""

    def __init__(self, gemini_api_key: str):
        """
        ì´ˆê¸°í™”

        Args:
            gemini_api_key: Gemini API í‚¤
        """
        # Gemini ì„¤ì •
        genai.configure(api_key=gemini_api_key)
        self.gemini_model = genai.GenerativeModel("gemini-1.5-flash-latest")
        logger.info("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

        # ML ì¶”ì²œê¸°ëŠ” ì‹±ê¸€í†¤ìœ¼ë¡œ ë¡œë“œ
        try:
            self.ml_recommender = get_ml_recommender()
            self.ml_available = True
        except Exception as e:
            logger.error(f"âŒ ML ì¶”ì²œê¸° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            self.ml_recommender = None
            self.ml_available = False

        # ì¶”ì²œ ì´ìœ  ìƒì„±ê¸° ë¡œë“œ
        try:
            self.reason_generator = get_reason_generator()
            logger.info("âœ… ì¶”ì²œ ì´ìœ  ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì¶”ì²œ ì´ìœ  ìƒì„±ê¸° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            self.reason_generator = None

    def _create_gemini_prompt(
        self,
        face_shape: str,
        skin_tone: str
    ) -> str:
        """
        Gemini APIìš© í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            face_shape: ì–¼êµ´í˜•
            skin_tone: í”¼ë¶€í†¤

        Returns:
            í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        prompt = f"""ì´ ì‚¬ëŒì˜ ì–¼êµ´ì„ ë¶„ì„í•˜ê³  í—¤ì–´ìŠ¤íƒ€ì¼ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**MediaPipe ë¶„ì„ ê²°ê³¼:**
- ì–¼êµ´í˜•: {face_shape}
- í”¼ë¶€í†¤: {skin_tone}

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "analysis": {{
    "face_shape": "{face_shape}",
    "personal_color": "{skin_tone}",
    "features": "ì´ëª©êµ¬ë¹„ íŠ¹ì§• (30ì ì´ë‚´)"
  }},
  "recommendations": [
    {{"style_name": "ìŠ¤íƒ€ì¼ëª… (15ì ì´ë‚´)", "reason": "ì¶”ì²œ ì´ìœ  (30ì ì´ë‚´)"}},
    {{"style_name": "ìŠ¤íƒ€ì¼ëª… (15ì ì´ë‚´)", "reason": "ì¶”ì²œ ì´ìœ  (30ì ì´ë‚´)"}},
    {{"style_name": "ìŠ¤íƒ€ì¼ëª… (15ì ì´ë‚´)", "reason": "ì¶”ì²œ ì´ìœ  (30ì ì´ë‚´)"}},
    {{"style_name": "ìŠ¤íƒ€ì¼ëª… (15ì ì´ë‚´)", "reason": "ì¶”ì²œ ì´ìœ  (30ì ì´ë‚´)"}}
  ]
}}

ì¤‘ìš”:
- 4ê°œì˜ í—¤ì–´ìŠ¤íƒ€ì¼ì„ ì¶”ì²œí•˜ì„¸ìš”
- í•œêµ­ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ ì‚¬ìš©
- ì–¼êµ´í˜•ê³¼ í”¼ë¶€í†¤ì— ê°€ì¥ ì˜ ì–´ìš¸ë¦¬ëŠ” ìŠ¤íƒ€ì¼ ì¶”ì²œ"""

        return prompt

    def _gemini_fallback(self, image_data: bytes, face_shape: str, skin_tone: str) -> Dict[str, Any]:
        """
        Gemini API ì¥ì•  ì‹œ fallback

        Circuit Breakerê°€ OPEN ìƒíƒœì¼ ë•Œ MediaPipe ë°ì´í„°ë§Œ ì‚¬ìš©í•œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜

        Args:
            image_data: ì´ë¯¸ì§€ ë°”ì´íŠ¸ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            face_shape: ì–¼êµ´í˜•
            skin_tone: í”¼ë¶€í†¤

        Returns:
            MediaPipe ë°ì´í„°ë§Œ í¬í•¨í•œ ê¸°ë³¸ ì‘ë‹µ
        """
        logger.warning(
            f"[FALLBACK] Gemini API ì‚¬ìš© ë¶ˆê°€. MediaPipe ë°ì´í„°ë§Œ ì‚¬ìš©: "
            f"ì–¼êµ´í˜•={face_shape}, í”¼ë¶€í†¤={skin_tone}"
        )

        return {
            "analysis": {
                "face_shape": face_shape,
                "personal_color": skin_tone,
                "features": "Gemini API ì¼ì‹œ ì¤‘ë‹¨ - MediaPipe ê¸°ë°˜ ë¶„ì„"
            },
            "recommendations": []
        }

    @with_circuit_breaker(gemini_breaker, fallback=lambda self, *args, **kwargs: self._gemini_fallback(*args, **kwargs))
    def _call_gemini(
        self,
        image_data: bytes,
        face_shape: str,
        skin_tone: str
    ) -> Dict[str, Any]:
        """
        Gemini API í˜¸ì¶œ

        Args:
            image_data: ì´ë¯¸ì§€ ë°”ì´íŠ¸
            face_shape: ì–¼êµ´í˜•
            skin_tone: í”¼ë¶€í†¤

        Returns:
            Gemini ë¶„ì„ ê²°ê³¼
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(io.BytesIO(image_data))

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_gemini_prompt(face_shape, skin_tone)

            # API í˜¸ì¶œ
            response = self.gemini_model.generate_content([prompt, image])

            # JSON íŒŒì‹±
            import json
            raw_text = response.text.strip()

            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            if raw_text.startswith("```json"):
                raw_text = raw_text[7:]
            if raw_text.startswith("```"):
                raw_text = raw_text[3:]
            if raw_text.endswith("```"):
                raw_text = raw_text[:-3]

            result = json.loads(raw_text.strip())

            logger.info(f"âœ… Gemini ì‘ë‹µ: {len(result.get('recommendations', []))}ê°œ ì¶”ì²œ")

            return result

        except json.JSONDecodeError as e:
            logger.error(
                f"âŒ Gemini ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}\n"
                f"ì‘ë‹µ ë‚´ìš©: {response.text[:200] if 'response' in locals() else 'N/A'}"
            )
            # ML ì¶”ì²œìœ¼ë¡œ í´ë°± (Gemini ì—†ì´ ì§„í–‰)
            return {
                "analysis": {
                    "face_shape": face_shape,
                    "personal_color": skin_tone,
                    "features": "Gemini ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ (ML ì¶”ì²œë§Œ ì‚¬ìš©)"
                },
                "recommendations": []
            }
        except Exception as e:
            logger.error(
                f"âŒ Gemini API ì˜¤ë¥˜ ({type(e).__name__}): {str(e)}\n"
                f"ì–¼êµ´í˜•={face_shape}, í”¼ë¶€í†¤={skin_tone}"
            )
            # ML ì¶”ì²œìœ¼ë¡œ í´ë°± (Gemini ì—†ì´ ì§„í–‰)
            return {
                "analysis": {
                    "face_shape": face_shape,
                    "personal_color": skin_tone,
                    "features": f"Gemini API ì˜¤ë¥˜ ({type(e).__name__}) - ML ì¶”ì²œë§Œ ì‚¬ìš©"
                },
                "recommendations": []
            }

    def _merge_recommendations(
        self,
        gemini_recommendations: List[Dict[str, Any]],
        ml_recommendations: List[Dict[str, Any]],
        face_shape: str,
        skin_tone: str
    ) -> List[Dict[str, Any]]:
        """
        Geminiì™€ ML ì¶”ì²œ ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)

        Args:
            gemini_recommendations: Gemini ì¶”ì²œ ë¦¬ìŠ¤íŠ¸
            ml_recommendations: ML ì¶”ì²œ ë¦¬ìŠ¤íŠ¸
            face_shape: ì–¼êµ´í˜•
            skin_tone: í”¼ë¶€í†¤

        Returns:
            ë³‘í•©ëœ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 7ê°œ)
        """
        merged = []
        seen_styles = set()

        # 1. Gemini ì¶”ì²œ ì¶”ê°€ (ìµœëŒ€ 4ê°œ)
        for rec in gemini_recommendations:
            style_name = rec.get("style_name", "").strip()

            if not style_name:
                continue

            # ë„ì–´ì“°ê¸° ì •ê·œí™” ì ìš© (ì¤‘ë³µ ê²€ì‚¬ìš©)
            normalized_name = normalize_style_name(style_name)

            if normalized_name in seen_styles:
                continue

            # hairstyle_id ì°¾ê¸° (ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ)
            hairstyle_id = None
            if self.ml_available and self.ml_recommender:
                hairstyle_id = self.ml_recommender.style_to_idx.get(normalized_name)

            # ML ì ìˆ˜ ì¶”ê°€ (ì •ê·œí™”ëœ ì´ë¦„ ì‚¬ìš©)
            ml_score = 0.0
            if self.ml_available and self.ml_recommender:
                try:
                    ml_score = self.ml_recommender.predict_score(
                        face_shape, skin_tone, style_name
                    )
                except:
                    pass

            merged.append({
                "hairstyle_id": hairstyle_id,  # âœ… DB ID ì¶”ê°€
                "style_name": style_name,
                "reason": rec.get("reason", ""),
                "source": "gemini",
                "score": ml_score,  # âœ… scoreë¡œ í•„ë“œëª… í†µì¼
                "rank": len(merged) + 1
            })

            seen_styles.add(normalized_name)

        # 2. ML ì¶”ì²œ ì¶”ê°€ (ì¤‘ë³µ ì œì™¸, ìµœëŒ€ 3ê°œ)
        for rec in ml_recommendations:
            if len(merged) >= 7:  # ìµœëŒ€ 7ê°œ
                break

            hairstyle_id = rec.get("hairstyle_id")  # âœ… MLì—ì„œ ID ê°€ì ¸ì˜¤ê¸°
            style_name = rec.get("hairstyle", "").strip()
            ml_score = rec.get("score", 0.0)

            if not style_name:
                continue

            # ë„ì–´ì“°ê¸° ì •ê·œí™” ì ìš© (ì¤‘ë³µ ê²€ì‚¬ìš©)
            normalized_name = normalize_style_name(style_name)

            if normalized_name in seen_styles:
                continue

            # í…œí”Œë¦¿ ê¸°ë°˜ ì´ìœ  ìƒì„±
            if self.reason_generator:
                try:
                    reason = self.reason_generator.generate_with_score(
                        face_shape, skin_tone, style_name, ml_score
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ ì´ìœ  ìƒì„± ì‹¤íŒ¨: {str(e)}")
                    reason = f"ML ëª¨ë¸ ì¶”ì²œ (ì ìˆ˜: {ml_score:.1f})"
            else:
                reason = f"ML ëª¨ë¸ ì¶”ì²œ (ì ìˆ˜: {ml_score:.1f})"

            merged.append({
                "hairstyle_id": hairstyle_id,  # âœ… DB ID ì¶”ê°€
                "style_name": style_name,
                "reason": reason,
                "source": "ml",
                "score": ml_score,  # âœ… scoreë¡œ í•„ë“œëª… í†µì¼
                "rank": len(merged) + 1
            })

            seen_styles.add(normalized_name)

        logger.info(
            f"âœ… ì¶”ì²œ ë³‘í•© ì™„ë£Œ: Gemini {len(gemini_recommendations)}ê°œ + "
            f"ML {len(ml_recommendations)}ê°œ â†’ ìµœì¢… {len(merged)}ê°œ"
        )

        return merged

    def recommend(
        self,
        image_data: bytes,
        face_shape: str,
        skin_tone: str
    ) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹¤í–‰

        Args:
            image_data: ì´ë¯¸ì§€ ë°”ì´íŠ¸
            face_shape: ì–¼êµ´í˜•
            skin_tone: í”¼ë¶€í†¤

        Returns:
            ì¶”ì²œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ¨ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œì‘: {face_shape} + {skin_tone}")

        # 1. Gemini ì¶”ì²œ (4ê°œ)
        gemini_result = self._call_gemini(image_data, face_shape, skin_tone)
        gemini_recommendations = gemini_result.get("recommendations", [])

        # 2. ML ì¶”ì²œ (Top-3)
        ml_recommendations = []
        if self.ml_available and self.ml_recommender:
            try:
                ml_recommendations = self.ml_recommender.recommend_top_k(
                    face_shape, skin_tone, k=3
                )
            except Exception as e:
                logger.error(f"âŒ ML ì¶”ì²œ ì‹¤íŒ¨: {str(e)}")

        # 3. ë³‘í•© (ì¤‘ë³µ ì œê±°)
        merged_recommendations = self._merge_recommendations(
            gemini_recommendations,
            ml_recommendations,
            face_shape,
            skin_tone
        )

        # 4. ê²°ê³¼ êµ¬ì„±
        result = {
            "analysis": gemini_result.get("analysis", {
                "face_shape": face_shape,
                "personal_color": skin_tone,
                "features": "ìë™ ë¶„ì„"
            }),
            "recommendations": merged_recommendations,
            "meta": {
                "total_count": len(merged_recommendations),
                "gemini_count": len([r for r in merged_recommendations if r["source"] == "gemini"]),
                "ml_count": len([r for r in merged_recommendations if r["source"] == "ml"]),
                "method": "hybrid"
            }
        }

        logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì™„ë£Œ: ì´ {len(merged_recommendations)}ê°œ")

        return result


# ========== ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ==========
_hybrid_service_instance = None


def create_hybrid_service(gemini_api_key: str) -> HybridRecommendationService:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (íŒ©í† ë¦¬ í•¨ìˆ˜)

    ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    FastAPI ì˜ì¡´ì„± ì£¼ì…ìš©ìœ¼ë¡œëŠ” core.dependencies.get_hybrid_service()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

    Args:
        gemini_api_key: Gemini API í‚¤

    Returns:
        HybridRecommendationService ì¸ìŠ¤í„´ìŠ¤
    """
    global _hybrid_service_instance

    if _hybrid_service_instance is None:
        logger.info("ğŸ”§ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        _hybrid_service_instance = HybridRecommendationService(gemini_api_key)
        logger.info("âœ… í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")

    return _hybrid_service_instance
