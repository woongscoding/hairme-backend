"""
ML ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ì§€í‘œ ëª¨ë“ˆ

ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê¸° ìœ„í•œ í‘œì¤€ ë©”íŠ¸ë¦­:
- Precision@K: Top-K ì¶”ì²œ ì¤‘ ì ì¤‘ë¥ 
- Recall@K: ì „ì²´ ê´€ë ¨ ì•„ì´í…œ ì¤‘ Top-Kì—ì„œì˜ ì ì¤‘ë¥ 
- NDCG@K: ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì •ê·œí™”ëœ í• ì¸ ëˆ„ì  ì´ë“
- MRR: í‰ê·  ì—­ìˆœìœ„ (ì²« ë²ˆì§¸ ì ì¤‘ ìœ„ì¹˜)
- Hit Rate@K: Top-K ì¤‘ í•˜ë‚˜ë¼ë„ ë§ìœ¼ë©´ ì ì¤‘

Author: HairMe ML Team
Date: 2025-12-03
Version: 1.0.0
"""

import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import json
import logging

logger = logging.getLogger(__name__)


@dataclass
class RecommendationMetrics:
    """ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ì§€í‘œ ê²°ê³¼"""

    # ê¸°ë³¸ ì§€í‘œ
    precision_at_k: Dict[int, float] = field(default_factory=dict)
    recall_at_k: Dict[int, float] = field(default_factory=dict)
    ndcg_at_k: Dict[int, float] = field(default_factory=dict)
    mrr: float = 0.0
    hit_rate_at_k: Dict[int, float] = field(default_factory=dict)

    # ì¶”ê°€ ì§€í‘œ
    coverage: float = 0.0  # ì¶”ì²œëœ ì•„ì´í…œì˜ ë‹¤ì–‘ì„± (ì „ì²´ ì•„ì´í…œ ì¤‘ ì¶”ì²œëœ ë¹„ìœ¨)
    avg_score: float = 0.0  # í‰ê·  ì¶”ì²œ ì ìˆ˜

    # ë©”íƒ€ ì •ë³´
    num_samples: int = 0
    evaluated_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "precision_at_k": self.precision_at_k,
            "recall_at_k": self.recall_at_k,
            "ndcg_at_k": self.ndcg_at_k,
            "mrr": self.mrr,
            "hit_rate_at_k": self.hit_rate_at_k,
            "coverage": self.coverage,
            "avg_score": self.avg_score,
            "num_samples": self.num_samples,
            "evaluated_at": self.evaluated_at
        }

    def to_json(self) -> str:
        """JSON ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)

    def summary(self) -> str:
        """ìš”ì•½ ë¬¸ìì—´ ìƒì„±"""
        lines = [
            "=" * 50,
            "ğŸ“Š ML ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ê²°ê³¼",
            "=" * 50,
            f"í‰ê°€ ìƒ˜í”Œ ìˆ˜: {self.num_samples}",
            "",
            "ğŸ“ˆ ì£¼ìš” ì§€í‘œ:",
        ]

        # Precision@K
        for k, v in sorted(self.precision_at_k.items()):
            lines.append(f"  Precision@{k}: {v:.4f} ({v*100:.2f}%)")

        # NDCG@K
        for k, v in sorted(self.ndcg_at_k.items()):
            lines.append(f"  NDCG@{k}: {v:.4f}")

        # Hit Rate@K
        for k, v in sorted(self.hit_rate_at_k.items()):
            lines.append(f"  Hit Rate@{k}: {v:.4f} ({v*100:.2f}%)")

        lines.extend([
            f"  MRR: {self.mrr:.4f}",
            f"  Coverage: {self.coverage:.4f} ({self.coverage*100:.2f}%)",
            f"  í‰ê·  ì¶”ì²œ ì ìˆ˜: {self.avg_score:.2f}",
            "",
            f"í‰ê°€ ì‹œê°: {self.evaluated_at}",
            "=" * 50
        ])

        return "\n".join(lines)


class RecommendationEvaluator:
    """
    ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ê¸°

    í”¼ë“œë°± ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

    í‰ê°€ ë°©ë²•:
    1. ê¸ì • í”¼ë“œë°± (positive): ì‚¬ìš©ìê°€ "ì¢‹ìŒ"ìœ¼ë¡œ í‰ê°€í•˜ê±°ë‚˜ í´ë¦­í•œ ìŠ¤íƒ€ì¼
    2. ì¶”ì²œ ê²°ê³¼ì™€ ê¸ì • í”¼ë“œë°± ë¹„êµ
    """

    def __init__(self, k_values: List[int] = None):
        """
        Args:
            k_values: í‰ê°€í•  K ê°’ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: [1, 3, 5])
        """
        self.k_values = k_values or [1, 3, 5]

    def precision_at_k(
        self,
        recommended: List[str],
        relevant: List[str],
        k: int
    ) -> float:
        """
        Precision@K: Top-K ì¶”ì²œ ì¤‘ ê´€ë ¨ ì•„ì´í…œ ë¹„ìœ¨

        Args:
            recommended: ì¶”ì²œëœ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
            relevant: ê´€ë ¨ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ì •ë‹µ)
            k: ìƒìœ„ Kê°œë§Œ í‰ê°€

        Returns:
            Precision@K ê°’ (0.0 ~ 1.0)
        """
        if not recommended or k <= 0:
            return 0.0

        top_k = recommended[:k]
        relevant_set = set(relevant)
        hits = sum(1 for item in top_k if item in relevant_set)

        return hits / k

    def recall_at_k(
        self,
        recommended: List[str],
        relevant: List[str],
        k: int
    ) -> float:
        """
        Recall@K: ì „ì²´ ê´€ë ¨ ì•„ì´í…œ ì¤‘ Top-Kì—ì„œ ì ì¤‘í•œ ë¹„ìœ¨

        Args:
            recommended: ì¶”ì²œëœ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
            relevant: ê´€ë ¨ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ì •ë‹µ)
            k: ìƒìœ„ Kê°œë§Œ í‰ê°€

        Returns:
            Recall@K ê°’ (0.0 ~ 1.0)
        """
        if not relevant or k <= 0:
            return 0.0

        top_k = recommended[:k]
        relevant_set = set(relevant)
        hits = sum(1 for item in top_k if item in relevant_set)

        return hits / len(relevant)

    def dcg_at_k(
        self,
        recommended: List[str],
        relevant: List[str],
        k: int,
        relevance_scores: Dict[str, float] = None
    ) -> float:
        """
        DCG@K: í• ì¸ ëˆ„ì  ì´ë“ (ìˆœìœ„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì ìš©)

        Args:
            recommended: ì¶”ì²œëœ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
            relevant: ê´€ë ¨ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
            k: ìƒìœ„ Kê°œë§Œ í‰ê°€
            relevance_scores: ì•„ì´í…œë³„ ê´€ë ¨ì„± ì ìˆ˜ (ì—†ìœ¼ë©´ ì´ì§„: 1 ë˜ëŠ” 0)

        Returns:
            DCG@K ê°’
        """
        if not recommended or k <= 0:
            return 0.0

        top_k = recommended[:k]
        relevant_set = set(relevant)

        dcg = 0.0
        for i, item in enumerate(top_k):
            if relevance_scores and item in relevance_scores:
                rel = relevance_scores[item]
            elif item in relevant_set:
                rel = 1.0
            else:
                rel = 0.0

            # DCG ê³µì‹: rel_i / log2(i + 2)
            dcg += rel / np.log2(i + 2)

        return dcg

    def ndcg_at_k(
        self,
        recommended: List[str],
        relevant: List[str],
        k: int,
        relevance_scores: Dict[str, float] = None
    ) -> float:
        """
        NDCG@K: ì •ê·œí™”ëœ í• ì¸ ëˆ„ì  ì´ë“

        ì´ìƒì ì¸ ìˆœì„œ ëŒ€ë¹„ ì‹¤ì œ ìˆœì„œì˜ í’ˆì§ˆì„ ì¸¡ì •í•©ë‹ˆë‹¤.

        Args:
            recommended: ì¶”ì²œëœ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
            relevant: ê´€ë ¨ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
            k: ìƒìœ„ Kê°œë§Œ í‰ê°€
            relevance_scores: ì•„ì´í…œë³„ ê´€ë ¨ì„± ì ìˆ˜

        Returns:
            NDCG@K ê°’ (0.0 ~ 1.0)
        """
        dcg = self.dcg_at_k(recommended, relevant, k, relevance_scores)

        if dcg == 0:
            return 0.0

        # ì´ìƒì ì¸ ìˆœì„œ: ê´€ë ¨ì„± ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        if relevance_scores:
            ideal_order = sorted(
                relevant,
                key=lambda x: relevance_scores.get(x, 0.0),
                reverse=True
            )
        else:
            ideal_order = relevant.copy()

        idcg = self.dcg_at_k(ideal_order, relevant, k, relevance_scores)

        if idcg == 0:
            return 0.0

        return dcg / idcg

    def mrr(
        self,
        recommended: List[str],
        relevant: List[str]
    ) -> float:
        """
        MRR (Mean Reciprocal Rank): ì²« ë²ˆì§¸ ì ì¤‘ ìœ„ì¹˜ì˜ ì—­ìˆ˜

        ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ RR (Reciprocal Rank)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì—¬ëŸ¬ ì¿¼ë¦¬ì˜ í‰ê· ì€ evaluate() ë©”ì„œë“œì—ì„œ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            recommended: ì¶”ì²œëœ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
            relevant: ê´€ë ¨ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸

        Returns:
            RR ê°’ (0.0 ~ 1.0)
        """
        if not recommended or not relevant:
            return 0.0

        relevant_set = set(relevant)

        for i, item in enumerate(recommended):
            if item in relevant_set:
                return 1.0 / (i + 1)

        return 0.0

    def hit_rate_at_k(
        self,
        recommended: List[str],
        relevant: List[str],
        k: int
    ) -> float:
        """
        Hit Rate@K: Top-K ì¤‘ í•˜ë‚˜ë¼ë„ ê´€ë ¨ ì•„ì´í…œì´ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0

        Args:
            recommended: ì¶”ì²œëœ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
            relevant: ê´€ë ¨ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
            k: ìƒìœ„ Kê°œë§Œ í‰ê°€

        Returns:
            1.0 (ì ì¤‘) ë˜ëŠ” 0.0 (ë¯¸ì ì¤‘)
        """
        if not recommended or not relevant or k <= 0:
            return 0.0

        top_k = recommended[:k]
        relevant_set = set(relevant)

        for item in top_k:
            if item in relevant_set:
                return 1.0

        return 0.0

    def evaluate_single(
        self,
        recommended: List[str],
        relevant: List[str],
        relevance_scores: Dict[str, float] = None
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì¶”ì²œ ê²°ê³¼ í‰ê°€

        Args:
            recommended: ì¶”ì²œëœ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
            relevant: ê´€ë ¨ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ì •ë‹µ)
            relevance_scores: ì•„ì´í…œë³„ ê´€ë ¨ì„± ì ìˆ˜ (ì„ íƒ)

        Returns:
            ê° ì§€í‘œë³„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            "precision_at_k": {},
            "recall_at_k": {},
            "ndcg_at_k": {},
            "hit_rate_at_k": {},
            "rr": self.mrr(recommended, relevant)
        }

        for k in self.k_values:
            result["precision_at_k"][k] = self.precision_at_k(recommended, relevant, k)
            result["recall_at_k"][k] = self.recall_at_k(recommended, relevant, k)
            result["ndcg_at_k"][k] = self.ndcg_at_k(recommended, relevant, k, relevance_scores)
            result["hit_rate_at_k"][k] = self.hit_rate_at_k(recommended, relevant, k)

        return result

    def evaluate_batch(
        self,
        predictions: List[Tuple[List[str], List[str]]],
        relevance_scores_list: List[Dict[str, float]] = None,
        all_items: List[str] = None
    ) -> RecommendationMetrics:
        """
        ì—¬ëŸ¬ ì¶”ì²œ ê²°ê³¼ë¥¼ ë°°ì¹˜ë¡œ í‰ê°€

        Args:
            predictions: [(ì¶”ì²œ ë¦¬ìŠ¤íŠ¸, ì •ë‹µ ë¦¬ìŠ¤íŠ¸), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
            relevance_scores_list: ê° ì˜ˆì¸¡ì— ëŒ€í•œ ê´€ë ¨ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)
            all_items: ì „ì²´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (coverage ê³„ì‚°ìš©)

        Returns:
            RecommendationMetrics ê°ì²´
        """
        if not predictions:
            return RecommendationMetrics()

        n = len(predictions)

        # ê° ì§€í‘œ ëˆ„ì 
        precision_sums = {k: 0.0 for k in self.k_values}
        recall_sums = {k: 0.0 for k in self.k_values}
        ndcg_sums = {k: 0.0 for k in self.k_values}
        hit_rate_sums = {k: 0.0 for k in self.k_values}
        mrr_sum = 0.0
        score_sum = 0.0
        score_count = 0

        # ì¶”ì²œëœ ì•„ì´í…œ ì¶”ì  (coverage ê³„ì‚°ìš©)
        recommended_items = set()

        for i, (recommended, relevant) in enumerate(predictions):
            rel_scores = relevance_scores_list[i] if relevance_scores_list else None

            # ë‹¨ì¼ í‰ê°€
            single_result = self.evaluate_single(recommended, relevant, rel_scores)

            # ëˆ„ì 
            for k in self.k_values:
                precision_sums[k] += single_result["precision_at_k"][k]
                recall_sums[k] += single_result["recall_at_k"][k]
                ndcg_sums[k] += single_result["ndcg_at_k"][k]
                hit_rate_sums[k] += single_result["hit_rate_at_k"][k]

            mrr_sum += single_result["rr"]

            # ì¶”ì²œ ì•„ì´í…œ ì¶”ì 
            recommended_items.update(recommended)

            # ê´€ë ¨ì„± ì ìˆ˜ ëˆ„ì 
            if rel_scores:
                for item in recommended:
                    if item in rel_scores:
                        score_sum += rel_scores[item]
                        score_count += 1

        # í‰ê·  ê³„ì‚°
        metrics = RecommendationMetrics(
            precision_at_k={k: v / n for k, v in precision_sums.items()},
            recall_at_k={k: v / n for k, v in recall_sums.items()},
            ndcg_at_k={k: v / n for k, v in ndcg_sums.items()},
            hit_rate_at_k={k: v / n for k, v in hit_rate_sums.items()},
            mrr=mrr_sum / n,
            num_samples=n,
            avg_score=score_sum / score_count if score_count > 0 else 0.0
        )

        # Coverage ê³„ì‚°
        if all_items:
            metrics.coverage = len(recommended_items) / len(all_items)

        return metrics


class FeedbackEvaluator:
    """
    í”¼ë“œë°± ê¸°ë°˜ í‰ê°€ê¸°

    ì‹¤ì œ ì‚¬ìš©ì í”¼ë“œë°± ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        positive_threshold: float = 70.0,
        k_values: List[int] = None
    ):
        """
        Args:
            positive_threshold: ê¸ì • í”¼ë“œë°±ìœ¼ë¡œ ê°„ì£¼í•  ì ìˆ˜ ì„ê³„ê°’
            k_values: í‰ê°€í•  K ê°’ ë¦¬ìŠ¤íŠ¸
        """
        self.positive_threshold = positive_threshold
        self.evaluator = RecommendationEvaluator(k_values or [1, 3, 5])

    def evaluate_from_feedback(
        self,
        feedback_data: List[Dict[str, Any]],
        model_predictions: List[List[str]] = None,
        all_styles: List[str] = None
    ) -> RecommendationMetrics:
        """
        í”¼ë“œë°± ë°ì´í„°ë¡œë¶€í„° í‰ê°€

        Args:
            feedback_data: í”¼ë“œë°± ë°ì´í„° ë¦¬ìŠ¤íŠ¸
                [{
                    "recommended_styles": ["ìŠ¤íƒ€ì¼1", "ìŠ¤íƒ€ì¼2", "ìŠ¤íƒ€ì¼3"],
                    "selected_style": "ìŠ¤íƒ€ì¼1",  # ì‚¬ìš©ìê°€ ì„ íƒí•œ ìŠ¤íƒ€ì¼
                    "feedback": "good" | "bad",
                    "score": 90  # í”¼ë“œë°± ì ìˆ˜ (ì„ íƒ)
                }, ...]
            model_predictions: ëª¨ë¸ ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ feedback_dataì—ì„œ ì¶”ì¶œ)
            all_styles: ì „ì²´ ìŠ¤íƒ€ì¼ ë¦¬ìŠ¤íŠ¸ (coverage ê³„ì‚°ìš©)

        Returns:
            RecommendationMetrics ê°ì²´
        """
        predictions = []
        relevance_scores_list = []

        for item in feedback_data:
            # ì¶”ì²œ ê²°ê³¼
            recommended = item.get("recommended_styles", [])

            # ì •ë‹µ (ê¸ì • í”¼ë“œë°±ì„ ë°›ì€ ìŠ¤íƒ€ì¼)
            relevant = []
            relevance_scores = {}

            selected_style = item.get("selected_style")
            feedback = item.get("feedback", "").lower()
            score = item.get("score", 0)

            # ê¸ì • í”¼ë“œë°±ì¸ ê²½ìš° í•´ë‹¹ ìŠ¤íƒ€ì¼ì„ ì •ë‹µìœ¼ë¡œ
            if feedback == "good" or score >= self.positive_threshold:
                if selected_style:
                    relevant.append(selected_style)
                    # ì •ê·œí™”ëœ ê´€ë ¨ì„± ì ìˆ˜ (0~1)
                    relevance_scores[selected_style] = score / 100.0 if score else 0.9

            # ì¶”ì²œì´ ìˆê³  ì •ë‹µë„ ìˆëŠ” ê²½ìš°ë§Œ í‰ê°€
            if recommended and relevant:
                predictions.append((recommended, relevant))
                relevance_scores_list.append(relevance_scores)

        if not predictions:
            logger.warning("âš ï¸ í‰ê°€ ê°€ëŠ¥í•œ í”¼ë“œë°± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return RecommendationMetrics()

        logger.info(f"ğŸ“Š {len(predictions)}ê°œ ìƒ˜í”Œë¡œ í‰ê°€ ìˆ˜í–‰")

        return self.evaluator.evaluate_batch(
            predictions,
            relevance_scores_list,
            all_styles
        )

    def calculate_business_metrics(
        self,
        feedback_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ ê³„ì‚°

        Args:
            feedback_data: í”¼ë“œë°± ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        total = len(feedback_data)
        if total == 0:
            return {
                "total_feedback": 0,
                "positive_rate": 0.0,
                "negative_rate": 0.0,
                "click_rate": 0.0,
                "avg_score": 0.0
            }

        positive_count = 0
        negative_count = 0
        click_count = 0
        score_sum = 0.0
        score_count = 0

        for item in feedback_data:
            feedback = item.get("feedback", "").lower()
            score = item.get("score", 0)
            clicked = item.get("naver_clicked", False)

            if feedback == "good" or score >= self.positive_threshold:
                positive_count += 1
            elif feedback == "bad" or (score > 0 and score < self.positive_threshold):
                negative_count += 1

            if clicked:
                click_count += 1

            if score > 0:
                score_sum += score
                score_count += 1

        return {
            "total_feedback": total,
            "positive_count": positive_count,
            "negative_count": negative_count,
            "positive_rate": positive_count / total,
            "negative_rate": negative_count / total,
            "click_rate": click_count / total,
            "avg_score": score_sum / score_count if score_count > 0 else 0.0,
            "evaluated_at": datetime.utcnow().isoformat()
        }


# ========== í¸ì˜ í•¨ìˆ˜ ==========

def evaluate_model_performance(
    predictions: List[Tuple[List[str], List[str]]],
    k_values: List[int] = None
) -> RecommendationMetrics:
    """
    ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (í¸ì˜ í•¨ìˆ˜)

    Args:
        predictions: [(ì¶”ì²œ ë¦¬ìŠ¤íŠ¸, ì •ë‹µ ë¦¬ìŠ¤íŠ¸), ...]
        k_values: í‰ê°€í•  K ê°’ ë¦¬ìŠ¤íŠ¸

    Returns:
        RecommendationMetrics ê°ì²´
    """
    evaluator = RecommendationEvaluator(k_values or [1, 3, 5])
    return evaluator.evaluate_batch(predictions)


def evaluate_from_s3_feedback(
    s3_bucket: str,
    feedback_prefix: str = "feedback/processed/",
    positive_threshold: float = 70.0
) -> RecommendationMetrics:
    """
    S3ì— ì €ì¥ëœ í”¼ë“œë°±ìœ¼ë¡œ í‰ê°€ (í¸ì˜ í•¨ìˆ˜)

    Args:
        s3_bucket: S3 ë²„í‚· ì´ë¦„
        feedback_prefix: í”¼ë“œë°± íŒŒì¼ prefix
        positive_threshold: ê¸ì • í”¼ë“œë°± ì„ê³„ê°’

    Returns:
        RecommendationMetrics ê°ì²´
    """
    try:
        import boto3
        import numpy as np

        s3 = boto3.client('s3')

        # í”¼ë“œë°± íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        response = s3.list_objects_v2(
            Bucket=s3_bucket,
            Prefix=feedback_prefix
        )

        feedback_data = []

        for obj in response.get('Contents', []):
            key = obj['Key']
            if key.endswith('.npz'):
                # NPZ íŒŒì¼ ë¡œë“œ
                result = s3.get_object(Bucket=s3_bucket, Key=key)
                data = np.load(result['Body'], allow_pickle=True)

                # ground_truthë¥¼ ì ìˆ˜ë¡œ ì‚¬ìš©
                if 'ground_truth' in data:
                    score = float(data['ground_truth'])
                    feedback_data.append({
                        "score": score,
                        "feedback": "good" if score >= positive_threshold else "bad"
                    })

        if not feedback_data:
            logger.warning(f"âš ï¸ {feedback_prefix}ì—ì„œ í”¼ë“œë°± ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return RecommendationMetrics()

        evaluator = FeedbackEvaluator(positive_threshold)
        return evaluator.calculate_business_metrics(feedback_data)

    except Exception as e:
        logger.error(f"âŒ S3 í”¼ë“œë°± í‰ê°€ ì‹¤íŒ¨: {e}")
        return RecommendationMetrics()


# ========== í…ŒìŠ¤íŠ¸ìš© ==========

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_predictions = [
        (["ìŠ¤íƒ€ì¼A", "ìŠ¤íƒ€ì¼B", "ìŠ¤íƒ€ì¼C"], ["ìŠ¤íƒ€ì¼A"]),
        (["ìŠ¤íƒ€ì¼D", "ìŠ¤íƒ€ì¼E", "ìŠ¤íƒ€ì¼F"], ["ìŠ¤íƒ€ì¼E"]),
        (["ìŠ¤íƒ€ì¼G", "ìŠ¤íƒ€ì¼H", "ìŠ¤íƒ€ì¼I"], ["ìŠ¤íƒ€ì¼J"]),  # Miss
        (["ìŠ¤íƒ€ì¼K", "ìŠ¤íƒ€ì¼L", "ìŠ¤íƒ€ì¼M"], ["ìŠ¤íƒ€ì¼K", "ìŠ¤íƒ€ì¼L"]),  # 2ê°œ ì ì¤‘
    ]

    # í‰ê°€ ì‹¤í–‰
    evaluator = RecommendationEvaluator(k_values=[1, 3])
    metrics = evaluator.evaluate_batch(test_predictions)

    print(metrics.summary())
    print("\nğŸ“‹ JSON ì¶œë ¥:")
    print(metrics.to_json())
