"""
Trainer Lambda í•¨ìˆ˜

S3ì—ì„œ í”¼ë“œë°± ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì„ ì¬í•™ìŠµí•©ë‹ˆë‹¤.

ì‹¤í–‰ í™˜ê²½:
    - Lambda (15ë¶„ ì œí•œ, 10GB ë©”ëª¨ë¦¬) ë˜ëŠ”
    - EC2 Spot Instance (ëŒ€ìš©ëŸ‰ í•™ìŠµ)

Flow:
    1. S3ì—ì„œ pending í”¼ë“œë°± ë°ì´í„° ë¡œë“œ
    2. ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì™€ ë³‘í•©
    3. ëª¨ë¸ ì¬í•™ìŠµ (incremental ë˜ëŠ” full)
    4. ìƒˆ ëª¨ë¸ì„ S3ì— ì—…ë¡œë“œ
    5. Lambda í•¨ìˆ˜ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±° (ì„ íƒ)

ë¹„ìš©:
    - Lambda (10GB, 15ë¶„): ~$0.15/ì‹¤í–‰
    - EC2 Spot t3.medium: ~$0.01/ì‹œê°„

Author: HairMe ML Team
Date: 2025-12-02
"""

import os
import io
import json
import tempfile
import logging
from datetime import datetime, timezone
from typing import Dict, Any, Optional, Tuple
from pathlib import Path

import numpy as np

logger = logging.getLogger(__name__)

# Configuration
S3_BUCKET = os.getenv('MLOPS_S3_BUCKET', 'hairme-mlops')
MODEL_VERSION_PREFIX = 'v6'
MIN_SAMPLES_FOR_TRAINING = 50  # ìµœì†Œ í•™ìŠµ ìƒ˜í”Œ ìˆ˜

try:
    import boto3
    from botocore.exceptions import ClientError
    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False


class ModelTrainer:
    """
    ML ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ

    Lambda ë˜ëŠ” EC2ì—ì„œ ì‹¤í–‰ë˜ì–´ í”¼ë“œë°± ê¸°ë°˜ ì¬í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """

    def __init__(self, s3_bucket: str = S3_BUCKET):
        """ì´ˆê¸°í™”"""
        self.s3_bucket = s3_bucket
        self.s3_client = None
        self.device = None

        if BOTO3_AVAILABLE:
            self.s3_client = boto3.client('s3')

        if TORCH_AVAILABLE:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            logger.info(f"ğŸ”§ PyTorch device: {self.device}")

    def load_pending_data(self) -> Tuple[Optional[np.ndarray], ...]:
        """
        S3ì—ì„œ pending í”¼ë“œë°± ë°ì´í„° ë¡œë“œ

        Returns:
            (face_features, skin_features, style_embeddings, ground_truths)
        """
        if not self.s3_client:
            return None, None, None, None

        try:
            # pending í´ë”ì˜ ëª¨ë“  NPZ íŒŒì¼ ëª©ë¡
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_bucket,
                Prefix='feedback/pending/'
            )

            if 'Contents' not in response:
                logger.info("â„¹ï¸ ëŒ€ê¸° ì¤‘ì¸ í”¼ë“œë°± ì—†ìŒ")
                return None, None, None, None

            face_list = []
            skin_list = []
            style_list = []
            gt_list = []

            for obj in response['Contents']:
                key = obj['Key']
                if not key.endswith('.npz'):
                    continue

                # NPZ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                obj_response = self.s3_client.get_object(
                    Bucket=self.s3_bucket,
                    Key=key
                )

                buffer = io.BytesIO(obj_response['Body'].read())
                data = np.load(buffer, allow_pickle=True)

                face_list.append(data['face_features'])
                skin_list.append(data['skin_features'])
                style_list.append(data['style_embedding'])
                gt_list.append(data['ground_truth'])

            if not face_list:
                return None, None, None, None

            logger.info(f"âœ… {len(face_list)}ê°œ í”¼ë“œë°± ë°ì´í„° ë¡œë“œ")

            return (
                np.stack(face_list),
                np.stack(skin_list),
                np.stack(style_list),
                np.concatenate(gt_list)
            )

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None, None, None

    def load_base_model(self) -> Optional[nn.Module]:
        """
        S3ì—ì„œ í˜„ì¬ ìš´ì˜ ëª¨ë¸ ë¡œë“œ

        Returns:
            PyTorch ëª¨ë¸ ë˜ëŠ” None
        """
        if not TORCH_AVAILABLE or not self.s3_client:
            return None

        try:
            # í˜„ì¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            with tempfile.NamedTemporaryFile(suffix='.pt', delete=False) as tmp:
                self.s3_client.download_file(
                    self.s3_bucket,
                    'models/current/model.pt',
                    tmp.name
                )

                checkpoint = torch.load(tmp.name, map_location=self.device, weights_only=False)

                # ëª¨ë¸ í´ë˜ìŠ¤ ë™ì  ì„í¬íŠ¸
                from models.ml_recommender import RecommendationModelV6

                model = RecommendationModelV6()

                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)

                model.to(self.device)
                logger.info("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp.name)

                return model

        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                logger.warning("âš ï¸ ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ - ìƒˆë¡œ í•™ìŠµ ì‹œì‘")
                return None
            raise
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def train(
        self,
        face_features: np.ndarray,
        skin_features: np.ndarray,
        style_embeddings: np.ndarray,
        ground_truths: np.ndarray,
        base_model: Optional[nn.Module] = None,
        epochs: int = 10,
        learning_rate: float = 0.0001,
        batch_size: int = 32
    ) -> Tuple[nn.Module, Dict[str, Any]]:
        """
        ëª¨ë¸ í•™ìŠµ

        Args:
            face_features: (N, 6) ì–¼êµ´ íŠ¹ì§•
            skin_features: (N, 2) í”¼ë¶€ íŠ¹ì§•
            style_embeddings: (N, 384) ìŠ¤íƒ€ì¼ ì„ë² ë”©
            ground_truths: (N,) ì •ë‹µ ë ˆì´ë¸”
            base_model: ê¸°ì¡´ ëª¨ë¸ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
            epochs: í•™ìŠµ ì—í­
            learning_rate: í•™ìŠµë¥ 
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            (trained_model, training_history)
        """
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorch not available")

        # ë¼ë²¨ ì •ê·œí™” (10~95 -> 0~1)
        LABEL_MIN, LABEL_MAX = 10.0, 95.0
        normalized_labels = (ground_truths - LABEL_MIN) / (LABEL_MAX - LABEL_MIN)

        # í…ì„œ ë³€í™˜
        face_tensor = torch.FloatTensor(face_features).to(self.device)
        skin_tensor = torch.FloatTensor(skin_features).to(self.device)
        style_tensor = torch.FloatTensor(style_embeddings).to(self.device)
        label_tensor = torch.FloatTensor(normalized_labels).to(self.device)

        # DataLoader ìƒì„±
        dataset = TensorDataset(face_tensor, skin_tensor, style_tensor, label_tensor)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        # ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
        if base_model is None:
            from models.ml_recommender import RecommendationModelV6
            model = RecommendationModelV6()
            model.to(self.device)
            logger.info("ğŸ†• ìƒˆ ëª¨ë¸ ìƒì„±")
        else:
            model = base_model
            logger.info("ğŸ“‚ ê¸°ì¡´ ëª¨ë¸ì—ì„œ fine-tuning")

        model.train()

        # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)

        # í•™ìŠµ íˆìŠ¤í† ë¦¬
        history = {
            'epochs': epochs,
            'losses': [],
            'samples': len(ground_truths),
            'started_at': datetime.now(timezone.utc).isoformat()
        }

        # í•™ìŠµ ë£¨í”„
        for epoch in range(epochs):
            epoch_loss = 0.0
            batch_count = 0

            for face_batch, skin_batch, style_batch, label_batch in dataloader:
                optimizer.zero_grad()

                predictions = model(face_batch, skin_batch, style_batch)
                loss = criterion(predictions, label_batch)

                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                batch_count += 1

            avg_loss = epoch_loss / batch_count
            history['losses'].append(avg_loss)

            if (epoch + 1) % 2 == 0:
                logger.info(f"  Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.6f}")

        history['final_loss'] = history['losses'][-1]
        history['finished_at'] = datetime.now(timezone.utc).isoformat()

        logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ: final_loss={history['final_loss']:.6f}")

        return model, history

    def save_model(
        self,
        model: nn.Module,
        history: Dict[str, Any],
        version: Optional[str] = None
    ) -> str:
        """
        í•™ìŠµëœ ëª¨ë¸ì„ S3ì— ì €ì¥

        Args:
            model: í•™ìŠµëœ ëª¨ë¸
            history: í•™ìŠµ íˆìŠ¤í† ë¦¬
            version: ëª¨ë¸ ë²„ì „ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)

        Returns:
            S3 í‚¤
        """
        if not self.s3_client:
            raise RuntimeError("S3 client not initialized")

        # ë²„ì „ ìƒì„±
        timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
        version = version or f"{MODEL_VERSION_PREFIX}_{timestamp}"

        # ì²´í¬í¬ì¸íŠ¸ ìƒì„±
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'config': {
                'version': 'v6',
                'normalized': True,
                'attention_type': 'multi_token',
                'token_dim': 128,
                'num_heads': 4
            },
            'training_history': history,
            'created_at': datetime.now(timezone.utc).isoformat()
        }

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(suffix='.pt', delete=False) as tmp:
            torch.save(checkpoint, tmp.name)

            # í˜„ì¬ ëª¨ë¸ë¡œ ì—…ë¡œë“œ
            current_key = 'models/current/model.pt'
            self.s3_client.upload_file(tmp.name, self.s3_bucket, current_key)

            # ì•„ì¹´ì´ë¸Œì—ë„ ì €ì¥
            archive_key = f'models/archive/{version}.pt'
            self.s3_client.upload_file(tmp.name, self.s3_bucket, archive_key)

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp.name)

        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        metadata = {
            'version': version,
            'samples_trained': history.get('samples', 0),
            'final_loss': history.get('final_loss', 0),
            'updated_at': datetime.now(timezone.utc).isoformat()
        }

        self.s3_client.put_object(
            Bucket=self.s3_bucket,
            Key='models/current/metadata.json',
            Body=json.dumps(metadata, indent=2),
            ContentType='application/json'
        )

        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {current_key}")

        return current_key

    def mark_data_processed(self):
        """pending ë°ì´í„°ë¥¼ processedë¡œ ì´ë™"""
        if not self.s3_client:
            return

        try:
            timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d_%H%M%S')
            batch_name = f"batch_{timestamp}"

            # pending íŒŒì¼ ëª©ë¡
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_bucket,
                Prefix='feedback/pending/'
            )

            if 'Contents' not in response:
                return

            for obj in response['Contents']:
                old_key = obj['Key']
                if not old_key.endswith('.npz'):
                    continue

                filename = old_key.split('/')[-1]
                new_key = f"feedback/processed/{batch_name}/{filename}"

                # Copy then delete
                self.s3_client.copy_object(
                    Bucket=self.s3_bucket,
                    CopySource={'Bucket': self.s3_bucket, 'Key': old_key},
                    Key=new_key
                )
                self.s3_client.delete_object(Bucket=self.s3_bucket, Key=old_key)

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            try:
                metadata_response = self.s3_client.get_object(
                    Bucket=self.s3_bucket,
                    Key='feedback/metadata.json'
                )
                metadata = json.loads(metadata_response['Body'].read().decode('utf-8'))
            except Exception:
                metadata = {}

            metadata['pending_count'] = 0
            metadata['last_training_at'] = datetime.now(timezone.utc).isoformat()

            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key='feedback/metadata.json',
                Body=json.dumps(metadata, indent=2),
                ContentType='application/json'
            )

            logger.info(f"âœ… ë°ì´í„° processedë¡œ ì´ë™: {batch_name}")

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì´ë™ ì‹¤íŒ¨: {e}")


def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Lambda í•¸ë“¤ëŸ¬

    EventBridge ë˜ëŠ” ì§ì ‘ í˜¸ì¶œë¡œ íŠ¸ë¦¬ê±°ë©ë‹ˆë‹¤.

    Args:
        event: {
            "trigger_type": "scheduled" | "data_threshold" | "manual",
            "metadata": {...}
        }
        context: Lambda context

    Returns:
        {
            "success": bool,
            "message": str,
            "model_version": str (if success)
        }
    """
    logger.info(f"ğŸš€ Trainer Lambda ì‹œì‘: {json.dumps(event)}")

    trigger_type = event.get('trigger_type', 'unknown')

    try:
        trainer = ModelTrainer()

        # 1. ë°ì´í„° ë¡œë“œ
        face, skin, style, labels = trainer.load_pending_data()

        if face is None or len(face) < MIN_SAMPLES_FOR_TRAINING:
            sample_count = len(face) if face is not None else 0
            logger.info(f"â„¹ï¸ í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {sample_count}/{MIN_SAMPLES_FOR_TRAINING}")
            return {
                "success": False,
                "message": f"Insufficient data ({sample_count}/{MIN_SAMPLES_FOR_TRAINING})",
                "trigger_type": trigger_type
            }

        logger.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(labels)}ê°œ ìƒ˜í”Œ")

        # 2. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
        base_model = trainer.load_base_model()

        # 3. í•™ìŠµ
        model, history = trainer.train(
            face_features=face,
            skin_features=skin,
            style_embeddings=style,
            ground_truths=labels,
            base_model=base_model,
            epochs=10,
            learning_rate=0.0001
        )

        # 4. ëª¨ë¸ ì €ì¥
        model_key = trainer.save_model(model, history)

        # 5. ë°ì´í„° ì´ë™
        trainer.mark_data_processed()

        logger.info(f"âœ… ì¬í•™ìŠµ ì™„ë£Œ: {model_key}")

        return {
            "success": True,
            "message": "Training completed successfully",
            "trigger_type": trigger_type,
            "model_key": model_key,
            "samples_trained": len(labels),
            "final_loss": history.get('final_loss', 0)
        }

    except Exception as e:
        logger.error(f"âŒ ì¬í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

        return {
            "success": False,
            "message": f"Training failed: {str(e)}",
            "trigger_type": trigger_type
        }


# ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    result = lambda_handler(
        event={"trigger_type": "manual"},
        context=None
    )

    print(json.dumps(result, indent=2))
