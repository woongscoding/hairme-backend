"""
A/B í…ŒìŠ¤íŠ¸ í‰ê°€ê¸° ëª¨ë“ˆ

DynamoDBì—ì„œ ì‹¤í—˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ Championê³¼ Challenger ëª¨ë¸ì˜
ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

í•µì‹¬ ì§€í‘œ:
- positive_feedback_rate: good / (good + bad)
- score_discrimination: ì¢‹ì€ ìŠ¤íƒ€ì¼ê³¼ ë‚˜ìœ ìŠ¤íƒ€ì¼ì˜ ì ìˆ˜ ì°¨ì´

Author: HairMe ML Team
Date: 2025-12-02
Version: 1.0.0
"""

import os
import logging
from dataclasses import dataclass
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone

logger = logging.getLogger(__name__)


@dataclass
class ABTestMetrics:
    """
    A/B í…ŒìŠ¤íŠ¸ ë³€í˜•ë³„ ì§€í‘œ

    Attributes:
        variant: "champion" or "challenger"
        sample_count: ì´ í”¼ë“œë°± ìˆ˜
        positive_count: good í”¼ë“œë°± ìˆ˜
        negative_count: bad í”¼ë“œë°± ìˆ˜
        positive_feedback_rate: good / (good + bad)
        avg_score_for_good: good ë°›ì€ ìŠ¤íƒ€ì¼ì˜ í‰ê·  ì˜ˆì¸¡ ì ìˆ˜
        avg_score_for_bad: bad ë°›ì€ ìŠ¤íƒ€ì¼ì˜ í‰ê·  ì˜ˆì¸¡ ì ìˆ˜
        score_discrimination: avg_good - avg_bad (í´ìˆ˜ë¡ ì¢‹ìŒ)
    """
    variant: str
    sample_count: int = 0
    positive_count: int = 0
    negative_count: int = 0
    positive_feedback_rate: float = 0.0
    avg_score_for_good: float = 0.0
    avg_score_for_bad: float = 0.0
    score_discrimination: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'variant': self.variant,
            'sample_count': self.sample_count,
            'positive_count': self.positive_count,
            'negative_count': self.negative_count,
            'positive_feedback_rate': round(self.positive_feedback_rate, 4),
            'avg_score_for_good': round(self.avg_score_for_good, 2),
            'avg_score_for_bad': round(self.avg_score_for_bad, 2),
            'score_discrimination': round(self.score_discrimination, 2)
        }


class ABTestEvaluator:
    """
    A/B í…ŒìŠ¤íŠ¸ í‰ê°€ê¸°

    DynamoDBì—ì„œ ì‹¤í—˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ë³€í˜•ë³„ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Usage:
        evaluator = ABTestEvaluator()
        metrics = evaluator.get_metrics_by_variant("exp_2025_12_02")
        result = evaluator.is_challenger_better(metrics)
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.dynamodb_table = None
        self._init_dynamodb()

    def _init_dynamodb(self) -> bool:
        """DynamoDB í…Œì´ë¸” ì´ˆê¸°í™”"""
        use_dynamodb = os.getenv('USE_DYNAMODB', 'false').lower() == 'true'
        if not use_dynamodb:
            logger.warning("âš ï¸ DynamoDBê°€ í™œì„±í™”ë˜ì§€ ì•ŠìŒ - A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ë¶ˆê°€")
            return False

        try:
            import boto3
            from botocore.config import Config

            aws_region = os.getenv('AWS_REGION', 'ap-northeast-2')
            table_name = os.getenv('DYNAMODB_TABLE_NAME', 'hairme-analysis')

            config = Config(
                connect_timeout=5,
                read_timeout=10,
                retries={'max_attempts': 3}
            )

            dynamodb = boto3.resource('dynamodb', region_name=aws_region, config=config)
            self.dynamodb_table = dynamodb.Table(table_name)
            return True

        except Exception as e:
            logger.error(f"âŒ DynamoDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def get_metrics_by_variant(
        self,
        experiment_id: str,
        limit: int = 10000
    ) -> Dict[str, ABTestMetrics]:
        """
        ì‹¤í—˜ IDë¡œ ë³€í˜•ë³„ ì§€í‘œ ê³„ì‚°

        DynamoDBì—ì„œ í•´ë‹¹ ì‹¤í—˜ì˜ í”¼ë“œë°± ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³ 
        Championê³¼ Challenger ê°ê°ì˜ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            experiment_id: ì‹¤í—˜ ID (ì˜ˆ: "exp_2025_12_02")
            limit: ìµœëŒ€ ì¡°íšŒ ê±´ìˆ˜

        Returns:
            {
                'champion': ABTestMetrics(...),
                'challenger': ABTestMetrics(...)
            }
        """
        if not self.dynamodb_table:
            logger.error("âŒ DynamoDB í…Œì´ë¸”ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return {}

        try:
            # DynamoDBì—ì„œ ì‹¤í—˜ ë°ì´í„° ì¡°íšŒ
            # Note: ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” GSI (experiment_id-feedback_at-index)ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
            response = self.dynamodb_table.scan(
                FilterExpression='experiment_id = :exp_id AND attribute_exists(feedback_at)',
                ExpressionAttributeValues={
                    ':exp_id': experiment_id
                },
                Limit=limit
            )

            items = response.get('Items', [])

            # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            while 'LastEvaluatedKey' in response and len(items) < limit:
                response = self.dynamodb_table.scan(
                    FilterExpression='experiment_id = :exp_id AND attribute_exists(feedback_at)',
                    ExpressionAttributeValues={
                        ':exp_id': experiment_id
                    },
                    ExclusiveStartKey=response['LastEvaluatedKey'],
                    Limit=limit - len(items)
                )
                items.extend(response.get('Items', []))

            logger.info(f"ğŸ“Š A/B í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¡°íšŒ: experiment={experiment_id}, count={len(items)}")

            # ë³€í˜•ë³„ë¡œ ë°ì´í„° ë¶„ë¥˜
            champion_data = []
            challenger_data = []

            for item in items:
                variant = item.get('ab_variant', 'champion')
                if variant == 'challenger':
                    challenger_data.append(item)
                else:
                    champion_data.append(item)

            # ê° ë³€í˜•ë³„ ì§€í‘œ ê³„ì‚°
            champion_metrics = self._calculate_metrics('champion', champion_data)
            challenger_metrics = self._calculate_metrics('challenger', challenger_data)

            return {
                'champion': champion_metrics,
                'challenger': challenger_metrics
            }

        except Exception as e:
            logger.error(f"âŒ A/B í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def _calculate_metrics(self, variant: str, items: List[Dict]) -> ABTestMetrics:
        """
        ë‹¨ì¼ ë³€í˜•ì— ëŒ€í•œ ì§€í‘œ ê³„ì‚°

        Args:
            variant: "champion" or "challenger"
            items: DynamoDB ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ABTestMetrics ì¸ìŠ¤í„´ìŠ¤
        """
        metrics = ABTestMetrics(variant=variant)

        if not items:
            return metrics

        good_scores = []
        bad_scores = []

        for item in items:
            # 3ê°œ ìŠ¤íƒ€ì¼ ê°ê°ì— ëŒ€í•´ í”¼ë“œë°± í™•ì¸
            for i in range(1, 4):
                feedback_key = f'style_{i}_feedback'
                score_key = f'style_{i}_score'

                feedback = item.get(feedback_key)
                score = item.get(score_key)

                if feedback in ['good', 'like']:
                    metrics.positive_count += 1
                    if score is not None:
                        good_scores.append(float(score))
                elif feedback in ['bad', 'dislike']:
                    metrics.negative_count += 1
                    if score is not None:
                        bad_scores.append(float(score))

        # ì´ ìƒ˜í”Œ ìˆ˜
        metrics.sample_count = metrics.positive_count + metrics.negative_count

        # ê¸ì • í”¼ë“œë°± ë¹„ìœ¨
        if metrics.sample_count > 0:
            metrics.positive_feedback_rate = metrics.positive_count / metrics.sample_count

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        if good_scores:
            metrics.avg_score_for_good = sum(good_scores) / len(good_scores)

        if bad_scores:
            metrics.avg_score_for_bad = sum(bad_scores) / len(bad_scores)

        # ì ìˆ˜ êµ¬ë¶„ë ¥ (ì¢‹ì€ ìŠ¤íƒ€ì¼ê³¼ ë‚˜ìœ ìŠ¤íƒ€ì¼ì˜ ì ìˆ˜ ì°¨ì´)
        if good_scores and bad_scores:
            metrics.score_discrimination = metrics.avg_score_for_good - metrics.avg_score_for_bad

        return metrics

    def is_challenger_better(
        self,
        metrics: Dict[str, ABTestMetrics],
        min_samples: int = 100,
        min_improvement: float = 0.02
    ) -> Dict[str, Any]:
        """
        Challengerê°€ Championë³´ë‹¤ ë‚˜ì€ì§€ íŒë‹¨

        Args:
            metrics: get_metrics_by_variant() ë°˜í™˜ê°’
            min_samples: ìµœì†Œ í•„ìš” ìƒ˜í”Œ ìˆ˜
            min_improvement: ìµœì†Œ ê°œì„ ìœ¨ (ê¸°ë³¸ 2%)

        Returns:
            {
                "conclusion": "challenger_wins" | "champion_wins" | "no_difference" | "insufficient_data",
                "champion_metrics": {...},
                "challenger_metrics": {...},
                "improvement": 0.05,  # 5% ê°œì„ 
                "recommendation": "ìƒˆ ëª¨ë¸ë¡œ êµì²´ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤" | "ê¸°ì¡´ ëª¨ë¸ ìœ ì§€ ê¶Œì¥",
                "confidence": "high" | "medium" | "low"
            }
        """
        champion = metrics.get('champion')
        challenger = metrics.get('challenger')

        result = {
            'champion_metrics': champion.to_dict() if champion else None,
            'challenger_metrics': challenger.to_dict() if challenger else None,
            'conclusion': 'insufficient_data',
            'improvement': 0.0,
            'recommendation': '',
            'confidence': 'low',
            'evaluated_at': datetime.now(timezone.utc).isoformat()
        }

        # ë°ì´í„° ê²€ì¦
        if not champion or not challenger:
            result['recommendation'] = "ë³€í˜•ë³„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë” ë§ì€ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”."
            return result

        # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
        if champion.sample_count < min_samples or challenger.sample_count < min_samples:
            result['recommendation'] = (
                f"ìµœì†Œ ìƒ˜í”Œ ìˆ˜({min_samples})ë¥¼ ì¶©ì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                f"Champion: {champion.sample_count}, Challenger: {challenger.sample_count}"
            )
            return result

        # ê¸ì • í”¼ë“œë°± ë¹„ìœ¨ ë¹„êµ
        rate_diff = challenger.positive_feedback_rate - champion.positive_feedback_rate
        result['improvement'] = round(rate_diff, 4)

        # ì‹ ë¢°ë„ ê³„ì‚° (ìƒ˜í”Œ ìˆ˜ ê¸°ë°˜)
        total_samples = champion.sample_count + challenger.sample_count
        if total_samples >= 1000:
            result['confidence'] = 'high'
        elif total_samples >= 500:
            result['confidence'] = 'medium'
        else:
            result['confidence'] = 'low'

        # ê²°ë¡  ë„ì¶œ
        if rate_diff > min_improvement:
            result['conclusion'] = 'challenger_wins'
            result['recommendation'] = (
                f"Challenger ëª¨ë¸ì´ {rate_diff*100:.1f}% ë” ë†’ì€ ê¸ì • í”¼ë“œë°± ë¹„ìœ¨ì„ ë³´ì…ë‹ˆë‹¤. "
                f"ìƒˆ ëª¨ë¸ë¡œ êµì²´ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            )
        elif rate_diff < -min_improvement:
            result['conclusion'] = 'champion_wins'
            result['recommendation'] = (
                f"Champion ëª¨ë¸ì´ {-rate_diff*100:.1f}% ë” ë†’ì€ ê¸ì • í”¼ë“œë°± ë¹„ìœ¨ì„ ë³´ì…ë‹ˆë‹¤. "
                f"ê¸°ì¡´ ëª¨ë¸ ìœ ì§€ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            )
        else:
            result['conclusion'] = 'no_difference'
            result['recommendation'] = (
                f"ë‘ ëª¨ë¸ ê°„ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤ (ì°¨ì´: {rate_diff*100:.1f}%). "
                f"ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì‹¤í—˜ì„ ì¢…ë£Œí•˜ì„¸ìš”."
            )

        # ì ìˆ˜ êµ¬ë¶„ë ¥ë„ í•¨ê»˜ ê³ ë ¤
        if challenger.score_discrimination > champion.score_discrimination:
            result['recommendation'] += " Challengerì˜ ì ìˆ˜ êµ¬ë¶„ë ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤."
        elif challenger.score_discrimination < champion.score_discrimination:
            result['recommendation'] += " Championì˜ ì ìˆ˜ êµ¬ë¶„ë ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤."

        logger.info(
            f"ğŸ“Š A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ì™„ë£Œ: {result['conclusion']} "
            f"(improvement={result['improvement']:.2%}, confidence={result['confidence']})"
        )

        return result

    def get_experiment_summary(self, experiment_id: str) -> Dict[str, Any]:
        """
        ì‹¤í—˜ ìš”ì•½ ì •ë³´ ë°˜í™˜

        Args:
            experiment_id: ì‹¤í—˜ ID

        Returns:
            {
                'experiment_id': 'exp_2025_12_02',
                'status': 'running' | 'completed',
                'started_at': '2025-12-02T00:00:00Z',
                'duration_days': 3,
                'total_samples': 250,
                'champion_samples': 225,
                'challenger_samples': 25,
                'current_winner': 'challenger'
            }
        """
        metrics = self.get_metrics_by_variant(experiment_id)

        champion = metrics.get('champion', ABTestMetrics(variant='champion'))
        challenger = metrics.get('challenger', ABTestMetrics(variant='challenger'))

        total = champion.sample_count + challenger.sample_count

        # í˜„ì¬ ìŠ¹ì íŒë‹¨ (ë‹¨ìˆœ ë¹„ìœ¨ ë¹„êµ)
        if total < 50:
            current_winner = 'undetermined'
        elif challenger.positive_feedback_rate > champion.positive_feedback_rate:
            current_winner = 'challenger'
        elif champion.positive_feedback_rate > challenger.positive_feedback_rate:
            current_winner = 'champion'
        else:
            current_winner = 'tie'

        return {
            'experiment_id': experiment_id,
            'status': 'running' if total < 100 else 'analyzing',
            'total_samples': total,
            'champion_samples': champion.sample_count,
            'challenger_samples': challenger.sample_count,
            'champion_positive_rate': round(champion.positive_feedback_rate, 4),
            'challenger_positive_rate': round(challenger.positive_feedback_rate, 4),
            'current_winner': current_winner
        }


# ========== ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ==========
_evaluator_instance: Optional[ABTestEvaluator] = None


def get_ab_evaluator() -> ABTestEvaluator:
    """
    A/B í…ŒìŠ¤íŠ¸ í‰ê°€ê¸° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Returns:
        ABTestEvaluator ì¸ìŠ¤í„´ìŠ¤
    """
    global _evaluator_instance

    if _evaluator_instance is None:
        _evaluator_instance = ABTestEvaluator()

    return _evaluator_instance
